
'''
üöÄ 3Filoff EDA Toolkit

viz.py v.0.4 ‚Ä¢ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ EDA

–æ—Ç—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ 2025 26 10

upd : 
   ‚Ä¢ plot_phik_correlation - –∞–≤—Ç–æ–º–∞—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å –ø–æ–¥—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ–¥ DPI

'''

import re
import os
import shap
from pathlib import Path
from itertools import combinations
from typing import Optional, Set, Union, List, Dict, DefaultDict, Literal, Callable, Any, Tuple
from collections import defaultdict

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.colors
from scipy import stats
from scipy.stats import chi2_contingency, pointbiserialr, skew, kurtosis, ttest_ind, mannwhitneyu
from pandas.api.types import is_float_dtype, is_integer_dtype, is_numeric_dtype

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã Jupyter
from IPython.display import display
from pandas.io.formats.style import Styler



# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ - —Å fallback –Ω–∞ '../datasets' (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ utils/)
DATA_ROOT = Path(os.getenv("DATA_ROOT", "./../datasets"))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–º–∏, –Ω–æ –¥–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
CSV_PATHS = {}

# –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –æ–ø–∏—Å–∞–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ –∏—Ö –∏–º–µ–Ω–∏
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
DATASET_DESCRIPTIONS = {}

# –ï–¥–∏–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –æ–ø–∏—Å–∞–Ω–∏–π –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –ø—Ä–æ–µ–∫—Ç–µ
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
COLUMN_DESCRIPTIONS = {}




# ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢ –§–£–ù–ö–¶–ò–ò 3Filoff ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# ## **–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏**

# –ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π, —á—Ç–æ–±—ã:
# - –∏–∑–±–µ–∂–∞—Ç—å –∑–∞–≥—Ä–æ–º–æ–∂–¥–µ–Ω–∏—è —è—á–µ–µ–∫ —é–ø–∏—Ç–µ—Ä –Ω–æ—É—Ç–±—É–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º—Å—è –∫–æ–¥–æ–º
# - —Å–¥–µ–ª–∞—Ç—å –∫–æ–¥ –±–æ–ª–µ–µ —á–∏—Ç–∞–µ–º—ã–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º
# - —É–ø—Ä–æ—Å—Ç–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∞ –≤ –±—É–¥—É—â–µ–º
# 
# –≠—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∑–≤–æ–ª—è—Ç –Ω–∞–º –±—ã—Å—Ç—Ä–æ –∏ —É–¥–æ–±–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏, –Ω–µ –¥—É–±–ª–∏—Ä—É—è –∫–æ–¥ –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –Ω–æ—É—Ç–±—É–∫–∞








#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


"""
–ú–æ–¥—É–ª—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è EDA: –∫—Ä–∞—Å–∏–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –Ω–æ—É—Ç–±—É–∫–µ + —ç–∫—Å–ø–æ—Ä—Ç –≤ Markdown.

–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - `display_table` - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç—Ä–∏ —Ä–µ–∂–∏–º–∞:
- notebook: —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è HTML-—Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞,
- markdown: –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è pipe-—Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –æ—Ç—á—ë—Ç—ã,
- both: –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏ —Ç–æ, –∏ –¥—Ä—É–≥–æ–µ.

–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é:
    from utils.viz import set_output_mode
    set_output_mode("markdown")
"""


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –≤—ã–≤–æ–¥–∞: "notebook", "markdown", –∏–ª–∏ "both"
EDA_OUTPUT_MODE: str = "notebook"



#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def _print_compact_markdown(df: pd.DataFrame) -> None:
    """
    –ü–µ—á–∞—Ç–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é Markdown-—Ç–∞–±–ª–∏—Ü—É –±–µ–∑ –∫–∞–≤—ã—á–µ–∫, —Å –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫.
    NaN –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –∫–∞–∫ –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏.
    """
    if df.empty:
        print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º")
        return

    # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, –ø–æ—Ç–æ–º –ø—Ä–∏–≤–æ–¥–∏–º –∫ str
    df_str = df.fillna("").astype(str)
    headers = "|".join(df_str.columns)
    separator = "|".join(["---"] * len(df_str.columns))
    rows = ["|".join(row) for row in df_str.values]
    lines = [f"|{headers}|", f"|{separator}|"] + [f"|{row}|" for row in rows]
    print("\n".join(lines))


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def set_global_styles(
    dpi: int = 150,
    palette: str = "rocket",
    grid_color: str = "#E0E0E0",
    font_family: str = "Sans",
    apply_pandas_display: bool = True,
    apply_seaborn_style: bool = True,
    apply_matplotlib_rc: bool = True,
) -> None:
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è pandas, seaborn –∏ matplotlib.
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –õ—ë–≥–∫–∞—è —Å–µ—Ç–∫–∞ –∏ –æ–∫—Ä—É–≥–ª—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏,
        - –ß–∏—Ç–∞–µ–º—ã–µ —à—Ä–∏—Ñ—Ç—ã (Arial),
        - –ü–∞–ª–∏—Ç—Ä–∞ 'rocket' (–≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç —Ç—ë–º–Ω–æ–≥–æ –∫ —è—Ä–∫–æ–º—É),
        - –ú–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –±–µ–∑ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        dpi                  : int  ‚Ä¢ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 150)
        palette              : str  ‚Ä¢ –ø–∞–ª–∏—Ç—Ä–∞ seaborn –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'tab10')
        grid_color           : str  ‚Ä¢ —Ü–≤–µ—Ç —Å–µ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é '#c5d1e0' - —Å–µ—Ä–æ-–≥–æ–ª—É–±–æ–π)
        font_family          : str  ‚Ä¢ —Å–µ–º–µ–π—Å—Ç–≤–æ —à—Ä–∏—Ñ—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'DejaVu Sans')
        apply_pandas_display : bool ‚Ä¢ –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ pandas (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        apply_seaborn_style  : bool ‚Ä¢ –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ —Å—Ç–∏–ª—å seaborn (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        apply_matplotlib_rc  : bool ‚Ä¢ –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ rcParams matplotlib (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ
    """
    # 1. Pandas: —á–∏—Ç–∞–µ–º–æ—Å—Ç—å
    if apply_pandas_display:
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', None)
        pd.set_option('display.expand_frame_repr', False)

    # 2. Seaborn: —Å—Ç–∏–ª—å
    if apply_seaborn_style:
        sns.set_style("whitegrid", {
            'axes.facecolor': 'white',
            'grid.color': grid_color,
            'grid.linewidth': 0.7,
            'axes.edgecolor': '#333333',
            'axes.labelcolor': '#333333',
            'xtick.color': '#555555',
            'ytick.color': '#555555',
            'font.family': font_family
        })
        sns.set_palette(palette)

    # 3. Matplotlib: –≤–Ω–µ—à–Ω–∏–π –≤–∏–¥
    if apply_matplotlib_rc:
        plt.rcParams.update({
            'figure.dpi': dpi,
            'savefig.dpi': dpi,
            'savefig.bbox': 'tight',
            'savefig.pad_inches': 0.2,

            'font.family': font_family,
            'font.size': 9,
            'axes.titlesize': 11,
            'axes.labelsize': 9,
            'axes.titleweight': 'bold',
            'axes.labelweight': 'normal',
            'axes.labelpad': 4.0,
            'axes.titlepad': 6.0,

            'xtick.labelsize': 8,
            'ytick.labelsize': 8,
            'xtick.color': '#555555',
            'ytick.color': '#555555',

            'lines.linewidth': 1.4,
            'lines.markersize': 4,

            'patch.edgecolor': 'white',
            'patch.linewidth': 0.8,

            'figure.facecolor': 'white',
            'axes.facecolor': 'white',
            'axes.spines.left': True,
            'axes.spines.bottom': True,
            'axes.spines.top': False,
            'axes.spines.right': False
        })

    # üíé –í—ã–≤–æ–¥ –Ω–∞—Å—Ç—Ä–æ–µ–∫
    print(f"üé® –°—Ç–∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: | DPI={dpi} | Palette='{palette}' | Grid='{grid_color}'")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def set_output_mode(mode: str, verbose: bool = True) -> None:
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –≤—ã–≤–æ–¥–∞ –¥–ª—è display_table.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
   -------
    mode : str
        –û–¥–∏–Ω –∏–∑: "notebook", "markdown", "both".
    verbose : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
        –ï—Å–ª–∏ True - –≤—ã–≤–æ–¥–∏—Ç –ø–æ–Ω—è—Ç–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å —ç–º–æ–¥–∑–∏.
    """
    global EDA_OUTPUT_MODE
    if mode not in ("notebook", "markdown", "both"):
        raise ValueError("mode must be one of: 'notebook', 'markdown', 'both'")
    EDA_OUTPUT_MODE = mode
    if verbose:
        mode_labels = {
            "notebook": "üëÅÔ∏è notebook ‚Ä¢ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
            "markdown": "üìã markdown ‚Ä¢ —á–∏—Å—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è",
            "both": "üëÅÔ∏è + üìã both ‚Ä¢ –∏ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ, –∏ –∫–æ–ø–∏—Ä—É–µ–º–æ"
        }
        print(f"üöÄ EDA-—Å—Ä–µ–¥–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {mode_labels[mode]}")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def display_table(
    df: pd.DataFrame,
    rows: Optional[int] = 5,
    float_precision: int = 3,
    max_header_length: int = 20,
    styler_func: Optional[Callable[[Styler], Styler]] = None,
    as_markdown: Optional[bool] = None,
    mode: Optional[str] = None
) -> None:
    """
    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –≤ –æ–¥–Ω–æ–º –∏–∑ —Ç—Ä—ë—Ö —Ä–µ–∂–∏–º–æ–≤: –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏–ª–∏ –¥–ª—è –æ–±–æ–∏—Ö —Ü–µ–ª–µ–π —Å—Ä–∞–∑—É.

    –¶–µ–ª—å:
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π:
        - –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥–µ (Jupyter) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É,
        - –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–∏—Å—Ç—É—é Markdown-—Ç–∞–±–ª–∏—Ü—É, –≥–æ—Ç–æ–≤—É—é –∫ –≤—Å—Ç–∞–≤–∫–µ –≤ –æ—Ç—á—ë—Ç—ã,
        - –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–≤–º–µ—â–∞—Ç—å –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É–¥–æ–±—Å—Ç–≤–∞.

    –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
        - "notebook" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é):
            –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–≤–æ–¥, –æ–±—Ä–µ–∑–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ü–≤–µ—Ç–æ–≤—É—é —Å—Ö–µ–º—É,
            –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –∫–∞—Å—Ç–æ–º–Ω—É—é —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—é. –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è —Ä–∞–∑–≤–µ–¥–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
        - "markdown":
            –ü–µ—á–∞—Ç–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é pipe-—Ç–∞–±–ª–∏—Ü—É —Å –ø–æ–ª–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–ª–æ–Ω–æ–∫ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —á–∏—Å–µ–ª.
            –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤ GitHub, Notion, Obsidian –∏ –¥—Ä.
        - "both":
            –°–Ω–∞—á–∞–ª–∞ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ç–∏–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, –∑–∞—Ç–µ–º - Markdown-–≤–µ—Ä—Å–∏—é —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º.
            –£–¥–æ–±–µ–Ω –ø—Ä–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–ø–∏—Ä—É–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df : pd.DataFrame
            –î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        rows : int or None, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5
            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞. –ï—Å–ª–∏ None - –≤—ã–≤–æ–¥—è—Ç—Å—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏.
        float_precision : int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –¥–ª—è –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∏—Å–µ–ª.
        max_header_length : int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20
            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ä–µ–∂–∏–º–µ "notebook" (–¥–ª–∏–Ω–Ω—ã–µ –æ–±—Ä–µ–∑–∞—é—Ç—Å—è —Å '...').
            –í —Ä–µ–∂–∏–º–∞—Ö "markdown" –∏ "both" (–¥–ª—è Markdown-—á–∞—Å—Ç–∏) –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è.
        styler_func : Optional[Callable[[Styler], Styler]], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤ —Ä–µ–∂–∏–º–µ "notebook"
            (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–¥—Å–≤–µ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã).
            –ù–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ Markdown-—Ä–µ–∂–∏–º–µ.
        as_markdown : Optional[bool], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:
                True ‚Üí mode="markdown",
                False ‚Üí mode="notebook".
            –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è, –µ—Å–ª–∏ –∑–∞–¥–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä `mode`.
        mode : Optional[str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∂–∏–º–∞ –≤—ã–≤–æ–¥–∞. –í–æ–∑–º–æ–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:
                "notebook", "markdown", "both".
            –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: mode > as_markdown > EDA_OUTPUT_MODE.

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None. –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É Jupyter.

    –ü—Ä–∏–º–µ—Ä—ã:
        >>> display_table(df)  # –∫—Ä–∞—Å–∏–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
        >>> display_table(df, mode="markdown")  # —Ç–æ–ª—å–∫–æ Markdown
        >>> display_table(df, mode="both")  # –∏ —Ç–æ, –∏ –¥—Ä—É–≥–æ–µ

        # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ
        >>> set_output_mode("both")
        >>> display_table(df)
    """
    if df.empty:
        print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º")
        return
    
    df = df.copy()
    df.columns = df.columns.astype(str)
    if hasattr(df.index, 'name'):  # –Ω–µ —Å–ª–æ–º–∞–µ–º MultiIndex –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        df.index = df.index.astype(str)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ —Å —É—á—ë—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
    if mode is not None:
        use_mode = mode
    elif as_markdown is not None:
        use_mode = "markdown" if as_markdown else "notebook"
    else:
        use_mode = EDA_OUTPUT_MODE

    if use_mode not in ("notebook", "markdown", "both"):
        raise ValueError("mode must be one of: 'notebook', 'markdown', 'both'")

    # –†–µ–∂–∏–º "both": —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ –¥–≤—É—Ö —Ä–µ–∂–∏–º–æ–≤
    if use_mode == "both":
        display_table(
            df, rows=rows, float_precision=float_precision,
            max_header_length=max_header_length, styler_func=styler_func,
            mode="notebook"
        )
        #print("\n" + "‚Ä¢" * 20 + " markdown " + "‚Ä¢" * 20 + "\n")
        print()
        display_table(
            df, rows=rows, float_precision=float_precision,
            max_header_length=max_header_length, styler_func=styler_func,
            mode="markdown"
        )
        return

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ (–∏–ª–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ, –µ—Å–ª–∏ rows=None)
    df_limited = df if rows is None else df.head(rows)
    if df_limited.empty:
        print("‚ö†Ô∏è –ù–µ—Ç —Å—Ç—Ä–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        return

    # –†–µ–∂–∏–º "markdown"
    if use_mode == "markdown":
        df_out = df_limited.copy()
        for col in df_out.select_dtypes(include=['number']).columns:
            if is_float_dtype(df[col]):
                df_out[col] = df_out[col].map(
                    lambda x: f"{x:.{float_precision}f}" if pd.notna(x) else ""
                )
            elif is_integer_dtype(df[col]):
                df_out[col] = df_out[col].map(
                    lambda x: f"{x:d}" if pd.notna(x) else ""
                )
        _print_compact_markdown(df_out)
        return

    # –†–µ–∂–∏–º "notebook"
    df_display = df_limited.copy()

    # –û–±—Ä–µ–∑–∫–∞ –∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ - —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ—É—Ç–±—É–∫–∞
    truncated_columns = []
    seen = {}
    for col in df.columns:
        if isinstance(col, str) and len(col) > max_header_length:
            truncated = col[:max_header_length - 3] + "..."
        else:
            truncated = col

        if truncated in seen:
            seen[truncated] += 1
            unique_truncated = f"{truncated}.{seen[truncated]}"
        else:
            seen[truncated] = 0
            unique_truncated = truncated

        truncated_columns.append(unique_truncated)

    df_display.columns = truncated_columns

    original_to_truncated = dict(zip(df.columns, truncated_columns))
    numeric_cols_orig = df.select_dtypes(include=['number']).columns.tolist()
    text_cols_orig = df.select_dtypes(include=['object', 'category']).columns.tolist()
    numeric_cols_display = [original_to_truncated[col] for col in numeric_cols_orig if col in df.columns]
    text_cols_display = [original_to_truncated[col] for col in text_cols_orig if col in df.columns]

    styled = df_display.style

    # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–µ–ª - –ë–ï–ó —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π —Ä–∞–∑—Ä—è–¥–æ–≤
    if numeric_cols_display:
        format_dict = {}
        for orig_col in numeric_cols_orig:
            disp_col = original_to_truncated[orig_col]
            if is_float_dtype(df[orig_col]):
                format_dict[disp_col] = f"{{:.{float_precision}f}}"
            elif is_integer_dtype(df[orig_col]):
                format_dict[disp_col] = "{}"  # —Ü–µ–ª—ã–µ –±–µ–∑ –∑–∞–ø—è—Ç—ã—Ö
            else:
                format_dict[disp_col] = "{:.2f}"
        styled = styled.format(format_dict)

    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
    if numeric_cols_display:
        styled = styled.set_properties(subset=numeric_cols_display, **{'text-align': 'right', 'font-family': 'tahoma'})
    if text_cols_display:
        styled = styled.set_properties(subset=text_cols_display, **{'text-align': 'left', 'font-family': 'tahoma'})

    # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∏–ª–∏ —Ç–∞–±–ª–∏—Ü—ã
    styled = styled.set_table_styles([
        {
            'selector': 'th:not(.row_heading)',
            'props': [
                ('background-color', '#ffffff !important'),
                ('color', "#7c213e"),
                ('text-align', 'center')
            ]
        },
        {
            'selector': 'thead, thead th, thead td, th.col_heading',
            'props': [
                ('background', 'transparent !important'),
                ('background-color', 'transparent !important'),
                ('border', 'none !important')
            ]
        },
        {
            'selector': 'th.col_heading',
            'props': [
                ('text-align', 'left'),
                ('font-family', 'tahoma'),
                ('font-weight', '400'),
                ('background-color', 'transparent'),
                ('padding', '8px 6px'),
                ('font-size', '11px'),
                ('color', "#5b7485")
            ]
        },
        {
            'selector': 'th.row_heading',
            'props': [
                ('background-color', "#dfe6eb"),
                ('border', '1px solid #758c9b'),
                ('font-family', 'tahoma'),
                ('text-align', 'right'),
                ('padding', '4px 6px'),
                ('font-size', '11px'),
                ('color', "#576c7b")
            ]
        },
        {
            'selector': 'td',
            'props': [
                ('font-family', 'tahoma'),
                ('border', '1px solid #a2b3be'),
                ('padding', '4px 6px'),
                ('font-size', '11px')
            ]
        }
    ])

    if styler_func is not None:
        styled = styler_func(styled)

    from IPython.display import display
    display(styled)









#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢






def standardize_column_names(
    df: pd.DataFrame,
    verbose: bool = False,
    handle_camel_case: bool = True,
    show_summary: bool = False
) -> None:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∫ snake_case –Ω–∞–ø—Ä—è–º—É—é (in-place).
    
    –¶–µ–ª—å: –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∫ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É –∞–Ω–∞–ª–∏–∑—É –∏ ML –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏
    —Å –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã–º–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ handle_camel_case=True):
        'Heart Rate'         ‚Üí 'heart_rate'
        'CK-MB'              ‚Üí 'ck_mb'
        'Systolic BP'        ‚Üí 'systolic_bp'
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame
            –î–∞—Ç–∞—Ñ—Ä–µ–π–º, –∫–æ–ª–æ–Ω–∫–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –±—É–¥—É—Ç –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω—ã –Ω–∞ –º–µ—Å—Ç–µ.
        verbose: bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False
            –í—ã–≤–æ–¥–∏—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è—Ö.
        handle_camel_case: bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –ª–∏ CamelCase ‚Üí snake_case.
        show_summary: bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False
            –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        None. –ò–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∫ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É.
    
    –ò—Å–∫–ª—é—á–µ–Ω–∏—è:
        ValueError: –µ—Å–ª–∏ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∏–∫–∞—é—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫.
    
    –ü—Ä–∏–º–µ—Ä—ã:
        >>> standardize_column_names(df)
        >>> standardize_column_names(df, verbose=True)
        >>> standardize_column_names(df, verbose=True, show_summary=True)
    
    –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
        import re
    """
    def _to_snake_case(name: str, handle_camel: bool = True) -> str:
        # –®–∞–≥ 1: –∑–∞–º–µ–Ω—è–µ–º –≤—Å–µ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
        s1 = re.sub(r'[^a-zA-Z0-9]+', '_', name)
        # –®–∞–≥ 2: (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º CamelCase ‚Üí snake_case
        if handle_camel:
            s2 = re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', s1)
        else:
            s2 = s1
        # –®–∞–≥ 3: –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ/–∫—Ä–∞–µ–≤—ã–µ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
        s3 = re.sub(r'_+', '_', s2.lower()).strip('_')
        return s3 if s3 else "unnamed_column"

    original_cols = list(df.columns)
    new_cols = [_to_snake_case(col, handle_camel=handle_camel_case) for col in original_cols]

    # üîí –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–ª–ª–∏–∑–∏–∏ –∏–º—ë–Ω
    if len(new_cols) != len(set(new_cols)):
        duplicates = [col for col in set(new_cols) if new_cols.count(col) > 1]
        raise ValueError(
            f"–ö–æ–ª–ª–∏–∑–∏—è –∏–º—ë–Ω –∫–æ–ª–æ–Ω–æ–∫ –ø–æ—Å–ª–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏. –î—É–±–ª–∏–∫–∞—Ç—ã: {duplicates}. "
            "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è - –æ–Ω–∏ –º–æ–≥—É—Ç –¥–∞–≤–∞—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π snake_case."
        )

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
    df.columns = new_cols

    # üñ®Ô∏è –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
    if show_summary:
        from utils.viz import dataset_profile  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å circular dependency
        dataset_profile(df, report='head')
    
    if verbose:
        total_cols = len(df.columns)
        changed = [(orig, new) for orig, new in zip(original_cols, new_cols) if orig != new]
        unchanged = total_cols - len(changed)

        if not changed:
            print("üìå –ö–æ–ª–æ–Ω–∫–∏ —É–∂–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç snake_case - –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        else:
            if unchanged == 0:
                print(f"üî§ –í—Å–µ {total_cols} –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ snake_case:")
            else:
                print(f"üî§ {len(changed)} –∏–∑ {total_cols} –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ snake_case "
                      f"(–æ—Å—Ç–∞–ª—å–Ω—ã–µ {unchanged} —É–∂–µ –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ):")

            rename_df = pd.DataFrame(changed, columns=["–ò—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è", "–ù–æ–≤–æ–µ –∏–º—è"])
            display_table(rename_df, rows=len(rename_df))
            print('')




#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# üßπ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∞—É–¥–∏—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
#
# –≠—Ç–æ—Ç –±–ª–æ–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö:
# - _find_typo_groups: –Ω–∞—Ö–æ–¥–∏—Ç –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ ‚â§ max_distance)
# - _levenshtein_distance: –∏–∑–º–µ—Ä—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å —Å—Ç—Ä–æ–∫ (–¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø–µ—á–∞—Ç–æ–∫ –∏ –¥—É–±–ª–µ–π)
# - _normalize_text: –ø—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É (—Ä–µ–≥–∏—Å—Ç—Ä, —ë/–µ, –∑–Ω–∞–∫–∏)
# - _is_likely_numeric: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —á–∏—Å–ª–∞–º–∏

# _find_typo_groups: –ù–∞—Ö–æ–¥–∏—Ç –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ ‚â§ max_distance)
def _find_typo_groups(values: Set[str], max_distance: int = 2) -> List[List[str]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ ‚â§ max_distance).
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        values: Set[str] - —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        max_distance: int - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        List[List[str]] - —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π; –∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç ‚â•2 —ç–ª–µ–º–µ–Ω—Ç–∞
    """
    if len(values) < 2:
        return []
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º
    normalized_to_original = {}
    for val in values:
        normalized = _normalize_text(str(val))
        if normalized not in normalized_to_original:
            normalized_to_original[normalized] = []
        normalized_to_original[normalized].append(val)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    normalized_values = list(normalized_to_original.keys())
    used = set()
    groups = []
    
    for i, norm_val1 in enumerate(normalized_values):
        if norm_val1 in used:
            continue
            
        current_group = normalized_to_original[norm_val1].copy()
        used.add(norm_val1)
        
        for j in range(i + 1, len(normalized_values)):
            norm_val2 = normalized_values[j]
            if norm_val2 in used:
                continue
            if _levenshtein_distance(norm_val1, norm_val2) <= max_distance:
                current_group.extend(normalized_to_original[norm_val2])
                used.add(norm_val2)
        
        if len(set(current_group)) > 1:
            groups.append(current_group)
    
    return groups

def _levenshtein_distance(s1: str, s2: str) -> int:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏.
    
    –û–ø–∏—Å–∞–Ω–∏–µ: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–ø–µ—Ä–∞—Ü–∏–π –≤—Å—Ç–∞–≤–∫–∏, —É–¥–∞–ª–µ–Ω–∏—è –∏–ª–∏ –∑–∞–º–µ–Ω—ã —Å–∏–º–≤–æ–ª–∞,
              –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏—è s1 –≤ s2. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫
              –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        s1: str - –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        s2: str - –≤—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        int - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (–Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ)
    """
    
    if len(s1) < len(s2):
        return _levenshtein_distance(s2, s1)
    if len(s2) == 0:
        return len(s1)
    previous_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    return previous_row[-1]


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def _normalize_text(text: str) -> str:
    """
    –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
    
    –û–ø–∏—Å–∞–Ω–∏–µ: –ü—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –∑–∞–º–µ–Ω—è–µ—Ç '—ë' –Ω–∞ '–µ',
              —É–¥–∞–ª—è–µ—Ç –º—è–≥–∫–∏–π/—Ç–≤—ë—Ä–¥—ã–π –∑–Ω–∞–∫–∏, –∑–∞–º–µ–Ω—è–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã,
              —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã. –£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã.
    """
    text = text.lower()
    text = text.replace('—ë', '–µ')
    text = text.replace('—å', '').replace('—ä', '')
    text = re.sub(r'[^–∞-—èa-z0-9\s]', ' ', text, flags=re.IGNORECASE)
    return ' '.join(text.split())


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def _is_likely_numeric(series: pd.Series) -> bool:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–µ—Ä–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —á–∏—Å–ª–æ–º –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        series: pd.Series - —Å–µ—Ä–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        bool - True, –µ—Å–ª–∏ –±–æ–ª–µ–µ 90% –Ω–µ–ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–æ–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤–æ float
    """
    
    non_null = series.dropna().astype(str)
    if non_null.empty:
        return False
    numeric_count = 0
    for val in non_null:
        try:
            float(val)
            numeric_count += 1
        except ValueError:
            pass
    return numeric_count / len(non_null) > 0.9


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# _bytes_to_human_readable: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—É—é —Å—Ç—Ä–æ–∫—É —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è
def _bytes_to_human_readable(size_bytes: int) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—É—é —Å—Ç—Ä–æ–∫—É —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        size_bytes: int - —Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        str - —Å—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ '1.23 –ö–ë', '456.0 –±–∞–π—Ç', '2.1 –ì–ë'
    """
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
    units = ['–±–∞–π—Ç', '–ö–ë', '–ú–ë', '–ì–ë']
    size = float(size_bytes)
    unit_index = 0
    
    # –ü–æ–∫–∞ —Ä–∞–∑–º–µ—Ä –±–æ–ª—å—à–µ 1024 –∏ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã
    while size >= 1024 and unit_index < len(units) - 1:
        size /= 1024
        unit_index += 1
    
    # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ –¥–≤—É—Ö –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
    return f"{round(size, 2)} {units[unit_index]}"


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# label_for_dataset: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –æ–±—ä–µ–∫—Ç—É DataFrame
from typing import Optional, Union, Literal
import pandas as pd

def label_for_dataset(
    df: pd.DataFrame,
    separator: Optional[str] = None,
    format: Literal["tuple", "string"] = "tuple"
) -> Union[tuple[str, str], str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –æ–±—ä–µ–∫—Ç—É DataFrame.

    –ü–æ–≤–µ–¥–µ–Ω–∏–µ:
        - format="tuple" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é): –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–º—è, –æ–ø–∏—Å–∞–Ω–∏–µ_—Å_separator).
        - format="string": –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É "–∏–º—è –æ–ø–∏—Å–∞–Ω–∏–µ_—Å_separator".

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∏–º—è (–±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤).
        - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö Jupyter Notebook.
        - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ DATASET_DESCRIPTIONS –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df : pd.DataFrame
            –û–±—ä–µ–∫—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
        separator : Optional[str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –ö–∞–∫ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ:
                - None ‚Üí " –æ–ø–∏—Å–∞–Ω–∏–µ"
                - "‚Ä¢" ‚Üí " ‚Ä¢ –æ–ø–∏—Å–∞–Ω–∏–µ"
                - "()" ‚Üí " (–æ–ø–∏—Å–∞–Ω–∏–µ)"
                - "[...]" ‚Üí " [...] –æ–ø–∏—Å–∞–Ω–∏–µ"
        format : {"tuple", "string"}, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "tuple"
            –§–æ—Ä–º–∞—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        tuple[str, str] –∏–ª–∏ str - –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç format.
    """
    dataset_descriptions = globals().get("DATASET_DESCRIPTIONS", {})
    
    # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å globals –∏–∑ Jupyter
    search_space = globals()  # fallback
    try:
        from IPython import get_ipython
        ipython = get_ipython()
        if ipython is not None:
            search_space = ipython.user_global_ns
    except Exception:
        pass

    for name, obj in search_space.items():
        if obj is df:
            raw_description = dataset_descriptions.get(name, "").strip()
            
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è
            if not raw_description:
                if format == "tuple":
                    return name, ""
                else:
                    return name

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
            if separator is None:
                formatted_desc = f" {raw_description}"
            elif len(separator) == 1:
                formatted_desc = f" {separator} {raw_description}"
            elif len(separator) == 2:
                formatted_desc = f" {separator[0]}{raw_description}{separator[1]}"
            else:
                formatted_desc = f" {separator} {raw_description}"

            if format == "tuple":
                return name, formatted_desc
            else:
                return name + formatted_desc

    # –î–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–µ –Ω–∞–π–¥–µ–Ω
    fallback_name = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π_–¥–∞—Ç–∞—Å–µ—Ç"
    if format == "tuple":
        return fallback_name, ""
    else:
        return fallback_name


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# label_for_column: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –∏ –µ—ë –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ COLUMN_DESCRIPTIONS.
def label_for_column(
    col: str,
    separator: Optional[str] = None,
    format: Literal["tuple", "string"] = "tuple"
) -> Union[tuple[str, str], str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥–ø–∏—Å—å –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ COLUMN_DESCRIPTIONS.

    –ü–æ–≤–µ–¥–µ–Ω–∏–µ:
        - format="tuple" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é): –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–º—è, –æ–ø–∏—Å–∞–Ω–∏–µ_—Å_separator).
        - format="string": –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É "–∏–º—è –æ–ø–∏—Å–∞–Ω–∏–µ_—Å_separator".

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        col : str
            –ò–º—è –∫–æ–ª–æ–Ω–∫–∏.
        separator : Optional[str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –ö–∞–∫ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ:
                - None - " –æ–ø–∏—Å–∞–Ω–∏–µ"
                - "‚Ä¢" - " ‚Ä¢ –æ–ø–∏—Å–∞–Ω–∏–µ"
                - "()" - " (–æ–ø–∏—Å–∞–Ω–∏–µ)"
                - "[...]" - " [...] –æ–ø–∏—Å–∞–Ω–∏–µ"
        format : {"tuple", "string"}, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "tuple"
            –§–æ—Ä–º–∞—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        tuple[str, str] –∏–ª–∏ str - –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç format.
    """
    column_descriptions = globals().get("COLUMN_DESCRIPTIONS", {})
    
    raw_description = column_descriptions.get(col, "")
    if not isinstance(raw_description, str):
        raw_description = str(raw_description).strip()
    raw_description = raw_description.strip()
    
    col_name = col
    if not raw_description:
        formatted_desc = ""
    else:
        if separator is None:
            formatted_desc = f" {raw_description}"
        elif len(separator) == 1:
            formatted_desc = f" {separator} {raw_description}"
        elif len(separator) == 2:
            formatted_desc = f" {separator[0]}{raw_description}{separator[1]}"
        else:
            formatted_desc = f" {separator} {raw_description}"

    if format == "tuple":
        return col_name, formatted_desc
    else:  
        return col_name + formatted_desc


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# _fix_decimal_comma: –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å –¥–µ—Å—è—Ç–∏—á–Ω–æ–π –∑–∞–ø—è—Ç–æ–π (¬´14,2¬ª - 14.2) –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ
def _fix_decimal_comma(df: pd.DataFrame, threshold: float = 0.9) -> pd.DataFrame:
    """
        –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å –¥–µ—Å—è—Ç–∏—á–Ω–æ–π –∑–∞–ø—è—Ç–æ–π (¬´14,2¬ª - 14.2) –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ.
        
        –û–ø–∏—Å–∞–Ω–∏–µ:
            –î–ª—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–º–µ–Ω–∏—Ç—å –∑–∞–ø—è—Ç—ã–µ –Ω–∞ —Ç–æ—á–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —á–∏—Å–ª–æ.
            –ö–æ–ª–æ–Ω–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∏—Å–µ–ª ‚â• threshold.
            –ü–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:
                - –µ—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–ª—ã–µ –∏ –Ω–µ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ - —Ç–∏–ø Int64,
                - –∏–Ω–∞—á–µ - float64 (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π NaN).
            –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ EDA –∏–ª–∏ ML.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            df: pd.DataFrame - –≤—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
            threshold: float - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∏—Å–µ–ª –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.9)

        –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
            pd.DataFrame - –∫–æ–ø–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏; –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π df –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è

        –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
            - –ù–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –Ω–µ—Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏.
            - –ù–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–æ–ª–µ–π —á–∏—Å–µ–ª < threshold (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–º–µ—Å—å —Ç–µ–∫—Å—Ç–∞ –∏ —á–∏—Å–µ–ª).
            - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç nullable-—Ç–∏–ø Int64 –¥–ª—è —Ü–µ–ª—ã—Ö –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤.
    """
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    object_cols = df.select_dtypes(include=['object']).columns
    
    cols_to_convert = []
    total_rows = len(df)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –∑–∞–º–µ–Ω–∞ –∑–∞–ø—è—Ç—ã—Ö –∏ –ø–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –±–µ–∑ —Ü–∏–∫–ª–æ–≤
    for col in object_cols:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        cleaned = df[col].astype(str).str.strip().str.replace(',', '.', regex=False)
        numeric_series = pd.to_numeric(cleaned, errors='coerce')
        valid_count = numeric_series.notna().sum()
        
        if valid_count / total_rows >= threshold:
            cols_to_convert.append(col)
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ - –∑–∞–≤–µ—Ä—à–∞–µ–º –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
    if not cols_to_convert:
        print("‚úîÔ∏è –≤—Å–µ –ø—Å–µ–≤–¥–æ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è)")
        return df
    
    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á—ë—Ç –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
    print("\nüõ†Ô∏è –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ `object` —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —á–∏—Å–ª–∞ –≤ `float64`\n")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    df_updated = df.copy()
    
    for col in cols_to_convert:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        cleaned = df[col].astype(str).str.strip().str.replace(',', '.', regex=False)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        numeric_series = pd.to_numeric(cleaned, errors='coerce')
        valid_count = numeric_series.notna().sum()
        print(f"   üîé {col}: {valid_count}/{total_rows} (—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {valid_count/total_rows:.1%})")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            df_updated[col] = pd.to_numeric(cleaned, errors='coerce')
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ {col}: {e}")
            continue

        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–Ω–∏–∑–∏—Ç—å —Ç–∏–ø –¥–æ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
        if (
            df_updated[col].notna().all() and
            (df_updated[col] % 1 == 0).all()
        ):
            try:
                df_updated[col] = df_updated[col].astype('Int64')
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–≤–µ—Å—Ç–∏ {col} –∫ Int64: {e}")
    
    print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    for col in cols_to_convert:
        if col in df_updated.columns:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∞ –Ω–µ –±—ã–ª–∞ —É–¥–∞–ª–µ–Ω–∞
            dtype_name = df_updated[col].dtype.name
            emoji_map = {
                'int8': '1Ô∏è‚É£', 'int16': '1Ô∏è‚É£', 'int32': '1Ô∏è‚É£', 'int64': '1Ô∏è‚É£',
                'uint8': '1Ô∏è‚É£', 'uint16': '1Ô∏è‚É£', 'uint32': '1Ô∏è‚É£', 'uint64': '1Ô∏è‚É£',
                'float16': 'üî¢', 'float32': 'üî¢', 'float64': 'üî¢',
                'object': 'üì¶', 'datetime64': 'üìÖ', 'category': 'üè∑Ô∏è'
            }
            dtype_display = f"{emoji_map.get(dtype_name, 'üö®')} {dtype_name}"
            print(f"   üìå {col} üî® –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ {dtype_display}")
    
    print(f"\n‚úîÔ∏è –≤—Å–µ `object` —Å—Ç–æ–ª–±—Ü—ã ({len(cols_to_convert)}) —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —á–∏—Å–ª–∞, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã –≤ `float64`")
    return df_updated


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# _detect_numerical_issue: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ —á–∏—Å–ª–æ–≤–æ–π —Å–µ—Ä–∏–∏ –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤
def _detect_numerical_issues(series: pd.Series, total_rows: int) -> List[str]:
    """
    –í—ã—è–≤–ª—è–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤ –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ä–∏—é –Ω–∞ –Ω–∞–ª–∏—á–∏–µ:
        - –ø—Ä–æ–ø—É—Å–∫–æ–≤ (>0%),
        - –≤—ã–±—Ä–æ—Å–æ–≤ (>5% –ø–æ IQR),
        - —Å–∏–ª—å–Ω–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ (|skewness| > 1.5),
        - –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π,
        - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∏—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤ (–º–µ–¥–∏–∞–Ω–∞ > 0, –º–∞–∫—Å–∏–º—É–º > –º–µ–¥–∏–∞–Ω—ã √ó 3 –∏ > 1000).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ audit_numerical –∏ dataset_profile –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        series: pd.Series - —á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        total_rows: int - –æ–±—â–µ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ (–¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ % –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –≤—ã–±—Ä–æ—Å–æ–≤)

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        List[str] - —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö; –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º –Ω–µ—Ç
    """
    issues = []
    n_total = total_rows
    n_missing = series.isna().sum()
    
    if n_missing > 0:
        missing_pct = n_missing / n_total * 100
        issues.append(f"–ø—Ä–æ–ø—É—Å–∫–æ–≤: {missing_pct:.1f}%")
    
    clean_series = series.dropna()
    if clean_series.empty:
        return issues

    # –í—ã–±—Ä–æ—Å—ã
    Q1 = clean_series.quantile(0.25)
    Q3 = clean_series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    n_outliers = ((clean_series < lower_bound) | (clean_series > upper_bound)).sum()
    outliers_pct = n_outliers / n_total * 100
    if outliers_pct > 5:
        issues.append(f"–≤—ã–±—Ä–æ—Å–æ–≤: {outliers_pct:.1f}%")

    # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è
    skewness = clean_series.skew()
    if not pd.isna(skewness) and abs(skewness) > 1.5:
        issues.append(f"–∞—Å–∏–º–º–µ—Ç—Ä–∏—è: {skewness:.2f}")

    # –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π
    if clean_series.nunique() == 1:
        issues.append("–ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π")

    # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º
    max_val = clean_series.max()
    median_val = clean_series.median()
    if median_val > 0 and max_val > median_val * 3 and max_val > 1000:
        issues.append(f"–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º: {int(max_val)}")

    return issues


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# _format_number: –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–∞
def _format_number(x, precision: int) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —á–∏—Å–ª–æ –≤ —Å—Ç—Ä–æ–∫—É —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ —Ç—ã—Å—è—á –∏ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é:
        - –¶–µ–ª—ã–µ —á–∏—Å–ª–∞ (–≤–∫–ª—é—á–∞—è float, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–µ —Ü–µ–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è) –≤—ã–≤–æ–¥—è—Ç—Å—è –±–µ–∑ –¥–µ—Å—è—Ç–∏—á–Ω–æ–π —Ç–æ—á–∫–∏.
        - –í–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –æ–∫—Ä—É–≥–ª—è—é—Ç—Å—è –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π.
        - –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (NaN) –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ "NaN".
        –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ç—ã—Å—è—á (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1,234,567).

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        x: —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–ª–∏ np.nan - –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        precision: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –¥–ª—è –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∏—Å–µ–ª

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        str - –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞, –≥–æ—Ç–æ–≤–∞—è –∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –≤ —è—á–µ–π–∫–µ —Ç–∞–±–ª–∏—Ü—ã
    """
    if pd.isna(x):
        return "NaN"
    if isinstance(x, (int, np.integer)) or (isinstance(x, float) and x.is_integer()):
        return f"{int(x):,}"
    else:
        return f"{x:,.{precision}f}"


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# preview: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
def preview(
    df: pd.DataFrame,
    mode: Literal["auto", "head", "sample", "info", "full"] = "auto",
    n: Optional[int] = None,
    float_precision: int = 3,
    max_header_length: int = 20,
    cmap: Optional[str] = None,
    col: Optional[str] = None,
    random_state: Optional[int] = 42
) -> None:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ó–∞–º–µ–Ω—è–µ—Ç print(df), df.head(), df.info(), audit_categorical_frequencies –≤ EDA:
        - 'auto': –∏–º–∏—Ç–∏—Ä—É–µ—Ç print(df) - head(5) + tail(5) —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        - 'head': –ø–µ—Ä–≤—ã–µ n —Å—Ç—Ä–æ–∫
        - 'sample': —Å–ª—É—á–∞–π–Ω—ã–µ n —Å—Ç—Ä–æ–∫ (–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ –ø—Ä–∏ random_state)
        - 'info': —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —Ç–∏–ø—ã, –ø—Ä–æ–ø—É—Å–∫–∏ (–∫–∞–∫ dataset_profile –≤ —Ä–µ–∂–∏–º–µ 'short')
        - 'full': –≤–µ—Å—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ ‚â§1000 —Å—Ç—Ä–æ–∫)
        - col="breed": –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–∞–Ω–∞–ª–æ–≥ audit_categorical_frequencies)
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç display_table, COLUMN_DESCRIPTIONS, DATASET_DESCRIPTIONS.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞
        mode: Literal["auto", "head", "sample", "info", "full"] - —Ä–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        n: Optional[int] - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ (–¥–ª—è 'head', 'sample'); –µ—Å–ª–∏ None - –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–µ–∂–∏–º–∞
        float_precision: int - –∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
        max_header_length: int - –º–∞–∫—Å. –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        cmap: Optional[str] - –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
        col: Optional[str] - –∏–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–æ—Ç
        random_state: Optional[int] - —Ñ–∏–∫—Å–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –¥–ª—è sample (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 42)

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ display_table –∏–ª–∏ print
    """
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ mode
    if mode in ("info"):
        dataset_profile(df, report="short")


    if mode in ("auto", "head", "sample", "full"):
        dataset_profile(df, report="summary")

        # –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
        if col is not None:
            if col not in df.columns:
                print(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
                return

            series = df[col]
            n_total = len(series)
            n_unique = series.nunique()

            col_name, col_desc = label_for_column(col, separator="‚Ä¢")
            col_label = f"{col_name}{col_desc}" if col_desc else col_name

            print(f"\nüéπ –ß–∞—Å—Ç–æ—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏–π '{col_label}'")
            print(f"üìê –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {n_total:,} √ó {n_unique} –≥—Ä—É–ø–ø")

            value_counts = series.value_counts(sort=True, ascending=False)
            result = pd.DataFrame({
                '–ó–Ω–∞—á–µ–Ω–∏–µ': value_counts.index,
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫': value_counts.values,
                '–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Å—Ç—Ä–æ–∫': (value_counts / n_total * 100).round(3)
            }).reset_index(drop=True)

            styler_func = None
            if cmap is None:
                cmap = 'YlGn'
            styler_func = lambda s: s.background_gradient(
                subset=["–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Å—Ç—Ä–æ–∫"], 
                cmap=cmap
            )

            display_table(
                result,
                rows=len(result),
                float_precision=3,
                max_header_length=1000,
                styler_func=styler_func
            )
            return

        n_rows, n_cols = df.shape

        # –†–µ–∂–∏–º full
        if mode == "full":
            if n_rows > 1000:
                print(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å—Ç—Ä–æ–∫ ({n_rows}) –¥–ª—è —Ä–µ–∂–∏–º–∞ 'full'. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'head' –∏–ª–∏ 'sample'.")
                return
            display_table(df, rows=n_rows, float_precision=float_precision, max_header_length=max_header_length)
            return

        # –†–µ–∂–∏–º auto - –∏–º–∏—Ç–∞—Ü–∏—è print(df)
        if mode == "auto":
            head_tail_n = n if n is not None else 5
            if n_rows <= 2 * head_tail_n:
                display_table(df, rows=n_rows, float_precision=float_precision, max_header_length=max_header_length)
            else:
                head_df = df.head(head_tail_n)
                tail_df = df.tail(head_tail_n)

                # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                def _format_number(x, precision: int) -> str:
                    if pd.isna(x):
                        return "NaN"
                    if isinstance(x, (int, np.integer)) or (isinstance(x, float) and x.is_integer()):
                        return f"{int(x):,}"
                    else:
                        return f"{x:,.{precision}f}"

                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    if col in head_df.columns:
                        head_df = head_df.copy()
                        head_df[col] = head_df[col].apply(lambda x: _format_number(x, float_precision))
                    if col in tail_df.columns:
                        tail_df = tail_df.copy()
                        tail_df[col] = tail_df[col].apply(lambda x: _format_number(x, float_precision))

                # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä–æ–∫—É-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
                separator_index = "‚ãÆ"
                separator_row = pd.DataFrame(
                    [["‚ãÆ"] * len(df.columns)],
                    columns=df.columns,
                    index=[separator_index]
                )

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ë–ï–ó —Å–±—Ä–æ—Å–∞ –∏–Ω–¥–µ–∫—Å–∞ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
                preview_df = pd.concat([head_df, separator_row, tail_df], ignore_index=False)

                print(f"\nüìã –ü–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {head_tail_n} —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
                display_table(
                    preview_df,
                    rows=len(preview_df),
                    float_precision=3,  # —É–∂–µ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤—Ä—É—á–Ω—É—é!
                    max_header_length=max_header_length
                )
            return

        # –†–µ–∂–∏–º—ã head / sample
        if n is None:
            n = 10

        if mode == "sample":
            sample_size = min(n, n_rows)
            df_to_show = df.sample(n=sample_size, random_state=random_state)
            rows_to_show = sample_size
        else:  # "head"
            df_to_show = df.head(n)
            rows_to_show = min(n, n_rows)

        # –°—Ç–∏–ª–∏–∑–∞—Ü–∏—è
        styler_func = None
        if cmap is not None:
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                styler_func = lambda s: s.background_gradient(
                    subset=[col for col in numeric_cols if col in df_to_show.columns],
                    cmap=cmap
                )

        display_table(
            df_to_show,
            rows=rows_to_show,
            float_precision=float_precision,
            max_header_length=max_header_length,
            styler_func=styler_func
        )


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# fn_safe_convert_to_datetime ‚Ä¢ –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü –≤ datetime, —Å–æ—Ö—Ä–∞–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω–∞—Ö.
def dataset_convert_datetime(
    df: pd.DataFrame,
    date_column: str,
    convert_to_utc: bool = True
) -> bool:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –≤ —Ç–∏–ø datetime —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω.

    –û–ø–∏—Å–∞–Ω–∏–µ:
        –§—É–Ω–∫—Ü–∏—è –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–æ–≤—ã–π –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ –≤–∞–ª–∏–¥–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü –≤ datetime.
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —É–¥–∞–ª—è–µ—Ç –∏—Ö (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ naive UTC).
        –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤—ã–≤–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–µ 10 –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - —Ü–µ–ª–µ–≤–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º (–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –º–µ—Å—Ç–µ)
        date_column: str - –∏–º—è —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        convert_to_utc: bool - –µ—Å–ª–∏ True, —É–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –∑–æ–Ω—É –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –æ—Å—Ç–∞–≤–ª—è—è –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤ UTC

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        bool - True –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏, False –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫
    """

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–∞
    if date_column not in df.columns:
        print(f"‚ùå —Å—Ç–æ–ª–±–µ—Ü '{date_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–∏–ø –∏ –ø—Ä–æ–ø—É—Å–∫–∏
    original_dtype = df[date_column].dtype
    initial_na_count = df[date_column].isna().sum()
    print(f"‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–∏–ø: {original_dtype}")
    print(f"‚Ä¢ –ü—Ä–æ–ø—É—Å–∫–æ–≤: {initial_na_count}")

    # –ö–æ–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ä—è–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    original_series = df[date_column].copy()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω - –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ, –±–µ–∑ —Ü–∏–∫–ª–æ–≤
    # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ datetime —Å utc=True - –µ—Å–ª–∏ –µ—Å—Ç—å –∑–æ–Ω—ã, –æ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—Å—è
    test_series = pd.to_datetime(original_series, errors='coerce', utc=True)
    has_timezone = test_series.dt.tz is not None

    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å —É—á—ë—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω
        converted_series = pd.to_datetime(original_series, errors='raise', utc=True)

        if convert_to_utc and has_timezone:
            print("‚ö†Ô∏è –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥–∞—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∑–æ–Ω–∞–º–∏ - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (UTC —É–±—Ä–∞–Ω)")
            converted_series = converted_series.dt.tz_localize(None)
        elif convert_to_utc and not has_timezone:
            print("‚ÑπÔ∏è –¥–∞—Ç—ã –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–æ–Ω—ã - –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–∞–∫ –µ—Å—Ç—å (UTC –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è)")
        elif not convert_to_utc and has_timezone:
            print("‚ÑπÔ∏è —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–æ–Ω—ã - –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ UTC")

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        df[date_column] = converted_series

        print(f"‚úîÔ∏è —Å—Ç–æ–ª–±–µ—Ü '{date_column}' —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ datetime")
        print(f"üíæ –Ω–æ–≤—ã–π —Ç–∏–ø: {df[date_column].dtype}")
        return True

    except (ValueError, TypeError) as e:
        print(f"\nüö® –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ '{date_column}'")
        print("üìù –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–µ—Ä–≤—ã–µ 10):")

        # –ò—â–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ - –±–µ–∑ —Ü–∏–∫–ª–æ–≤
        invalid_mask = pd.to_datetime(original_series, errors='coerce').isna()
        invalid_values = original_series[invalid_mask & original_series.notna()].head(10)
        
        for idx, val in invalid_values.items():
            print(f"  —Å—Ç—Ä–æ–∫–∞ {idx}: [{val}]")

        if len(invalid_values) == 0:
            print("  (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö)")

        print("\n‚ùå –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ - –∏—Å–ø—Ä–∞–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π")
        return False


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# load_dataset: –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–æ–π –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
# UPD: decimal: str = '.',
def load_dataset(
    dataset_name: str,
    file_path: Optional[Union[str, Path]] = None,
    sep: str = ',',
    decimal: str = '.',
    drop_duplicates: bool = False,
    auto_audit_numeric: bool = True,
    replace_whitespace_with_nan: bool = False
) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–æ–π –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞.
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –£–º–Ω–∞—è –∑–∞–º–µ–Ω–∞ pd.read_csv() –¥–ª—è EDA –∏ ML:
        - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–µ—Å—è—Ç–∏—á–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä decimal,
        - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∑–∞–º–µ–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –æ–¥–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ NaN (–µ—Å–ª–∏ replace_whitespace_with_nan=True),
        - –£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ —Å—Ç—Ä–æ–∫–∞—Ö,
        - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –¥–∞—ë—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏,
        - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏,
        - –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á—ë—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ CSV_PATHS –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –ø–æ –∏–º–µ–Ω–∏.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        dataset_name: str - –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∫–ª—é—á –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º CSV_PATHS)
        file_path: Optional[Union[str, Path]] - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - –±–µ—Ä—ë—Ç—Å—è –∏–∑ CSV_PATHS)
        sep: str - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ',')
        decimal: str - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –¥–µ—Å—è—Ç–∏—á–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é '.')
        drop_duplicates: bool - —É–¥–∞–ª–∏—Ç—å –ø–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        auto_audit_numeric: bool - –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        replace_whitespace_with_nan: bool - –∑–∞–º–µ–Ω—è—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–¥–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ NaN (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        pd.DataFrame - –æ—á–∏—â–µ–Ω–Ω—ã–π –∏ –≥–æ—Ç–æ–≤—ã–π –∫ –∞–Ω–∞–ª–∏–∑—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    –ò—Å–∫–ª—é—á–µ–Ω–∏—è:
        KeyError - –µ—Å–ª–∏ dataset_name –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ CSV_PATHS
        FileNotFoundError - –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
        UnicodeDecodeError - –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
    """
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    if file_path is None:
        # –ò—â–µ–º CSV_PATHS –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç —Ç–µ–∫—É—â–µ–≥–æ –º–æ–¥—É–ª—è
        import sys
        current_module = sys.modules[__name__]
        if not hasattr(current_module, 'CSV_PATHS'):
            raise RuntimeError(
                "–ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'CSV_PATHS' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –º–æ–¥—É–ª–µ viz. "
                "–°–æ–∑–¥–∞–π—Ç–µ –µ—ë –≤ –Ω–æ—É—Ç–±—É–∫–µ: `import utils.viz; utils.viz.CSV_PATHS = {...}`"
            )
        CSV_PATHS = getattr(current_module, 'CSV_PATHS')
        if dataset_name not in CSV_PATHS:
            available = ', '.join(CSV_PATHS.keys())
            raise KeyError(
                f"üö® –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ CSV_PATHS | "
                f"üì¢ –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã: {available}"
            )
        file_path = CSV_PATHS[dataset_name]
    file_path = str(file_path)
    if not isinstance(file_path, str):
        raise TypeError(f"üö® 'file_path' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ Path, –ø–æ–ª—É—á–µ–Ω–æ {type(file_path).__name__}")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {os.path.abspath(file_path)}")
    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    try:
        df = pd.read_csv(file_path, sep=sep, decimal=decimal)
        df = df.infer_objects()
        original_rows = len(df)
        original_cols = len(df.columns)
        missing_cols = df.columns[df.isna().any()].tolist()
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ - –≤—ã—á–∏—Å–ª—è–µ–º –û–î–ò–ù –†–ê–ó
        duplicates_full = df[df.duplicated(keep=False)]
        num_duplicates_total = len(duplicates_full)
        unique_duplicate_groups = len(duplicates_full.drop_duplicates())
        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–µ
        file_size = _bytes_to_human_readable(os.path.getsize(file_path))
        print(f"üíæ —Ñ–∞–π–ª '{os.path.basename(file_path)}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")
        print(f"     üß† –ü–∞–º—è—Ç—å          : {file_size}")
        print(f"     üìê –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ : {original_rows} —Å—Ç—Ä–æ–∫ √ó {original_cols} –∫–æ–ª–æ–Ω–æ–∫")
        # 3. –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        candidate_ids = []
        if num_duplicates_total > 0:
            duplicate_ratio = num_duplicates_total / original_rows
            print(f"\nüö® –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {num_duplicates_total} –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫ ({duplicate_ratio:.1%})")
            print(f"     üî¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: [ {unique_duplicate_groups} ]")
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ—è–≤–∏—Ç—Å—è –ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è?
            df_clean_test = df.drop_duplicates()
            for col in df_clean_test.columns:
                if df_clean_test[col].nunique() == len(df_clean_test) and df_clean_test[col].nunique() > 1:
                    candidate_ids.append(col)
            if candidate_ids:
                ids_str = ", ".join(f"`{col}`" for col in candidate_ids[:2])
                print(f"     üîç –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–ª–æ–Ω–∫–∞ {ids_str} —Å—Ç–∞–Ω–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–π - –≤–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä üÜî")
                print("     üí° –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ: –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ—Ç–µ—Ä—è—é—Ç —Å–º—ã—Å–ª–æ–≤—É—é —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å")
            elif duplicate_ratio > 0.5:
                print("     ‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø–æ–≤—Ç–æ—Ä–æ–≤ (>50%) - –≤–µ—Ä–æ—è—Ç–Ω–æ, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏")
                print("     üí° –£–¥–∞–ª–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è")
            elif original_rows < 100:
                print("     üí° –î–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π - –ø–æ–≤—Ç–æ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–º–∏")
            else:
                print("     üí° –ü–æ–≤—Ç–æ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ –æ—à–∏–±–∫–∞–º–∏, —Ç–∞–∫ –∏ –≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
            if not drop_duplicates:
                if len(duplicates_full) <= 50:
                    display_table(duplicates_full, len(duplicates_full))
                else:
                    display_table(duplicates_full, 10)
                print("üì¢ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è: –∑–∞–ø—É—Å—Ç–∏ —Å drop_duplicates=True")
        else:
            print("\n‚úîÔ∏è –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ)
        if drop_duplicates and num_duplicates_total > 0:
            df = df.drop_duplicates(keep='first').reset_index(drop=True)
            cleaned_rows = len(df)
            removed = original_rows - cleaned_rows
            print(f"\nüßπ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫: [ {removed} ] —Å—Ç—Ä–æ–∫ —É–¥–∞–ª–µ–Ω–æ üóëÔ∏è")
            if candidate_ids:
                print(f"     ‚úîÔ∏è –ë–µ–∑–æ–ø–∞—Å–Ω–æ: –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –∫–æ–ª–æ–Ω–∫–∏ {', '.join(f'`{c}`' for c in candidate_ids[:2])} –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å")
            print(f"     üìê –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {cleaned_rows} —Å—Ç—Ä–æ–∫ √ó {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫\n")
        # 4. –£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if auto_audit_numeric:
            potential_numeric_cols = []
            string_cols = df.select_dtypes(include=['object']).columns
            for col in string_cols:
                non_null = df[col].dropna()
                if len(non_null) == 0:
                    continue
                try:
                    # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —á–∏—Å–ª–æ
                    test_series = non_null.astype(str).str.replace(',', '.', regex=False)
                    numeric_test = pd.to_numeric(test_series, errors='coerce')
                    valid_count = numeric_test.notna().sum()
                    total_count = len(non_null)
                    valid_ratio = valid_count / total_count if total_count > 0 else 0
                    if valid_ratio >= 0.9:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ ‚â•90% —á–∏—Å–µ–ª
                        potential_numeric_cols.append({
                            'col': col,
                            'valid_count': valid_count,
                            'total_count': total_count,
                            'valid_ratio': valid_ratio
                        })
                except (ValueError, TypeError):
                    continue
            if potential_numeric_cols:
                print(f"üì¢ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏:")
                for item in potential_numeric_cols:
                    print(f"       ‚Ä¢ {item['col']}: —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ {item['valid_count']} –∏–∑ {item['total_count']} [ {item['valid_ratio']:.1%} ]")
                print()
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ decimal
                if decimal == '.':
                    has_comma_values = any(
                        ',' in str(val) 
                        for col_item in potential_numeric_cols 
                        for val in df[col_item['col']].dropna().head(3)
                        if isinstance(val, str)
                    )
                    if has_comma_values:
                        print("   üí° –°–æ–≤–µ—Ç: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ decimal=',' –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∏—Å–µ–ª —Å –∑–∞–ø—è—Ç—ã–º–∏\n")
            else:
                print("‚úîÔ∏è –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

        # 5. –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∑–∞–º–µ–Ω–∞ "—Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª–æ–≤" –Ω–∞ NaN
        string_cols = df.select_dtypes(include=['object']).columns
        if len(string_cols) > 0:
            whitespace_counts = pd.Series(index=string_cols, dtype='int')
            for col in string_cols:
                only_spaces = df[col].astype(str).str.match(r'^\s*$')
                whitespace_counts[col] = only_spaces.sum()

            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö –í–°–ï–ì–î–ê
            problematic_cols = whitespace_counts[whitespace_counts > 0]
            if not problematic_cols.empty:
                if replace_whitespace_with_nan:
                    print("üßπ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ó–ê–ú–ï–ù–ê –ü–†–û–ë–ï–õ–û–í –ù–ê NaN:")
                    for col in problematic_cols.index:
                        count = whitespace_counts[col]
                        total = len(df)
                        pct = (count / total) * 100
                        print(f"   ‚Ä¢ {col}: {count} ({pct:.2f}%) ‚Üí NaN")
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–º–µ–Ω—É
                    for col in problematic_cols.index:
                        only_spaces = df[col].astype(str).str.match(r'^\s*$')
                        df.loc[only_spaces, col] = np.nan
                else:
                    print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —è—á–µ–π–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã:")
                    for col in problematic_cols.index:
                        count = whitespace_counts[col]
                        total = len(df)
                        pct = (count / total) * 100
                        print(f"     ‚Ä¢ {col}: {count} ({pct:.2f}%)")
                    #print("   üí° –¥–ª—è –∞–≤—Ç–æ–∑–∞–º–µ–Ω—ã –Ω–∞ NaN, –∏—Å–ø–æ–ª—å–∑—É–π - replace_whitespace_with_nan=True")
            else:
                print("‚úîÔ∏è —è—á–µ–µ–∫, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã —É –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–≤—Å–µ–≥–¥–∞)
            for col in string_cols:
                df[col] = df[col].str.strip()
        else:
            print("‚úîÔ∏è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ—Ç - –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        # 6. –û—Ç—á—ë—Ç –æ –ø—Ä–æ–ø—É—Å–∫–∞—Ö
        if missing_cols:
            print("‚ö†Ô∏è –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:")
            for col in missing_cols:
                pct = df[col].isna().sum() / len(df) * 100
                print(f"     ‚Ä¢ {col}: {df[col].isna().sum()} ({pct:.2f}%)")
        else:
            print("‚úîÔ∏è –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤")
        return df
    # 7. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫: –ü–†–û–ë–†–ê–°–´–í–ê–ï–ú –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
    except FileNotFoundError:
        print(f"‚ùå —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {os.path.abspath(file_path)}")
        raise
    except pd.errors.EmptyDataError:
        print("‚ùå —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π")
        raise
    except UnicodeDecodeError:
        print("‚ùå –æ—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ encoding='utf-8' –∏–ª–∏ 'cp1251'")
        raise
    except Exception as e:
        print(f"‚ùå –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {str(e)}")
        raise


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# dataset_profile: –í—ã–≤–æ–¥–∏—Ç –ø—Ä–æ—Ñ–∏–ª—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –æ –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ
def dataset_profile(
    df: pd.DataFrame,
    report: Literal["head", "summary", "cols", "short", "full"] = "head"
) -> None:
    """
    –í—ã–≤–æ–¥–∏—Ç –ø—Ä–æ—Ñ–∏–ª—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –æ –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Å–º–æ—Ç—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ EDA –∏–ª–∏ ML.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 5 —É—Ä–æ–≤–Ω–µ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ - –æ—Ç –∫—Ä–∞—Ç–∫–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ DATASET_DESCRIPTIONS –∏ COLUMN_DESCRIPTIONS
        –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–¥–ø–∏—Å–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        report: Literal["head", "summary", "cols", "short", "full"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏:
            - "head"   : —Ç–æ–ª—å–∫–æ –∏–º—è –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            - "summary": + –ø–∞–º—è—Ç—å, —Ä–∞–∑–º–µ—Ä, —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            - "cols"   : + —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
            - "short"  : –∫–∞–∫ "summary" + –∫–æ–ª–æ–Ω–∫–∏ —Å —ç–º–æ–¥–∑–∏-—Ç–∏–ø–∞–º–∏
            - "full"   : –∫–∞–∫ "short" + –ø—Ä–æ–ø—É—Å–∫–∏, –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –≤—ã–±—Ä–æ—Å—ã

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞
    """
    if report not in ("head", "summary", "cols", "short", "full"):
        raise ValueError(
            f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ report='{report}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 'head', 'summary', 'cols', 'short', 'full'"
        )

    # –¢–∏–ø–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
    numeric_cols = set(df.select_dtypes(include=[np.number]).columns)
    categorical_cols = set(df.select_dtypes(include=["object", "category"]).columns)
    datetime_cols = set(df.select_dtypes(include=["datetime64"]).columns)
    boolean_cols = set(df.select_dtypes(include=["boolean"]).columns)
    string_cols = set(df.select_dtypes(include=["string"]).columns)
    other_cols = (
        set(df.columns) 
        - numeric_cols - categorical_cols - datetime_cols 
        - boolean_cols - string_cols
    )

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    dataset_name, dataset_desc = label_for_dataset(df, separator='‚Ä¢')
    n_rows, n_cols = df.shape
    memory_kb = df.memory_usage(deep=True).sum() / 1024

    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–≤—Å–µ–≥–¥–∞)
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç {dataset_name}{dataset_desc}")

    # –ü–∞–º—è—Ç—å –∏ —Ä–∞–∑–º–µ—Ä (summary, short, full)
    if report in ("summary", "short", "full"):
        print(f"     üß† –ü–∞–º—è—Ç—å                   : {memory_kb:.1f} KB")
        print(f"     üìê –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞          : {n_rows} —Å—Ç—Ä–æ–∫ √ó {n_cols} –∫–æ–ª–æ–Ω–æ–∫")
        if numeric_cols:
            print(f"     üî¢ –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤       : {len(numeric_cols)}")
        if categorical_cols:
            print(f"     üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ : {len(categorical_cols)}")
        if datetime_cols:
            print(f"     üìÖ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏   : {len(datetime_cols)}")
        if boolean_cols:
            print(f"     ‚úÖ –ë—É–ª–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤        : {len(boolean_cols)}")
        if string_cols:
            print(f"     üî§ –¢–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤      : {len(string_cols)}")
        if other_cols:
            print(f"     ‚ö†Ô∏è –ü—Ä–æ—á–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤         : {len(other_cols)}")
            print(f"        –¢–∏–ø—ã: {', '.join(str(df[col].dtype) for col in other_cols)}")
        if n_cols == 0:
            print("     ‚ö†Ô∏è –î–∞—Ç–∞—Å–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–æ–∫")

    # –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if report in ("cols", "short", "full"):
        print("\nüéπ –ö–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞:")

        type_config = {
            'numeric':      ('üî¢', "—á–∏—Å–ª–æ–≤–æ–π"),
            'categorical':  ('üè∑Ô∏è', "–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π"),
            'datetime':     ('üìÖ', "–¥–∞—Ç–∞/–≤—Ä–µ–º—è"),
            'boolean':      ('‚úÖ', "–±—É–ª–µ–≤"),
            'string':       ('üî§', "—Å—Ç—Ä–æ–∫–∞"),
            'other':        ('üì¶', "–ø—Ä–æ—á–∏–π")
        }

        col_type_map = {}
        for col in df.columns:
            if col in numeric_cols:
                col_type_map[col] = 'numeric'
            elif col in categorical_cols:
                col_type_map[col] = 'categorical'
            elif col in datetime_cols:
                col_type_map[col] = 'datetime'
            elif col in boolean_cols:
                col_type_map[col] = 'boolean'
            elif col in string_cols:
                col_type_map[col] = 'string'
            else:
                col_type_map[col] = 'other'

        for col in df.columns:
            col_type = col_type_map[col]
            emoji, _ = type_config[col_type]
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥–ø–∏—Å—å –û–î–ò–ù –†–ê–ó
            if report == "cols":
                col_name, desc = label_for_column(col, separator="-")
                print(f"     ‚Ä¢ [ {col_name} ]{desc}")
            elif report == "short":
                col_name, desc = label_for_column(col, separator="-")
                print(f"     {emoji} {col_name}{desc}")
            elif report == "full":
                col_name, desc = label_for_column(col, separator="‚Ä¢")
                parts = [desc]

                # –ü—Ä–æ–ø—É—Å–∫–∏ - –¥–ª—è –í–°–ï–• —Ç–∏–ø–æ–≤ –∫–æ–ª–æ–Ω–æ–∫
                n_missing = df[col].isna().sum()
                if n_missing > 0:
                    pct = n_missing / len(df) * 100
                    parts.append(f" ‚ö†Ô∏è –ø—Ä–æ–ø—É—Å–∫–æ–≤: {n_missing} ({pct:.1f}%)")

                # –ö–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å - –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö-–ø–æ–¥–æ–±–Ω—ã—Ö
                if col_type in ('categorical', 'string', 'boolean'):
                    n_unique = df[col].nunique()
                    parts.append(f" üíé [–≥—Ä—É–ø–ø: {n_unique}]")
                    if len(df) > 0 and n_unique / len(df) > 0.5:
                        parts.append(" ‚ö†Ô∏è –≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å")

                # –ü—Ä–æ–±–ª–µ–º—ã - –¢–û–õ–¨–ö–û –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö (—á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä)
                if col_type == 'numeric':
                    issues = _detect_numerical_issues(df[col], len(df))
                    if issues:
                        parts.append(f"\n         üìå {' ‚Ä¢ '.join(issues)}")

                print(f"     {emoji} {col_name}{''.join(parts)}")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# dataset_quick_audit: –ü—Ä–æ–≤–æ–¥–∏—Ç –±—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —É–≥–ª—É–±–ª—ë–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
# dataset_quick_audit: –ü—Ä–æ–≤–æ–¥–∏—Ç –±—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —É–≥–ª—É–±–ª—ë–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
def dataset_quick_audit(
    df: pd.DataFrame,
    report: Literal["head", "summary", "short"] = "summary",
    outlier_iqr_multiplier: float = 1.5,
    extreme_iqr_multiplier: float = 3.0,
    detect_outliers: bool = True,
    detect_extremes: bool = True,
) -> None:
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç –±—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —É–≥–ª—É–±–ª—ë–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è, –Ω–æ –ª—ë–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –Ω–∞ —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
        - –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫,
        - –ø—Ä–æ–ø—É—Å–∫–∏ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º,
        - –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã,
        - –≤—ã–±—Ä–æ—Å—ã –∏ —ç–∫—Å—Ç—Ä–µ–º—É–º—ã (IQR √ó outlier_iqr_multiplier, IQR √ó extreme_iqr_multiplier),
        - –∞—Å–∏–º–º–µ—Ç—Ä–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π,
        - –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏,
        - —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (>0.7),
        - –ø—Ä–æ–±–ª–µ–º—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–º—É—Å–æ—Ä, –¥–∏—Å–±–∞–ª–∞–Ω—Å).
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç dataset_profile –¥–ª—è –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

    –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
        outlier_iqr_multiplier : float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.5
            –ú–Ω–æ–∂–∏—Ç–µ–ª—å IQR –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤.
        extreme_iqr_multiplier : float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3.0
            –ú–Ω–æ–∂–∏—Ç–µ–ª—å IQR –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
        detect_outliers : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –≤—ã–±—Ä–æ—Å—ã.
        detect_extremes : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        report: Literal["head", "summary", "short"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
            - "head": —Ç–æ–ª—å–∫–æ –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞,
            - "summary": + –ø–∞–º—è—Ç—å –∏ —Ä–∞–∑–º–µ—Ä,
            - "short": + —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞

    –ü—Ä–∏–º–µ—Ä—ã:
        >>> dataset_quick_audit(df)  # –∫–∞–∫ —Ä–∞–Ω—å—à–µ
        >>> dataset_quick_audit(df, outlier_iqr_multiplier=2.0)  # –º—è–≥—á–µ
        >>> dataset_quick_audit(df, detect_extremes=False)  # –±–µ–∑ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
    """
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ report
    if report not in ("head", "summary", "short"):
        raise ValueError(
            f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ report='{report}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 'head', 'summary', 'short'"
        )

    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    n_rows, n_cols = df.shape
    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_columns = df.select_dtypes(include=["object", "category"]).columns.tolist()

    print(f"–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞\n")

    dataset_profile(df, report=report)
    print()
    
    # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    n_duplicates = df.duplicated().sum()
    if n_duplicates > 0:
        pct = n_duplicates / n_rows * 100
        print(f"üö® –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: {n_duplicates} ({pct:.2f}%)")
    else:
        print("‚úîÔ∏è –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫ –Ω–µ—Ç")

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    missing_any = False
    for col in df.columns:
        col_name, col_desc = label_for_column(col, separator='‚Ä¢')
        n_missing = df[col].isnull().sum()
        if n_missing > 0:
            pct = n_missing / n_rows * 100
            print(f"üö® –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {col_name}{col_desc}: {n_missing} ({pct:.2f}%)")
            missing_any = True
    if not missing_any:
        print("‚úîÔ∏è –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç")

    # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
    id_candidates = []
    for col in df.columns:
        n_uniq = df[col].nunique()
        n_total = len(df)
        pct_unique = n_uniq / n_total * 100

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º float-–∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ—Ü–µ–ª—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        if pd.api.types.is_float_dtype(df[col]):
            non_null = df[col].dropna()
            if len(non_null) > 0 and not (non_null % 1 == 0).all():
                continue  # –ù–µ—Ü–µ–ª—ã–µ float - –ø–ª–æ—Ö–æ–π –∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞ ID

        if pct_unique >= 95.0:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –∏–º—è –∫–æ–ª–æ–Ω–∫–∏
            name_score = 1.0 if any(k in col.lower() for k in ['id', 'key', 'code', 'uid']) else 0.0
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
            type_score = 1.0 if df[col].dtype in ['object', 'int64', 'int32'] else 0.5
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
            unique_score = pct_unique / 100

            score = name_score * 3 + type_score * 2 + unique_score  # –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞

            col_name, col_desc = label_for_column(col, separator='‚Ä¢')
            status = "‚ö†Ô∏è (–Ω–µ —É–Ω–∏–∫–∞–ª–µ–Ω!)" if n_uniq < n_total else ""
            info = f"{col_name}{col_desc} ({n_uniq} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö, {pct_unique:.1f}%){status}"
            id_candidates.append({
                'col': col,
                'info': info,
                'score': score,
                'is_duplicate': n_uniq < n_total
            })

    if id_candidates:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        best = max(id_candidates, key=lambda x: x['score'])
        print(f"üÜî –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {best['info']}")
        if best['is_duplicate']:
            print(f"     üìå –ö–æ–ª–æ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
    else:
        print("‚úîÔ∏è –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

    # –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if numeric_columns:
        # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –º–∞–∫—Å–∏–º—É–º–∞
        def _has_suspicious_max(series: pd.Series) -> bool:
            if series.empty:
                return False
            median_val = series.median()
            max_val = series.max()
            return median_val > 0 and max_val > median_val * 3 and max_val > 1000

        # –í—ã–±—Ä–æ—Å—ã (IQR √ó outlier_iqr_multiplier)
        if detect_outliers:
            outliers_any = False
            for col in numeric_columns:
                col_name, col_desc = label_for_column(col, separator='‚Ä¢')
                valid_data = df[col].dropna()
                if len(valid_data) == 0 or valid_data.nunique() <= 20:
                    continue

                q1, q3 = valid_data.quantile([0.25, 0.75])
                iqr = q3 - q1
                lower_bound = q1 - outlier_iqr_multiplier * iqr
                upper_bound = q3 + outlier_iqr_multiplier * iqr
                n_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                outlier_pct = n_outliers / n_rows * 100

                if n_outliers > 0:
                    print(f"üî∂ –í—ã–±—Ä–æ—Å—ã –≤ {col_name}{col_desc}: {n_outliers} ({outlier_pct:.1f}%)")
                    outliers_any = True
                    if _has_suspicious_max(valid_data):
                        print(f"     üì¢ –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(valid_data.max()):,}")
                else:
                    if _has_suspicious_max(valid_data):
                        print(f"üî∂ –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ {col_name}{col_desc}: {int(valid_data.max()):,}")
                        outliers_any = True

            if not outliers_any:
                print("‚úîÔ∏è –í—ã–±—Ä–æ—Å–æ–≤ –∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")

        # –≠–∫—Å—Ç—Ä–µ–º—É–º—ã (IQR √ó extreme_iqr_multiplier)
        if detect_extremes:
            extremes_any = False
            for col in numeric_columns:
                col_name, col_desc = label_for_column(col, separator='‚Ä¢')
                valid_data = df[col].dropna()
                if len(valid_data) == 0 or valid_data.nunique() <= 20:
                    continue

                q1, q3 = valid_data.quantile([0.25, 0.75])
                iqr = q3 - q1
                lower_bound = q1 - extreme_iqr_multiplier * iqr
                upper_bound = q3 + extreme_iqr_multiplier * iqr
                n_extremes = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                if n_extremes > 0:
                    pct = n_extremes / n_rows * 100
                    print(f"üí• –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {col_name}{col_desc}: {n_extremes} ({pct:.1f}%)")
                    extremes_any = True
                    if _has_suspicious_max(valid_data):
                        print(f"     üì¢ –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(valid_data.max()):,}")
                else:
                    if _has_suspicious_max(valid_data):
                        print(f"üí• –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ {col_name}{col_desc}: {int(valid_data.max()):,}")
                        extremes_any = True

            if not extremes_any:
                print("‚úîÔ∏è –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤ –Ω–µ—Ç")

        # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è
        skew_any = False
        for col in numeric_columns:
            col_name, col_desc = label_for_column(col, separator='‚Ä¢')
            valid_data = df[col].dropna()
            n = len(valid_data)
            
            if n == 0:
                continue
            
            skew_value = stats.skew(valid_data) if n > 2 else np.nan
            
            if pd.isna(skew_value) or abs(skew_value) <= 0.1:
                continue
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —Å–∏–º–≤–æ–ª
            if skew_value > 0:
                if abs(skew_value) > 1.0:
                    symbol = "‚ñ∂‚ñ∂"
                    strength = "—Å–∏–ª—å–Ω–æ"
                elif abs(skew_value) > 0.5:
                    symbol = "‚ñ∂"
                    strength = ""
                else:
                    symbol = "‚ñ∑"
                    strength = "—Å–ª–∞–±–æ"
                direction = "–ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"
            else:
                if abs(skew_value) > 1.0:
                    symbol = "‚óÄ‚óÄ"
                    strength = "—Å–∏–ª—å–Ω–æ"
                elif abs(skew_value) > 0.5:
                    symbol = "‚óÄ"
                    strength = ""
                else:
                    symbol = "‚óÅ"
                    strength = "—Å–ª–∞–±–æ"
                direction = "–ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É
            strength_text = f" {strength}" if strength else ""
            print(f"‚öñÔ∏è –ê—Å–∏–º–º–µ—Ç—Ä–∏—è –≤ {col_name}{col_desc}: {skew_value:.2f} {symbol}{strength_text} {direction}")
            skew_any = True

        if not skew_any:
            print("‚úîÔ∏è –ó–Ω–∞—á–∏–º–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ –Ω–µ—Ç")


        # –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        near_constant_any = False
        for col in numeric_columns:
            col_name, col_desc = label_for_column(col, separator='‚Ä¢')
            n_unique = df[col].nunique()
            if n_unique == 1:
                print(f"üîá –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ {col_name}{col_desc}: –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã")
                near_constant_any = True
            elif n_unique == 2 and len(df) > 10:
                top2_sum = df[col].value_counts().nlargest(2).sum()
                if top2_sum / len(df) > 0.99:
                    print(
                        f"üîá –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ {col_name}{col_desc}: "
                        f"99%+ –∑–Ω–∞—á–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–æ –≤ –¥–≤—É—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö"
                    )
                    near_constant_any = True
        if not near_constant_any:
            print("‚úîÔ∏è –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ—Ç")

        # –°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        if len(numeric_columns) > 1:
            corr_matrix = df[numeric_columns].corr()
            high_corr_found = False
            for i in range(len(numeric_columns)):
                for j in range(i + 1, len(numeric_columns)):
                    r = corr_matrix.iloc[i, j]
                    if 0.7 < abs(r) < 1.0:
                        col1, col2 = numeric_columns[i], numeric_columns[j]
                        col1_name, col1_desc = label_for_column(col1, separator='‚Ä¢')
                        col2_name, col2_desc = label_for_column(col2, separator='‚Ä¢')
                        print(f"üîó –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: '{col1_name}'{col1_desc} ‚ñ∏ {r:.3f} ‚óÇ '{col2_name}'{col2_desc}")
                        high_corr_found = True
            if not high_corr_found:
                print("‚úîÔ∏è –°–∏–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–µ—Ç")
        else:
            print("‚úîÔ∏è –°–∏–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–µ—Ç (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
    else:
        print("‚úîÔ∏è –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π")

    # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if categorical_columns:
        problem_lines = []
        clean_lines = []
        
        for col in categorical_columns:
            col_name, col_desc = label_for_column(col, separator='‚Ä¢')
            n_unique = df[col].nunique()
            n_total = len(df[col])
            issues = []

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            non_null_series = df[col].dropna().astype(str)
            if len(non_null_series) > 0:
                # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–¥–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                whitespace_only_mask = non_null_series.str.strip().eq('')
                n_whitespace_only = whitespace_only_mask.sum()
                if n_whitespace_only > 0:
                    pct_whitespace = n_whitespace_only / n_total * 100
                    issues.append(f"—Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤: {n_whitespace_only} ({pct_whitespace:.2f}%)")

                # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∏—Å–∫–ª—é—á–∞—è —É–∂–µ —É—á—Ç—ë–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã)
                non_whitespace_series = non_null_series[~whitespace_only_mask]
                if len(non_whitespace_series) > 0:
                    junk_mask = non_whitespace_series.str.lower().isin(["null", "n/a", "nan", "none"])
                    n_junk = junk_mask.sum()
                    if n_junk > 0:
                        pct_junk = n_junk / n_total * 100
                        issues.append(f"–º—É—Å–æ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ('null', 'n/a', etc.): {n_junk} ({pct_junk:.2f}%)")

            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            if n_unique == 1:
                issues.append("—Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ - –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å")
            elif n_total > 0:
                top_freq = df[col].value_counts().iloc[0]
                top_pct = top_freq / n_total * 100
                if top_pct > 95:
                    top_val = df[col].value_counts().index[0]
                    issues.append(f"—Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å: '{top_val}' - {top_pct:.1f}%")

            full_name = f"{col_name}{col_desc}"
            if issues:
                problem_lines.append(f"‚ö†Ô∏è {full_name}: {', '.join(issues)}")
            else:
                clean_lines.append(f"üíé —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ {full_name}: {n_unique}")

        # –°–Ω–∞—á–∞–ª–∞ –≤—ã–≤–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—ã
        if problem_lines:
            for line in problem_lines:
                print(line)
        else:
            print("‚úîÔ∏è –ü—Ä–æ–±–ª–µ–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

        # –ü–æ—Ç–æ–º - —á–∏—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        if clean_lines:
            for line in clean_lines:
                print(f'{line}')
    else:
        print("‚úîÔ∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ—Ç")

#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢



# dataset_overview: –î–∞—ë—Ç –ø–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∫ ML
def dataset_overview(
    df: pd.DataFrame,
    report: Literal["summary", "ML"] = "summary",
    show_rows: Optional[int] = None,
    cmap: str = "summer",
    max_unique_values: int = 10
) -> None:
    """
    –î–∞—ë—Ç –ø–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∫ ML.

    –û–ø–∏—Å–∞–Ω–∏–µ:
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è dataset_profile —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
        - –∞–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤,
        - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –±–∏–Ω–∞—Ä–Ω—ã–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —Ç.–¥.),
        - —Å–æ–≤–µ—Ç—ã –ø–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ,
        - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –¥–ª—è ML (–≤ —Ä–µ–∂–∏–º–µ "ML").
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º EDA –∏–ª–∏ ML.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        report: Literal["summary", "ML"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏:
            - "summary": —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –¥—É–±–ª–∏–∫–∞—Ç—ã, –ø—Ä–æ–ø—É—Å–∫–∏, —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,
            - "ML": –∫–∞–∫ "summary" + –∫–æ–ª–æ–Ω–∫–∞ "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ" —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        show_rows: Optional[int] - –ø–æ–∫–∞–∑–∞—Ç—å —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        cmap: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞
    """
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ report
    if report not in ("summary", "ML"):
        raise ValueError(
            f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ report='{report}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 'summary', 'ML'"
        )
    
    df_name, df_desc = label_for_dataset(df, separator="‚Ä¢")

    print(f'–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ {df_name}\n')

    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    n_rows, n_cols = df.shape
    dataset_profile(df, report='summary')
    print()

    # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        total_rows = len(df)
        duplicate_ratio = duplicates / total_rows
        df_clean = df.drop_duplicates()
        unique_combos = len(df_clean)

        print(f"üö® –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏: {duplicates} –∏–∑ {total_rows} ({duplicate_ratio:.1%})")
        print(f"     üî¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {unique_combos}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ—è–≤–∏–ª—Å—è –ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤?
        candidate_ids = []
        for col in df_clean.columns:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            if df_clean[col].nunique() == len(df_clean) and df_clean[col].nunique() > 1:
                candidate_ids.append(col)

        if candidate_ids:
            ids_list = []
            for c in candidate_ids[:3]:
                col_name, col_desc = label_for_column(c, separator="()")
                ids_list.append(f"{col_name}{col_desc}")
            ids_str = ", ".join(ids_list)
            more = f" –∏ –µ—â—ë {len(candidate_ids) - 3}" if len(candidate_ids) > 3 else ""
            print(f"     üîç –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π ID: {ids_str}{more}")
            print(f"     üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —ç—Ç—É –∫–æ–ª–æ–Ω–∫—É –∫–∞–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä")
        elif duplicates == total_rows - 1 and unique_combos == 1:
            print(f"   ‚ö†Ô∏è –í—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã - –≤–æ–∑–º–æ–∂–Ω–æ, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª")
        elif duplicate_ratio > 0.5:
            print(f"   ‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø–æ–≤—Ç–æ—Ä–æ–≤ (>50%) - –≤–µ—Ä–æ—è—Ç–Ω–æ, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã")
            print(f"   üßπ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å")
        elif total_rows < 100:
            print(f"     üí° –î–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π ({total_rows} —Å—Ç—Ä–æ–∫) - –ø–æ–≤—Ç–æ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–º–∏")
            print(f"     üßê –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º")
        else:
            print(f"     üí° –ü–æ–≤—Ç–æ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ –æ—à–∏–±–∫–∞–º–∏, —Ç–∞–∫ –∏ –≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
            print(f"     üßê –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∞–Ω–Ω—ã—Ö")

    else:
        print("‚úîÔ∏è –ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

    # –û–±—â–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
    if n_rows < 100:
        print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π ({n_rows} —Å—Ç—Ä–æ–∫)")
    elif n_rows > 10000:
        print(f"\nüöÄ –í–ù–ò–ú–ê–ù–ò–ï: –¥–∞—Ç–∞—Å–µ—Ç –±–æ–ª—å—à–æ–π ({n_rows} —Å—Ç—Ä–æ–∫)")

    # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    info_data = []
    max_unique_values = 10  # –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏
    for col in df.columns:
        dtype = str(df[col].dtype)
        n_unique = df[col].nunique()
        n_total = len(df)
        ratio = n_unique / n_total

        # –°–ë–û–† –ü–†–û–ë–õ–ï–ú (–∫–∞–∫ —É —Ç–µ–±—è)
        problems = []
        n_missing = df[col].isna().sum()
        if n_missing > 0:
            missing_pct = n_missing / n_total * 100
            problems.append(f"üö® –ø—Ä–æ–ø—É—Å–∫–∏: {n_missing} ({missing_pct:.1f}%)")

        if dtype == "object":
            whitespace_mask = df[col].astype(str).str.match(r'^\s*$') & df[col].notna()
            n_whitespace = whitespace_mask.sum()
            if n_whitespace > 0:
                whitespace_pct = n_whitespace / n_total * 100
                problems.append(f"‚ö†Ô∏è –ø—Ä–æ–±–µ–ª—ã: {n_whitespace} ({whitespace_pct:.1f}%)")

            non_null = df[col].dropna().astype(str)
            junk_mask = non_null.str.lower().isin(['null', 'n/a', 'nan', 'none'])
            if junk_mask.any():
                problems.append("–º—É—Å–æ—Ä: 'null', 'n/a' –∏ –ø–æ–¥–æ–±–Ω–æ–µ")

        if dtype in ("object", "category") and n_unique <= 50:
            top_freq = df[col].value_counts().iloc[0]
            top_pct = top_freq / len(df) * 100
            if top_pct > 95:
                problems.append(f"‚öñÔ∏è –¥–∏—Å–±–∞–ª–∞–Ω—Å: {top_pct:.1f}%")

        if dtype == "object" and n_unique > 50 and ratio < 0.8:
            problems.append("–≤—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å")

        if dtype == "object" and ratio > 0.95:
            problems.append(f"–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ ID: {n_unique}/{n_total} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")

        problems_str = " ".join(problems) if problems else ""

        # –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
        if n_unique == n_total:
            if dtype == "object":
                feature_type = "üÜî —É–Ω–∏–∫–∞–ª—å–Ω—ã–π"
                recommendation = "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ üÜî"
            else:
                feature_type = "üìè —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π"
                recommendation = "—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è - –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –º–æ–¥–µ–ª–∏"
        elif n_unique == 2:
            # –ë–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (–≤–∞–∂–Ω–æ!)
            feature_type = "üíä –±–∏–Ω–∞—Ä–Ω—ã–π"
            unique_vals = set(df[col].dropna().unique())
            if unique_vals <= {0, 1, 0.0, 1.0}:
                recommendation = "–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ 0/1"
            elif unique_vals <= {'Male', 'Female'}:
                recommendation = "–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ 0/1"
            else:
                recommendation = "–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å"
        elif pd.api.types.is_numeric_dtype(df[col]):
            if df[col].dtype in ['int8', 'int16', 'int32', 'int64'] and n_unique <= 20:
                feature_type = "üî¢ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π"
                recommendation = "–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –∏–ª–∏ —á–∏—Å–ª–æ–≤–æ–π"
            else:
                feature_type = "üî¢ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π"
                recommendation = "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ"
        elif dtype == "category":
            feature_type = "üè∑Ô∏è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π"
            recommendation = "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å"
        elif dtype == "object":
            if n_unique <= 2:
                feature_type = "üíä –±–∏–Ω–∞—Ä–Ω—ã–π"
                recommendation = "–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ 0/1"
            elif n_unique <= 20:
                feature_type = "üè∑Ô∏è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π (–Ω–∏–∑–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å)"
                recommendation = "–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π"
            elif n_unique <= 50:
                feature_type = "üè∑Ô∏è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π (—Å—Ä–µ–¥–Ω—è—è)"
                recommendation = "Target Encoding / CatBoost"
            else:
                feature_type = "üìñ –≤—ã—Å–æ–∫–æ–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω—ã–π"
                recommendation = "Hashing, CatBoost, –∏–ª–∏ NLP"
        else:
            feature_type = "‚ùì –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π"
            recommendation = "–ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é"

        # –£–ù–ò–ö–ê–õ–¨–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø (–µ—Å–ª–∏ –º–∞–ª–æ)
        unique_vals_sample = df[col].dropna().unique()
        if len(unique_vals_sample) <= max_unique_values:
            try:
                unique_vals_sorted = sorted(unique_vals_sample, key=str)
            except:
                unique_vals_sorted = unique_vals_sample
            unique_vals_str = ", ".join(map(str, unique_vals_sorted))
        else:
            try:
                sample_vals = sorted(unique_vals_sample[:max_unique_values], key=str)
            except:
                sample_vals = unique_vals_sample[:max_unique_values]
            unique_vals_str = ", ".join(map(str, sample_vals)) + ", ..."

        # –≠–º–æ–¥–∑–∏ –¥–ª—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
        dtype_emoji = {
            "int8": "1Ô∏è‚É£", "int16": "1Ô∏è‚É£", "int32": "1Ô∏è‚É£", "int64": "1Ô∏è‚É£",
            "uint8": "1Ô∏è‚É£", "uint16": "1Ô∏è‚É£", "uint32": "1Ô∏è‚É£", "uint64": "1Ô∏è‚É£",
            "float16": "üî¢", "float32": "üî¢", "float64": "üî¢",
            "object": "üì¶", "datetime64[ns]": "üìÖ", "category": "üè∑Ô∏è"
        }
        dtype_display = f"{dtype_emoji.get(dtype, 'üö®')} {dtype}"

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏
        col_name, col_desc = label_for_column(col, separator='')

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é
        scaling_recommendation = "-"
        if pd.api.types.is_numeric_dtype(df[col]) and n_unique > 1:
            unique_vals = df[col].dropna().unique()
            if set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                scaling_recommendation = "–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–±–∏–Ω–∞—Ä–Ω—ã–π)"
            else:
                q1 = df[col].quantile(0.25)
                q3 = df[col].quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                n_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                has_outliers = n_outliers > 0.05 * n_rows

                min_val, max_val = df[col].min(), df[col].max()
                if min_val >= 0 and max_val <= 1 and (max_val - min_val) <= 1:
                    scaling_recommendation = "–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (—É–∂–µ –≤ [0,1])"
                elif has_outliers:
                    scaling_recommendation = "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ: –≤—ã–±—Ä–æ—Å—ã!)"
                elif (max_val - min_val) > 100:
                    scaling_recommendation = "—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è (—à–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω)"
                else:
                    scaling_recommendation = "–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è"
        else:
            scaling_recommendation = "–Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ"

        info_data.append({
            "–ö–æ–ª–æ–Ω–∫–∞": col_name,
            "–û–ø–∏—Å–∞–Ω–∏–µ": col_desc,
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö": n_unique,
            "–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö": f"{dtype_emoji.get(dtype, 'üö®')} {dtype}",
            "–¢–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞": feature_type,
            "–ü—Ä–æ–±–ª–µ–º—ã": problems_str,
            "–ó–Ω–∞—á–µ–Ω–∏—è": unique_vals_str,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": recommendation,
            "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ": scaling_recommendation
        })

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    info_df = pd.DataFrame(info_data)
    display_columns = list(info_df.columns) if report == "ML" else [col for col in info_df.columns if col not in ["–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ","–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"]]

    display_table(
        info_df[display_columns],
        rows=len(info_df),
        float_precision=0,
        styler_func=lambda s: s.background_gradient(subset=["–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö"], cmap=cmap)
    )

    # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
    if show_rows is not None:
        sample_size = min(show_rows, n_rows)
        print(f"\nüé≤ –°–ª—É—á–∞–π–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ ({sample_size}) –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ {df_name}:")
        display_table(df.sample(n=sample_size, random_state=42), max_header_length=8, rows=sample_size)
        print('')



#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢




def handle_duplicates(
    df: pd.DataFrame,
    action: Literal["check", "clean"] = "check",
    id_col: Optional[str] = None,
    show_samples: int = 0
) -> Optional[pd.DataFrame]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ID-–∫–æ–ª–æ–Ω–∫–∏, –ø–æ–∏—Å–∫–æ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤.

    Workflow:
        1. –£–¥–∞–ª—è–µ—Ç –ø–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫ (–≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è id).
        2. –ï—Å–ª–∏ –∑–∞–¥–∞–Ω id_col:
            a. –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ id_col + –ø—Ä–∏–∑–Ω–∞–∫–∏.
            b. –ù–∞–∑–Ω–∞—á–∞–µ—Ç id_col –∫–∞–∫ –∏–Ω–¥–µ–∫—Å (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ action='clean').
            c. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã: –æ–¥–∏–Ω id ‚Üí —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö.
            d. –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–∏–Ω–¥–µ–∫—Å –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è).
        3. –ï—Å–ª–∏ id_col –Ω–µ –∑–∞–¥–∞–Ω - –∏—â–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (‚â•95% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏).

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df : pd.DataFrame
        action : {"check", "clean"}, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "check"
        id_col : Optional[str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
        show_samples : int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        pd.DataFrame - –µ—Å–ª–∏ action='clean'
        None - –µ—Å–ª–∏ action='check'
    """
    if df.empty:
        print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º")
        return None

    dataset_profile(df, report='summary')
    current_df = df.copy()

    # –®–ê–ì 1: –ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫ (–≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è id)
    total_before_full = len(current_df)
    dup_full = current_df.duplicated(keep=False)
    n_dup_full = dup_full.sum()
    if n_dup_full > 0:
        pct_full = n_dup_full / total_before_full * 100
        print(f"üïµÔ∏è –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫: {n_dup_full} ({pct_full:.3f}%)")
        if show_samples > 0:
            print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
            display_table(current_df[dup_full].head(show_samples), rows=show_samples)
        
        if action == "clean":
            current_df = current_df.drop_duplicates(keep='first').copy()
            print(f"‚úîÔ∏è –£–¥–∞–ª–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –û—Å—Ç–∞–ª–æ—Å—å {len(current_df)} —Å—Ç—Ä–æ–∫\n")
    else:
        print("‚úîÔ∏è –ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n")

    # –®–ê–ì 2: –†–∞–±–æ—Ç–∞ —Å id_col
    final_id_col = id_col
    if id_col is not None:
        col_info = label_for_column(id_col, separator='‚Ä¢', format="string")
        if id_col not in current_df.columns:
            print(f"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ {id_col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥ —Å ID\n")
            final_id_col = None
        else:
            #2a: –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ id_col (–±–µ–∑ —É—á—ë—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
            total_before_id = len(current_df)
            n_unique_ids = current_df[id_col].nunique()
            n_total_ids = len(current_df)
            n_dup_ids = n_total_ids - n_unique_ids
            
            if n_dup_ids > 0:
                pct_dup_ids = n_dup_ids / total_before_id * 100
                print(f"üïµÔ∏è –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ {col_info} (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è ID): {n_dup_ids} ({pct_dup_ids:.3f}%)")
                if show_samples > 0:
                    duplicated_ids = current_df[current_df.duplicated(subset=[id_col], keep=False)][id_col].unique()[:show_samples]
                    sample_df = current_df[current_df[id_col].isin(duplicated_ids)].head(show_samples * 2)
                    print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–æ–∫ —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è ID:")
                    display_table(sample_df.reset_index(drop=True), rows=len(sample_df))
                
                if action == "clean":
                    current_df = current_df.drop_duplicates(subset=[id_col], keep='first').copy()
                    print(f"‚úîÔ∏è –£–¥–∞–ª–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ {col_info}. –û—Å—Ç–∞–ª–æ—Å—å {len(current_df)} —Å—Ç—Ä–æ–∫\n")
            else:
                print(f"‚úîÔ∏è –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ {col_info} (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è ID) –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n")

            #2b: –ù–∞–∑–Ω–∞—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –¢–û–õ–¨–ö–û –µ—Å–ª–∏ action="clean"
            if action == "clean":
                current_df = current_df.set_index(id_col)
                print(f"üÜî –ö–æ–ª–æ–Ω–∫–∞ {col_info} –Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å\n")
            else:
                print(f"üîç –†–µ–∂–∏–º check: –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø–æ {col_info} –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞\n")

            #2c: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø–æ ID (–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞, –Ω–æ –ø–æ-—Ä–∞–∑–Ω–æ–º—É –¥–ª—è check/clean)
            print("üïµÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø–æ ID (–æ–¥–∏–Ω ID ‚Üí —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è):")
            conflict_cols = []
            total_ids = current_df[id_col].nunique() if action == "check" else current_df.index.nunique()

            for col in current_df.columns:
                if action == "check":
                    # –î–ª—è check: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É id
                    n_conflict_ids = (current_df.groupby(id_col)[col].nunique() > 1).sum()
                else:
                    # –î–ª—è clean: –∏–Ω–¥–µ–∫—Å —É–∂–µ id_col
                    n_conflict_ids = (current_df.groupby(current_df.index)[col].nunique() > 1).sum()

                if n_conflict_ids > 0:
                    conflict_cols.append((col, n_conflict_ids))

            if conflict_cols:
                print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤ {len(conflict_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö:")
                for col, n_ids in conflict_cols:
                    pct_conflict = n_ids / total_ids * 100 if total_ids > 0 else 0
                    print(f"      ‚Ä¢ {col}: {n_ids} ID ({pct_conflict:.3f}%) –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")

                if show_samples > 0:
                    print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤:")
                    example_rows = []
                    for col, _ in conflict_cols[:2]:
                        if action == "check":
                            conflict_ids = current_df.groupby(id_col)[col].nunique()
                            conflict_ids = conflict_ids[conflict_ids > 1].index[:show_samples]
                            for id_val in conflict_ids:
                                examples = current_df[current_df[id_col] == id_val]
                                if len(examples) <= 5:
                                    example_rows.append(examples)
                        else:
                            conflict_ids = current_df.groupby(current_df.index)[col].nunique()
                            conflict_ids = conflict_ids[conflict_ids > 1].index[:show_samples]
                            for id_val in conflict_ids:
                                examples = current_df.loc[[id_val]]
                                if len(examples) <= 5:
                                    example_rows.append(examples)
                        if len(example_rows) >= show_samples:
                            break

                    if example_rows:
                        examples_df = pd.concat(example_rows).head(show_samples * 3)
                        display_table(examples_df.reset_index(drop=(action == "clean")), rows=len(examples_df))

                # --- –¢–æ–ª—å–∫–æ –¥–ª—è clean: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ ---
                if action == "clean":
                    print(f"   üßπ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤: –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ ID...")
                    current_df = current_df.groupby(current_df.index).first().copy()
                    print(f"   ‚úîÔ∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã —Ä–∞–∑—Ä–µ—à–µ–Ω—ã. –û—Å—Ç–∞–ª–æ—Å—å {len(current_df)} —Å—Ç—Ä–æ–∫\n")
            else:
                print("    ‚úîÔ∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø–æ ID –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            print()

            #2d: –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–∏–Ω–¥–µ–∫—Å –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è)
            total_before_features = len(current_df)
            dup_features = current_df.duplicated(keep=False)
            n_dup_features = dup_features.sum()
            if n_dup_features > 0:
                pct_features = n_dup_features / total_before_features * 100
                print(f"üïµÔ∏è –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º: {n_dup_features} ({pct_features:.3f}%)")
                if show_samples > 0:
                    print(f"\nüìã –ü—Ä–∏–º–µ—Ä—ã:")
                    display_df = current_df[dup_features].reset_index().head(show_samples)
                    display_table(display_df, rows=len(display_df))
                
                if action == "clean":
                    current_df = current_df.drop_duplicates(keep='first').copy()
                    print(f"    ‚úîÔ∏è –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –û—Å—Ç–∞–ª–æ—Å—å {len(current_df)} —Å—Ç—Ä–æ–∫\n")
            else:
                print("‚úîÔ∏è –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n")

    # –®–ê–ì 3: –ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ (–µ—Å–ª–∏ id_col –Ω–µ –∑–∞–¥–∞–Ω)
    if final_id_col is None:
        print("üïµÔ∏è –ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        total = len(current_df)
        
        if total == 0:
            print("      ‚Ä¢ –î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç\n")
        else:
            candidates = []
            for col in current_df.columns:
                n_uniq = current_df[col].nunique()
                pct_unique = n_uniq / total * 100

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º float-–∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ—Ü–µ–ª—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                if pd.api.types.is_float_dtype(current_df[col]):
                    non_null = current_df[col].dropna()
                    if len(non_null) > 0 and not (non_null % 1 == 0).all():
                        continue  # –ù–µ—Ü–µ–ª—ã–µ float - –ø–ª–æ—Ö–æ–π –∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞ ID

                if pct_unique >= 95.0:
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –∏–º—è –∫–æ–ª–æ–Ω–∫–∏
                    name_score = 1.0 if any(k in col.lower() for k in ['id', 'key', 'code', 'uid']) else 0.0
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
                    type_score = 1.0 if current_df[col].dtype in ['object', 'int64', 'int32'] else 0.5
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                    unique_score = pct_unique / 100

                    score = name_score * 3 + type_score * 2 + unique_score  # –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞

                    candidates.append({
                        'col': col,
                        'n_unique': n_uniq,
                        'pct_unique': pct_unique,
                        'score': score
                    })

            if candidates:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
                best = max(candidates, key=lambda x: x['score'])
                status = "‚ö†Ô∏è (–Ω–µ —É–Ω–∏–∫–∞–ª–µ–Ω!)" if best['n_unique'] < total else ""
                print(f"     üíé –ù–∞–π–¥–µ–Ω –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: '{best['col']}' "
                      f"({best['n_unique']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö, {best['pct_unique']:.3f}%){status}")
                if best['n_unique'] < total:
                    print(f"     üìå –ö–æ–ª–æ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
                # --- –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é ---
                print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ handle_duplicates(df, id_col='{best['col']}', action='clean') –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–æ ID.")
            else:
                print("     üíé –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n")

    # –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if action == "clean":
        return current_df
    else:
        return None


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# audit_numerical: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –∏ –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –≤ —Å—Ç–∏–ª–µ EDA.
def audit_numerical(
    df: pd.DataFrame,
    report: Literal["summary", "full"] = "full",
    include: Optional[List[str]] = None,
    exclude: Optional[List[str]] = None,
    cmap: str = "Oranges"
) -> None:
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∞—É–¥–∏—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –í—ã—è–≤–ª—è–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö:
        - –ø—Ä–æ–ø—É—Å–∫–∏ (>0%),
        - –≤—ã–±—Ä–æ—Å—ã (>5% –ø–æ IQR),
        - —Å–∏–ª—å–Ω—É—é –∞—Å–∏–º–º–µ—Ç—Ä–∏—é (|skewness| > 1.5),
        - –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏,
        - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ –º–∞–∫—Å–∏–º—É–º—ã.
        –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á—ë—Ç –∏ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –º–µ—Ç—Ä–∏–∫.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞—É–¥–∏—Ç–∞
        report: Literal["summary", "full"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏:
            - "summary": —Ç–æ–ª—å–∫–æ —á–µ–∫–ª–∏—Å—Ç –ø—Ä–æ–±–ª–µ–º,
            - "full": + —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞,
        include: Optional[List[str]] - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¢–û–õ–¨–ö–û —ç—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏
        exclude: Optional[List[str]] - –∏—Å–∫–ª—é—á–∏—Ç—å —ç—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
        cmap: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞
    """

     # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ report
    if report not in ("summary", "full"):
        raise ValueError(
            f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ report='{report}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 'summary', 'full'"
        )

    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if include is not None:
        numeric_cols = [col for col in numeric_cols if col in include]
    if exclude is not None:
        numeric_cols = [col for col in numeric_cols if col not in exclude]
    
    if not numeric_cols:
        print("‚úîÔ∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return

    # 2. –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    df_name, df_desc = label_for_dataset(df, separator="‚Ä¢")
    df_label = f"{df_name}{df_desc}" if df_desc else df_name

    print("–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n")
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º: {df_label}")
    print(f"üî¢ –ß–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(numeric_cols)}\n")

    print(f"üìã –ß–µ–∫–ª–∏—Å—Ç:")
    issues_found = False
    all_metric_records = []

    for col in sorted(numeric_cols):
        col_name, col_desc = label_for_column(col, separator="‚Ä¢")
        full_col_name = f"{col_name}{col_desc}"

        series = df[col]
        n_total = len(series)
        n_missing = series.isna().sum()
        missing_pct = (n_missing / n_total * 100) if n_total > 0 else 0

        clean_series = series.dropna()
        if clean_series.empty:
            outliers_pct = 0.0
            skewness = np.nan
        else:
            # –í—ã–±—Ä–æ—Å—ã –ø–æ IQR
            Q1 = clean_series.quantile(0.25)
            Q3 = clean_series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outlier_mask = (clean_series < lower_bound) | (clean_series > upper_bound)
            n_outliers = outlier_mask.sum()
            outliers_pct = (n_outliers / n_total * 100) if n_total > 0 else 0
            
            # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è (skewness)
            skewness = clean_series.skew()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
        issue_details = _detect_numerical_issues(series, n_total)
        has_issues = len(issue_details) > 0

        if has_issues:
            issues_found = True
            print(f"    üö® {full_col_name}")
            print(f"         üìå {' ‚Ä¢ '.join(issue_details)}")
        else:
            print(f"    ‚úîÔ∏è {full_col_name} üíé –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ö–æ—Ä–æ—à–µ–µ")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        if pd.isna(skewness):
            skew_type = "–ù/–î"
        elif abs(skewness) <= 0.5:
            skew_type = "‚âà —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è"
        elif skewness > 0.5:
            skew_type = "–ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è ‚ñ∂"
        else:  # skewness < -0.5
            skew_type = "‚óÄ –ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        all_metric_records.append({
            "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
            "–ü—Ä–æ–ø—É—Å–∫–∏ (%)": missing_pct,
            "–í—ã–±—Ä–æ—Å—ã (%)": outliers_pct,
            "–ê—Å–∏–º–º–µ—Ç—Ä–∏—è": skewness if not pd.isna(skewness) else np.nan,
            "–¢–∏–ø –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏": skew_type,  # ‚Üê –Ω–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞
            "–°—Ä–µ–¥–Ω–µ–µ": clean_series.mean() if not clean_series.empty else np.nan,
            "–ú–µ–¥–∏–∞–Ω–∞": clean_series.median() if not clean_series.empty else np.nan,
            "Std": clean_series.std() if not clean_series.empty else np.nan,
            "–ú–∏–Ω–∏–º—É–º": clean_series.min() if not clean_series.empty else np.nan,
            "–ú–∞–∫—Å–∏–º—É–º": clean_series.max() if not clean_series.empty else np.nan
        })

    if not issues_found:
        print("\n‚úîÔ∏è –í—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞!")
        return

    # 3. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    if report in ("full") and all_metric_records:
        print(f"\nüìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (–≤—Å–µ–≥–æ: {len(all_metric_records)} –∫–æ–ª–æ–Ω–æ–∫):")
        quality_df = pd.DataFrame(all_metric_records)

        display_table(
            quality_df,
            rows=len(quality_df),
            float_precision=3,
            max_header_length = 1000,
            styler_func=lambda s: s.background_gradient(subset=["–í—ã–±—Ä–æ—Å—ã (%)", "–ê—Å–∏–º–º–µ—Ç—Ä–∏—è"], cmap=cmap,
            #low=0.1,   # –¥–∞–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –±—É–¥—É—Ç —Å—Ä–µ–¥–Ω–µ–π —è—Ä–∫–æ—Å—Ç–∏
            #high=0.3   # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ - —Ç—ë–º–Ω—ã–µ, –Ω–æ –Ω–µ —á—ë—Ä–Ω—ã–µ
            )
        )
        

#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# report_numerical_consistency: –ê—É–¥–∏—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –≤ —Å—Ç–∏–ª–µ EDA.
def report_numerical_consistency(
    dataframes: List[pd.DataFrame],
    report: Literal["min", "cols", "full"] = "full",
    plot: bool = False 
) -> None:
    """
    –ê—É–¥–∏—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏

    –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏, –Ω–∞–ª–∏—á–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ A/B-—Ç–µ—Å—Ç–∞—Ö –∏–ª–∏ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        dataframes: list[pd.DataFrame] - —Å–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        col: str - –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        dataset_labels: list[str] - –º–µ—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
        plot: bool - —Ñ–ª–∞–≥ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤
        metric: str - —Ç–∏–ø –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–í—ã–±—Ä–æ—Å—ã (%)", "–ü—Ä–æ–ø—É—Å–∫–∏ (%)")

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        pd.DataFrame - —Ç–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    """
    if not dataframes:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return

    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    dataset_labels = []
    for df in dataframes:
        name, desc = label_for_dataset(df, separator="‚Ä¢")
        label = f"{name}{desc}"
        dataset_labels.append(label)

    # 2. –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –ß–ò–°–õ–û–í–´–ï –∫–æ–ª–æ–Ω–∫–∏
    common_columns = set(dataframes[0].columns)
    for df in dataframes[1:]:
        common_columns &= set(df.columns)
    
    if not common_columns:
        print("üîç –ù–µ—Ç –æ–±—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –º–µ–∂–¥—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏")
        return

    numerical_common = set()
    for col in common_columns:
        is_numeric = all(
            pd.api.types.is_numeric_dtype(df[col])
            for df in dataframes
        )
        if is_numeric:
            numerical_common.add(col)

    if not numerical_common:
        print("‚úîÔ∏è –ù–µ—Ç –æ–±—â–∏—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∞–Ω–∞–ª–∏–∑—É
    print("üìä –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏\n")

    EMOJI_NUMBERS = ["0Ô∏è‚É£", "1Ô∏è‚É£", "2Ô∏è‚É£", "3Ô∏è‚É£", "4Ô∏è‚É£", "5Ô∏è‚É£", "6Ô∏è‚É£", "7Ô∏è‚É£", "8Ô∏è‚É£", "9Ô∏è‚É£", "üîü"]
    print("üóÉÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:")
    for i, label in enumerate(dataset_labels):
        emoji_num = EMOJI_NUMBERS[i+1] if i+1 < len(EMOJI_NUMBERS) else f"{i+1}"
        print(f"{emoji_num} {label}")
    
    print(f'\nüìã –ß–µ–∫–ª–∏—Å—Ç:')
    all_metric_records = []
    issues_found = False

    # 4. –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
    for col in sorted(numerical_common):
        col_name, col_desc = label_for_column(col, separator="‚Ä¢")
        full_col_name = f"{col_name}{col_desc}"

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É
        stats_per_df = []
        has_issues = False

        for i, df in enumerate(dataframes):
            series = df[col]
            n_total = len(series)
            n_missing = series.isna().sum()
            missing_pct = (n_missing / n_total * 100) if n_total > 0 else 0

            # –†–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –Ω–µ–ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            clean_series = series.dropna()
            if clean_series.empty:
                stats = {
                    "mean": np.nan,
                    "median": np.nan,
                    "std": np.nan,
                    "min": np.nan,
                    "max": np.nan,
                    "missing_pct": missing_pct,
                    "outliers_pct": 0.0
                }
            else:
                # –í—ã–±—Ä–æ—Å—ã –ø–æ –º–µ—Ç–æ–¥—É IQR
                Q1 = clean_series.quantile(0.25)
                Q3 = clean_series.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outlier_mask = (clean_series < lower_bound) | (clean_series > upper_bound)
                n_outliers = outlier_mask.sum()
                outliers_pct = (n_outliers / n_total * 100) if n_total > 0 else 0

                stats = {
                    "mean": clean_series.mean(),
                    "median": clean_series.median(),
                    "std": clean_series.std(),
                    "min": clean_series.min(),
                    "max": clean_series.max(),
                    "missing_pct": missing_pct,
                    "outliers_pct": outliers_pct
                }

            stats_per_df.append(stats)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
            all_metric_records.append({
                "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                "–ú–µ—Ç—Ä–∏–∫–∞": "–°—Ä–µ–¥–Ω–µ–µ",
                "–ò—Å—Ç–æ—á–Ω–∏–∫": dataset_labels[i],
                "–ó–Ω–∞—á–µ–Ω–∏–µ": stats["mean"]
            })
            all_metric_records.append({
                "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ú–µ–¥–∏–∞–Ω–∞",
                "–ò—Å—Ç–æ—á–Ω–∏–∫": dataset_labels[i],
                "–ó–Ω–∞—á–µ–Ω–∏–µ": stats["median"]
            })
            all_metric_records.append({
                "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                "–ú–µ—Ç—Ä–∏–∫–∞": "Std",
                "–ò—Å—Ç–æ—á–Ω–∏–∫": dataset_labels[i],
                "–ó–Ω–∞—á–µ–Ω–∏–µ": stats["std"]
            })
            all_metric_records.append({
                "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ü—Ä–æ–ø—É—Å–∫–∏ (%)",
                "–ò—Å—Ç–æ—á–Ω–∏–∫": dataset_labels[i],
                "–ó–Ω–∞—á–µ–Ω–∏–µ": stats["missing_pct"]
            })
            all_metric_records.append({
                "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                "–ú–µ—Ç—Ä–∏–∫–∞": "–í—ã–±—Ä–æ—Å—ã (%)",
                "–ò—Å—Ç–æ—á–Ω–∏–∫": dataset_labels[i],
                "–ó–Ω–∞—á–µ–Ω–∏–µ": stats["outliers_pct"]
            })

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–æ–±–ª–µ–º—ã
        means = [s["mean"] for s in stats_per_df if pd.notna(s["mean"])]
        medians = [s["median"] for s in stats_per_df if pd.notna(s["median"])]

        if means and medians:
            mean_cv = np.std(means) / np.mean(means) if np.mean(means) != 0 else 0
            median_cv = np.std(medians) / np.mean(medians) if np.mean(medians) != 0 else 0

            outlier_pcts = [s["outliers_pct"] for s in stats_per_df]
            max_outliers = max(outlier_pcts) if outlier_pcts else 0
            min_outliers = min(outlier_pcts) if outlier_pcts else 0

            if mean_cv > 0.2 or median_cv > 0.2 or (max_outliers > 5 and max_outliers - min_outliers > 10):
                has_issues = True
                issues_found = True

        if has_issues:
            print(f"üö® {full_col_name} üì¢ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è")
            for i, stats in enumerate(stats_per_df):
                if pd.notna(stats["mean"]):
                    print(f"     üìå {dataset_labels[i][:20]}: —Å—Ä–µ–¥–Ω–µ–µ={stats['mean']:.2f}, –≤—ã–±—Ä–æ—Å—ã={stats['outliers_pct']:.1f}%")
        else:
            print(f"‚úîÔ∏è {full_col_name} üíé —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–∂–∏")

    if not issues_found and not all_metric_records:
        print("\n‚úîÔ∏è –í—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã!")
        return

    # 5. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if report in ("cols", "full") and all_metric_records:
        print(f"\nüìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ (–≤—Å–µ–≥–æ: {len(all_metric_records)} –∑–∞–ø–∏—Å–µ–π):")
        metrics_df = pd.DataFrame(all_metric_records)
        try:
            pivot_df = metrics_df.pivot_table(
                index=["–ö–æ–ª–æ–Ω–∫–∞", "–ú–µ—Ç—Ä–∏–∫–∞"],
                columns="–ò—Å—Ç–æ—á–Ω–∏–∫",
                values="–ó–Ω–∞—á–µ–Ω–∏–µ",
                aggfunc="first"
            ).reset_index()
            display_table(pivot_df, rows=20, max_header_length=20)
        except Exception:
            display_table(metrics_df, rows=15, max_header_length=20)

    # 6. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–º–∞—Ç—Ä–∏—Ü—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)
    if report == "full":
        print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º:")
        for col in sorted(numerical_common):
            col_name, col_desc = label_for_column(col, separator="‚Ä¢")
            full_col_name = f"{col_name}{col_desc}"

            # –ú–∞—Ç—Ä–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫
            matrix_rows = []
            metrics = ["–°—Ä–µ–¥–Ω–µ–µ", "–ú–µ–¥–∏–∞–Ω–∞", "Std", "Min", "Max", "–ü—Ä–æ–ø—É—Å–∫–∏ (%)", "–í—ã–±—Ä–æ—Å—ã (%)"]
            
            for metric in metrics:
                row = {"–ú–µ—Ç—Ä–∏–∫–∞": metric}
                for i, df in enumerate(dataframes):
                    series = df[col].dropna()
                    if series.empty:
                        val = "-"
                    else:
                        if metric == "–°—Ä–µ–¥–Ω–µ–µ":
                            val = series.mean()
                        elif metric == "–ú–µ–¥–∏–∞–Ω–∞":
                            val = series.median()
                        elif metric == "Std":
                            val = series.std()
                        elif metric == "Min":
                            val = series.min()
                        elif metric == "Max":
                            val = series.max()
                        elif metric == "–ü—Ä–æ–ø—É—Å–∫–∏ (%)":
                            missing_pct = (df[col].isna().sum() / len(df) * 100)
                            val = missing_pct
                        elif metric == "–í—ã–±—Ä–æ—Å—ã (%)":
                            Q1 = series.quantile(0.25)
                            Q3 = series.quantile(0.75)
                            IQR = Q3 - Q1
                            bounds = (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)
                            outliers = series[(series < bounds[0]) | (series > bounds[1])]
                            val = (len(outliers) / len(df) * 100)
                        else:
                            val = "-"
                        if isinstance(val, (int, float)) and not pd.isna(val):
                            if metric in ["–ü—Ä–æ–ø—É—Å–∫–∏ (%)", "–í—ã–±—Ä–æ—Å—ã (%)"]:
                                val = f"{val:.1f}"
                            else:
                                val = f"{val:.2f}"
                    row[dataset_labels[i]] = val
                matrix_rows.append(row)

            print(f"\nüéπ –ö–æ–ª–æ–Ω–∫–∞: {full_col_name}")
            matrix_df = pd.DataFrame(matrix_rows)
            display_table(matrix_df, rows=len(matrix_rows), max_header_length=25)

            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–¢–û–õ–¨–ö–û –µ—Å–ª–∏ plot=True –∏ >=2 –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤)
            if plot and len(dataframes) >= 2:
                try:
                    from scipy.stats import probplot, levene, ttest_ind, f_oneway

                    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                    plot_data = []
                    groups = []
                    for i, df in enumerate(dataframes):
                        clean_vals = df[col].dropna()
                        groups.append(clean_vals.values)
                        for val in clean_vals:
                            plot_data.append({
                                "–ó–Ω–∞—á–µ–Ω–∏–µ": val,
                                "–ò—Å—Ç–æ—á–Ω–∏–∫": dataset_labels[i]
                            })
                    
                    if not plot:
                        continue

                    plot_df = pd.DataFrame(plot_data)
                    
                    # 1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã + KDE
                    plt.figure(figsize=(12, 4))
                    
                    plt.subplot(1, 2, 1)
                    sns.histplot(
                        data=plot_df,
                        x="–ó–Ω–∞—á–µ–Ω–∏–µ",
                        hue="–ò—Å—Ç–æ—á–Ω–∏–∫",
                        kde=True,
                        alpha=0.6,
                        stat="count"
                    )
                    plt.title(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {full_col_name}", fontsize=11)
                    plt.xlabel("–ó–Ω–∞—á–µ–Ω–∏–µ")
                    plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
                    plt.grid(True, linestyle='--', alpha=0.5)

                    # 2. QQ-plot
                    plt.subplot(1, 2, 2)
                    colors = sns.color_palette("husl", len(dataframes))
                    for i, (df_label, group) in enumerate(zip(dataset_labels, groups)):
                        if len(group) > 0:
                            probplot(group, dist="norm", plot=plt)
                            lines = plt.gca().get_lines()
                            if lines:
                                lines[-1].set_color(colors[i])
                                lines[-2].set_color(colors[i])
                    plt.title("QQ-plot (–Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å)", fontsize=11)
                    plt.legend([label[:15] for label in dataset_labels], fontsize=8)

                    plt.tight_layout()
                    plt.show()

                    # 3. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
                    valid_groups = [g for g in groups if len(g) > 1]
                    if len(valid_groups) >= 2:
                        # Levene's test
                        w_stat, p_levene = levene(*valid_groups, center='median')
                        print(f"     üìä Levene‚Äôs test (—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –¥–∏—Å–ø–µ—Ä—Å–∏–π): p = {p_levene:.4f}")
                        if p_levene < 0.05:
                            print(f"        ‚ö†Ô∏è  –î–∏—Å–ø–µ—Ä—Å–∏–∏ –∑–Ω–∞—á–∏–º–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è (–≥–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å)")
                        else:
                            print(f"        ‚úîÔ∏è –î–∏—Å–ø–µ—Ä—Å–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–≤–Ω—ã (–≥–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å)")

                        # t-test –∏–ª–∏ ANOVA
                        if len(valid_groups) == 2:
                            t_stat, p_ttest = ttest_ind(valid_groups[0], valid_groups[1], equal_var=False)
                            print(f"     üìä Welch‚Äôs t-test (—Å—Ä–µ–¥–Ω–∏–µ): p = {p_ttest:.4f}")
                            p_vals = [p_levene, p_ttest]
                        else:
                            f_stat, p_anova = f_oneway(*valid_groups)
                            print(f"     üìä ANOVA (—Å—Ä–µ–¥–Ω–∏–µ): p = {p_anova:.4f}")
                            p_vals = [p_levene, p_anova]
                        
                        if any(p < 0.05 for p in p_vals):
                            print(f"        ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è")
                        else:
                            print(f"        ‚úîÔ∏è –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

                except Exception as e:
                    print(f"     ‚ùå –û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏/—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {str(e)}")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# audit_categorical: –ü—Ä–æ–≤–æ–¥–∏—Ç –∞—É–¥–∏—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
def audit_categorical(
    df: pd.DataFrame,
    columns: Optional[List[str]] = None,
    max_distance: int = 2,
    min_frequency: int = 1,
    cmap: str = "Oranges"
) -> None:
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∞—É–¥–∏—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –í—ã—è–≤–ª—è–µ—Ç –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ ‚â§ max_distance),
        –∫–æ—Ç–æ—Ä—ã–µ, –≤–µ—Ä–æ—è—Ç–Ω–æ, —è–≤–ª—è—é—Ç—Å—è –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å—å—é –æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏:
        - –æ–ø–µ—á–∞—Ç–∫–∏ ("–ú–æ—Å–∫–≤–∞" vs "–ú–æ—Å–∫–∞–≤–∞"),
        - —Ä–∞–∑–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä ("Apple" vs "apple"),
        - —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è ("St." vs "Street").
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–∫–ª—é—á–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ.
        –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á—ë—Ç –∏ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≥—Ä—É–ø–ø.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞—É–¥–∏—Ç–∞
        columns: Optional[List[str]] - –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞; –µ—Å–ª–∏ None - –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
        max_distance: int - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)
        min_frequency: int - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —É—á–∞—Å—Ç–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)
        cmap: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞
    """
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if columns is None:
        columns = []
        for col in df.columns:
            # –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ object –∏ category
            if df[col].dtype.name in ('object', 'category'):
                # –ò—Å–∫–ª—é—á–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ
                if not _is_likely_numeric(df[col]):
                    columns.append(col)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    columns = [col for col in columns if col in df.columns]
    
    if not columns:
        print("‚úîÔ∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏")
        return

    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    df_name, df_desc = label_for_dataset(df, separator="‚Ä¢")
    df_label = f"{df_name}{df_desc}" if df_desc else df_name

    print("–ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n")
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º             : {df_label}")
    print(f"üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(columns)}\n")

    print("üìã –ß–µ–∫–ª–∏—Å—Ç:")
    issues_found = False
    all_typo_records = []

    for col in sorted(columns):
        col_name, col_desc = label_for_column(col, separator="‚Ä¢")
        full_col_name = f"{col_name}{col_desc}"

        # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞—á–µ–Ω–∏–π
        value_counts = df[col].dropna().astype(str).value_counts()
        value_counts = value_counts[value_counts >= min_frequency]
        
        has_issues = False
        typo_groups = []

        if len(value_counts) > 1:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            normalized_to_original = {}
            for val, freq in value_counts.items():
                normalized = _normalize_text(str(val))
                if normalized not in normalized_to_original:
                    normalized_to_original[normalized] = []
                normalized_to_original[normalized].append((val, freq))

            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            normalized_values = list(normalized_to_original.keys())
            used = set()
            
            for i, norm_val1 in enumerate(normalized_values):
                if norm_val1 in used:
                    continue
                    
                current_group = normalized_to_original[norm_val1].copy()
                used.add(norm_val1)
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–æ –≤—Å–µ–º–∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–º–∏
                for j in range(i + 1, len(normalized_values)):
                    norm_val2 = normalized_values[j]
                    if norm_val2 in used:
                        continue
                    if _levenshtein_distance(norm_val1, norm_val2) <= max_distance:
                        current_group.extend(normalized_to_original[norm_val2])
                        used.add(norm_val2)
                
                # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è - –ø—Ä–æ–±–ª–µ–º–∞
                if len(set(orig for orig, _ in current_group)) > 1:
                    has_issues = True
                    typo_groups.append(current_group)

        if has_issues:
            issues_found = True
            total_problematic = sum(len(group) for group in typo_groups)
            print(f"    üö® {full_col_name} üì¢ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {total_problematic}")
            
            # –í—ã–≤–æ–¥–∏–º –¥–æ 3 –≥—Ä—É–ø–ø
            for group in typo_groups[:3]:
                originals = [f"{orig}" for orig, freq in group]
                print(f"         üìÑ {originals}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
            for group in typo_groups:
                for orig, freq in group:
                    all_typo_records.append({
                        "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": orig,
                        "–ß–∞—Å—Ç–æ—Ç–∞": freq,
                        "–ì—Ä—É–ø–ø–∞": ", ".join([f"{o}" for o, f in group])
                    })
        else:
            print(f"    ‚úîÔ∏è {full_col_name} üíé –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ö–æ—Ä–æ—à–µ–µ")

    if not issues_found:
        print("\n‚úîÔ∏è –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å!")
        return

    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print(f"\nüìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–≤—Å–µ–≥–æ: {len(all_typo_records)}):")
    typo_df = pd.DataFrame(all_typo_records)
    display_table(
        typo_df, 
        rows=15, 
        max_header_length=25, 
        styler_func=lambda s: s.background_gradient(subset=["–ß–∞—Å—Ç–æ—Ç–∞"], cmap=cmap)
    )
    
    print("\nüõ†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("     ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å")
    print("     ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω")



# 3_analyze_category_frequencies ‚Ä¢ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ.
def audit_categorical_frequencies (
    df: pd.DataFrame,
    col: str,
    cmap: str = 'YlGn',
    show_dataset_info: bool = True,
    force_categorical: bool = True,
    sort_by_value: Optional[Literal["asc", "desc", None]] = None 
) -> None:
    """  Optional[Literal["asc", "desc"]]
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ –∏ –≤—ã–≤–æ–¥–∏—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –§—É–Ω–∫—Ü–∏—è —Å—Ç—Ä–æ–∏—Ç —Ç–∞–±–ª–∏—Ü—É —Å —á–∞—Å—Ç–æ—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è:
        - –ó–Ω–∞—á–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ —Å —ç—Ç–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        - –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Å—Ç—Ä–æ–∫
        
        –¢–∞–±–ª–∏—Ü–∞ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ display_table —Å —Ü–≤–µ—Ç–æ–≤–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–π –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
        –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–º—É —Å—Ç–æ–ª–±—Ü—É. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–ø–æ —Ç–∏–ø—É –∏ —á–∏—Å–ª—É —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
        ‚Ä¢ –¶–≤–µ—Ç–æ–≤–∞—è –ø–æ–¥—Å–≤–µ—Ç–∫–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ cmap (YlGn, Reds, viridis –∏ –¥—Ä.)
        ‚Ä¢ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –ø–æ —á–∞—Å—Ç–æ—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é (–≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ/—É–±—ã–≤–∞–Ω–∏–µ)
        ‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ (DATASET_DESCRIPTIONS, COLUMN_DESCRIPTIONS)
        ‚Ä¢ –ó–∞—â–∏—Ç–∞ –æ—Ç –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ-–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (—Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - —Ü–µ–ª–µ–≤–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        col: str - –∏–º—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–π –∫–æ–ª–æ–Ω–∫–∏
        cmap: str - —Ü–≤–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–π –ø–æ–¥—Å–≤–µ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'YlGn')
        show_dataset_info: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        force_categorical: bool - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∞ —á–∏—Å–ª–æ–≤–∞—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        sort_by_value: Literal["asc", "desc", None] - –ø–æ—Ä—è–¥–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏:
            - "asc": –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é –∑–Ω–∞—á–µ–Ω–∏–π
            - "desc": –ø–æ —É–±—ã–≤–∞–Ω–∏—é –∑–Ω–∞—á–µ–Ω–∏–π  
            - None: –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None (–≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç —á–µ—Ä–µ–∑ display_table)
    
    –ü—Ä–∏–º–µ—Ä—ã:
        >>> audit_categorical_frequencies (df, "breed")
        >>> audit_categorical_frequencies (df, "age_category", sort_by_value="asc")
        >>> audit_categorical_frequencies (df, "numeric_code", force_categorical=True)
    """

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ sort_by_value
    if sort_by_value not in ("asc", "desc", None):
        raise ValueError(
            f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ sort_by_value='{sort_by_value}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 'asc', 'desc'"
        )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–æ–Ω–∫–∏
    if col not in df.columns:
        print(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ DataFrame.")
        return None
    
    series = df[col]
    n_unique = series.nunique()
    n_total = len(series)

    # –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é?
    is_categorical_by_type = pd.api.types.is_object_dtype(series) or pd.api.types.is_categorical_dtype(series)
    is_few_unique = n_unique <= 25 or (n_unique / n_total) < 0.05
    is_likely_categorical = is_categorical_by_type or (pd.api.types.is_numeric_dtype(series) and is_few_unique)

    if not force_categorical and not is_likely_categorical:
        print(f"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ '{col}' (—Ç–∏–ø: {series.dtype}) –Ω–µ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é. "
              f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {n_unique} –∏–∑ {n_total}. "
              f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ force_categorical=True, —á—Ç–æ–±—ã –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É.")
        return None

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_key, dataset_desc = label_for_dataset(df, separator='‚Ä¢')

    # —Ä–∞—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç
    value_counts = series.value_counts(sort=True, ascending=False)  # –°–Ω–∞—á–∞–ª–∞ - –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    total_rows = len(df)

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    result = pd.DataFrame({
        '–ó–Ω–∞—á–µ–Ω–∏–µ': value_counts.index,
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫': value_counts.values,
        '–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Å—Ç—Ä–æ–∫': (value_counts / total_rows * 100).round(3)
    }).reset_index(drop=True)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if sort_by_value == 'asc':
        result = result.sort_values(by='–ó–Ω–∞—á–µ–Ω–∏–µ', ascending=True).reset_index(drop=True)
    elif sort_by_value == 'desc':
        result = result.sort_values(by='–ó–Ω–∞—á–µ–Ω–∏–µ', ascending=False).reset_index(drop=True)
    # –ï—Å–ª–∏ sort_by_value == None - –æ—Å—Ç–∞—ë—Ç—Å—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ (–∫–∞–∫ –±—ã–ª–æ)

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    if show_dataset_info:
        print(f"üóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç '{dataset_key}'{dataset_desc}")
        n_rows, n_cols = df.shape
        memory_kb = df.memory_usage(deep=True).sum() / 1024

    col, desc = label_for_column(col, separator="‚Ä¢")

    # –í—ã–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    header = f"üéπ –ß–∞—Å—Ç–æ—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏–π '{col}'{desc}"
    print(header)
    print(f"üìê –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ {total_rows:,} √ó {n_unique} –≥—Ä—É–ø–ø")

    if not is_likely_categorical:
        print(f"\n‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ '{col}' (—Ç–∏–ø: {series.dtype}) –Ω–µ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é.")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    max_pct = result['–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Å—Ç—Ä–æ–∫'].max()

    display_table(
        result,
        rows=len(result),
        float_precision=3,
        max_header_length = 1000,
        styler_func=lambda s: s.background_gradient(subset=["–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —Å—Ç—Ä–æ–∫"], cmap=cmap)
    )

    return None


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# audit_categorical_cross: –ü—Ä–æ–≤–æ–¥–∏—Ç –∫—Ä–æ—Å—Å-–∞—É–¥–∏—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏
def audit_categorical_cross(
    dataframes: List[pd.DataFrame],
    report: Literal["min", "diff", "full"] = "full",
) -> None:
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç –∫—Ä–æ—Å—Å-–∞—É–¥–∏—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ö–æ—Ç—è –±—ã –≤ –¥–≤—É—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö:
        - –≤—ã—è–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–µ –≤–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö,
        - –Ω–∞—Ö–æ–¥–∏—Ç –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –∑–∞–ø–∏—Å—å –æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ (–æ–ø–µ—á–∞—Ç–∫–∏, —Ä–µ–≥–∏—Å—Ç—Ä, —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è),
        - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –≤ –∫–∞–∫–∏—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ DATASET_DESCRIPTIONS –∏ COLUMN_DESCRIPTIONS
        –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–¥–ø–∏—Å–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        dataframes: List[pd.DataFrame] - —Å–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        report: Literal["min", "diff", "full"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏:
            - "min": —Ç–æ–ª—å–∫–æ —Å–≤–æ–¥–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏,
            - "diff": + —Ç–∞–±–ª–∏—Ü–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π,
            - "full": + –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –ø–æ –∫–∞–∂–¥–æ–π –ø—Ä–æ–±–ª–µ–º–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞
    """
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ report
    if report not in ("min", "diff", "full"):
        raise ValueError(
            f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ report='{report}'. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 'min', 'diff', 'full'"
        )
    
    if not dataframes:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã
    non_empty_dfs = [df for df in dataframes if not df.empty]
    if not non_empty_dfs:
        print("‚ö†Ô∏è –í—Å–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –ø—É—Å—Ç—ã–µ")
        return
    dataframes = non_empty_dfs

    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    dataset_labels = []
    for df in dataframes:
        name, desc = label_for_dataset(df, separator="‚Ä¢")
        label = f"{name}{desc}"
        dataset_labels.append(label)

    # 2. –°–æ–±–∏—Ä–∞–µ–º, –≤ –∫–∞–∫–∏—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö –µ—Å—Ç—å –∫–∞–∂–¥–∞—è –∫–æ–ª–æ–Ω–∫–∞
    col_to_dfs: DefaultDict[str, List[Tuple[int, pd.DataFrame]]] = defaultdict(list)
    for idx, df in enumerate(dataframes):
        for col in df.columns:
            col_to_dfs[col].append((idx, df))

    # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ö–æ—Ç—è –±—ã –≤ –¥–≤—É—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö
    candidate_cols = {col for col, info in col_to_dfs.items() if len(info) >= 2}

    if not candidate_cols:
        print("üîç –ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ö–æ—Ç—è –±—ã –≤ –¥–≤—É—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö")
        return

    # 3. –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ (object/category) –∏ –Ω–µ —á–∏—Å–ª–æ–≤—ã–µ
    categorical_common = set()
    for col in candidate_cols:
        dfs_with_col = [df for _, df in col_to_dfs[col]]
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–æ –≤—Å–µ—Ö —ç—Ç–∏—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö —Ç–∏–ø - object –∏–ª–∏ category
        is_object_or_cat = all(
            df[col].dtype.name in ('object', 'category')
            for df in dfs_with_col
        )
        if is_object_or_cat:
            is_numeric = any(_is_likely_numeric(df[col]) for df in dfs_with_col)
            if not is_numeric:
                categorical_common.add(col)

    if not categorical_common:
        print("‚úîÔ∏è –í—Å–µ –æ–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã - —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –Ω–µ—Ç")
        return

    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∞–Ω–∞–ª–∏–∑—É
    print("–ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞–º–∏\n")

    EMOJI_NUMBERS = ["0Ô∏è‚É£", "1Ô∏è‚É£", "2Ô∏è‚É£", "3Ô∏è‚É£", "4Ô∏è‚É£", "5Ô∏è‚É£", "6Ô∏è‚É£", "7Ô∏è‚É£", "8Ô∏è‚É£", "9Ô∏è‚É£", "üîü"]
    print("üóÉÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:")
    for i, label in enumerate(dataset_labels):
        emoji_num = EMOJI_NUMBERS[i+1] if i+1 < len(EMOJI_NUMBERS) else f"{i+1}"
        print(f"    {emoji_num} {label}")
    
    print(f'\nüìã –ß–µ–∫–ª–∏—Å—Ç:')
    all_diff_records = []

    # 5. –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∫–æ–ª–æ–Ω–∫–∏
    for col in sorted(categorical_common):
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã –∏ –∏–Ω–¥–µ–∫—Å—ã, –≥–¥–µ –µ—Å—Ç—å —ç—Ç–∞ –∫–æ–ª–æ–Ω–∫–∞
        relevant_info = col_to_dfs[col]  # list of (original_idx, df)
        relevant_dfs = [df for _, df in relevant_info]
        relevant_indices = [idx for idx, _ in relevant_info]
        relevant_labels = [dataset_labels[idx] for idx in relevant_indices]

        # –°–æ–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤
        raw_sets: List[Set[str]] = []
        for df in relevant_dfs:
            vals = set(df[col].dropna().astype(str))
            raw_sets.append(vals)

        all_raw = set().union(*raw_sets)
        common_raw = set.intersection(*raw_sets) if raw_sets else set()
        diff_raw = all_raw - common_raw

        col_name, col_desc = label_for_column(col, separator="‚Ä¢")
        full_col_name = f"{col_name}{col_desc}"

        if not diff_raw:
            print(f"    ‚úîÔ∏è {full_col_name} üíé –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö")
            continue

        print(f"    üö® {full_col_name} üì¢ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ/–ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        norm_sets = [{_normalize_text(v) for v in s} for s in raw_sets]
        all_norm = set().union(*norm_sets)
        common_norm = set.intersection(*norm_sets) if norm_sets else set()

        if len(all_norm) < len(all_raw):
            print(f"        üìå –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å—Ç–∞–ª–æ –±–æ–ª—å—à–µ: {len(common_norm)} –∏–∑ {len(all_norm)}")
        else:
            print(f"        üìå –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {len(all_raw)} (—Å–æ–≤–ø–∞–¥–∞–µ—Ç {len(common_raw)})")

        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å–∏ –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
        for val in diff_raw:
            sources_indices = [i for i, s in enumerate(raw_sets) if val in s]
            sources_labels = [relevant_labels[i] for i in sources_indices]
            norm_val = _normalize_text(val)
            all_diff_records.append({
                "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                "–û—Ä–∏–≥–∏–Ω–∞–ª": val,
                "–ù–æ—Ä–º. –∑–Ω–∞—á–µ–Ω–∏–µ": norm_val,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": " | ".join(sources_labels)
            })

        # Fuzzy-–ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º –≤ —ç—Ç–æ–π –∫–æ–ª–æ–Ω–∫–µ
        all_unique_vals = all_raw
        typo_groups = _find_typo_groups(all_unique_vals, max_distance=2)
        
        if typo_groups:
            print(f"        üßê –í–æ–∑–º–æ–∂–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏:")
            for group in typo_groups[:3]:
                print(f"             üìÑ {group}")

    # 6. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if not all_diff_records:
        print("\n‚úîÔ∏è –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã!")
        return

    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    if report in ("diff", "full"):
        print(f"\n–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π (–≤—ã—è–≤–ª–µ–Ω–æ: {len(all_diff_records)} —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π):")
        diff_df = pd.DataFrame(all_diff_records)
        display_table(diff_df, rows=15, max_header_length=20)

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–º–∞—Ç—Ä–∏—Ü—ã)
    if report == "full":
        print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º:")
        for col in sorted(categorical_common):
            relevant_info = col_to_dfs[col]
            relevant_dfs = [df for _, df in relevant_info]
            relevant_indices = [idx for idx, _ in relevant_info]
            relevant_labels = [dataset_labels[idx] for idx in relevant_indices]

            raw_sets = [set(df[col].dropna().astype(str)) for df in relevant_dfs]
            diff_raw = set().union(*raw_sets) - set.intersection(*raw_sets)
            if not diff_raw:
                continue

            col_name, col_desc = label_for_column(col, separator="‚Ä¢")
            full_col_name = f"{col_name}{col_desc}"
            print(f"\nüéπ –ö–æ–ª–æ–Ω–∫–∞: {full_col_name}")

            # –°–æ–±–∏—Ä–∞–µ–º: norm_val -> {local_index: (–æ—Ä–∏–≥–∏–Ω–∞–ª, —á–∞—Å—Ç–æ—Ç–∞)}
            norm_to_stats = {}
            for local_i, df in enumerate(relevant_dfs):
                value_counts = df[col].dropna().astype(str).value_counts()
                for val, count in value_counts.items():
                    norm_val = _normalize_text(val)
                    if norm_val not in norm_to_stats:
                        norm_to_stats[norm_val] = {}
                    if local_i not in norm_to_stats[norm_val]:
                        norm_to_stats[norm_val][local_i] = (val, count)
                    else:
                        prev_val, prev_count = norm_to_stats[norm_val][local_i]
                        norm_to_stats[norm_val][local_i] = (prev_val, prev_count + count)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
            matrix_rows = []
            for norm_val in sorted(norm_to_stats.keys()):
                row = {"–ù–æ—Ä–º. –∑–Ω–∞—á–µ–Ω–∏–µ": norm_val}
                for local_i, label in enumerate(relevant_labels):
                    if local_i in norm_to_stats[norm_val]:
                        orig, count = norm_to_stats[norm_val][local_i]
                        row[label] = f"{orig} ({count})"
                    else:
                        row[label] = "-"
                matrix_rows.append(row)

            if matrix_rows:
                matrix_df = pd.DataFrame(matrix_rows)
                display_table(matrix_df, rows=len(matrix_rows), max_header_length=30)


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


#audit_categorical_typos: –í—ã—è–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
def audit_categorical_typos(
    df: pd.DataFrame,
    columns: Optional[List[str]] = None,
    max_distance: int = 2,
    min_frequency: int = 1,
    cmap="Oranges"
) -> None:
    """
    –í—ã—è–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.

    –û–ø–∏—Å–∞–Ω–∏–µ: –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –Ω–∞–ª–∏—á–∏—è –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π, 
            –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–∫–ª—é—á–∞–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π.
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–≤–æ–¥—è—Ç—Å—è –≤ –≤–∏–¥–µ –æ—Ç—á–µ—Ç–∞ –∏ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –î–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        columns: Optional[List[str]] - –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å. 
            –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω (None), –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
        max_distance: int - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏—Ö –≤ –≥—Ä—É–ø–ø—É –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
        min_frequency: int - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–µ, —á—Ç–æ–±—ã –æ–Ω–æ –±—ã–ª–æ –≤–∫–ª—é—á–µ–Ω–æ –≤ –∞–Ω–∞–ª–∏–∑.
        cmap: str - –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã).

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –§—É–Ω–∫—Ü–∏—è –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —è–≤–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è, –∞ –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö –∏ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –≤ stdout.
    """
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if columns is None:
        candidate_cols = []
        for col in df.columns:
            if df[col].dtype.name in ('object', 'category'):
                if not _is_likely_numeric(df[col]):
                    candidate_cols.append(col)
        columns = candidate_cols

    columns = [col for col in columns if col in df.columns]
    
    if not columns:
        print("‚úîÔ∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–ø–µ—á–∞—Ç–æ–∫")
        return

    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    df_name, df_desc = label_for_dataset(df, separator="‚Ä¢")
    df_label = f"{df_name}{df_desc}" if df_desc else df_name

    print("–ê–Ω–∞–ª–∏–∑ –æ–ø–µ—á–∞—Ç–æ–∫ –≤–Ω—É—Ç—Ä–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞\n")
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º             : {df_label}")
    print(f"üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(columns)}\n")

    print("üìã –ß–µ–∫–ª–∏—Å—Ç:")
    issues_found = False
    all_typo_records = []

    for col in sorted(columns):
        col_name, col_desc = label_for_column(col, separator="‚Ä¢")
        full_col_name = f"{col_name}{col_desc}"

        value_counts = df[col].dropna().astype(str).value_counts()
        value_counts = value_counts[value_counts >= min_frequency]
        
        has_issues = False
        typo_groups = []

        if len(value_counts) > 1:
            values = value_counts.index.tolist()
            clean_value_map = {}
            for val in values:
                clean_val = re.sub(r'\s+', ' ', str(val).strip().lower())
                clean_value_map[clean_val] = val

            candidates_clean = list(clean_value_map.keys())
            used_clean = set()

            for i, v1_clean in enumerate(candidates_clean):
                if v1_clean in used_clean:
                    continue
                group_originals = [clean_value_map[v1_clean]]
                used_clean.add(v1_clean)
                for v2_clean in candidates_clean[i + 1:]:
                    if v2_clean in used_clean:
                        continue
                    if _levenshtein_distance(v1_clean, v2_clean) <= max_distance:
                        group_originals.append(clean_value_map[v2_clean])
                        used_clean.add(v2_clean)
                if len(group_originals) > 1:
                    has_issues = True
                    group_with_freq = [(orig, value_counts.get(orig, 0)) for orig in group_originals]
                    typo_groups.append(group_with_freq)

        if has_issues:
            issues_found = True
            total_problematic = sum(len(group) for group in typo_groups)
            print(f"    üö® {full_col_name} üì¢ –æ–ø–µ—á–∞—Ç–æ–∫: {total_problematic}")
            
            # –í—ã–≤–æ–¥–∏–º –≥—Ä—É–ø–ø—ã –°–†–ê–ó–£ –ø–æ–¥ –∫–æ–ª–æ–Ω–∫–æ–π
            for group_with_freq in typo_groups[:3]:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º 3 –≥—Ä—É–ø–ø–∞–º–∏
                originals = [f"{orig}" for orig, freq in group_with_freq]
                print(f"         üìÑ {originals}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
            for group_with_freq in typo_groups:
                for orig, freq in group_with_freq:
                    all_typo_records.append({
                        "–ö–æ–ª–æ–Ω–∫–∞": full_col_name,
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": orig,
                        "–ß–∞—Å—Ç–æ—Ç–∞": freq,
                        "–ì—Ä—É–ø–ø–∞": ", ".join([f"{o}" for o, f in group_with_freq])
                    })
        else:
            print(f"    ‚úîÔ∏è {full_col_name} üíé –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ö–æ—Ä–æ—à–µ–µ")

    if not issues_found:
        print("\n‚ú® –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∏!")
        return

    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print(f"\nüìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –æ–ø–µ—á–∞—Ç–æ–∫ (–≤—ã—è–≤–ª–µ–Ω–æ –æ–ø–µ—á–∞—Ç–æ–∫: {len(all_typo_records)}):")
    typo_df = pd.DataFrame(all_typo_records)
    display_table(typo_df, 
               rows=15, 
               max_header_length=25, 
               styler_func=lambda s: s.background_gradient(subset=["–ß–∞—Å—Ç–æ—Ç–∞"], cmap=cmap)
            )
    
    print("\nüõ†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("     ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∏")
    print("     ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# audit_numerical_distribution ‚Ä¢ –í—ã–≤–æ–¥–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
def audit_numerical_distribution(
    df: pd.DataFrame, 
    col: str,
    show_recommendations: bool = False
) -> None:
    """
    –í—ã–≤–æ–¥–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.

    –û—Ç—á—ë—Ç –≤–∫–ª—é—á–∞–µ—Ç:
        - –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞, std, –º–∏–Ω/–º–∞–∫—Å, –∫–≤–∞—Ä—Ç–∏–ª–∏)
        - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ (skewness) –∏ —ç–∫—Å—Ü–µ—Å—Å–∞ (kurtosis)
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç –≤—ã–±—Ä–æ—Å–æ–≤ (–ø–æ –ø—Ä–∞–≤–∏–ª—É 1.5—ÖIQR)
        - –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ –ø—Ä–∞–≤–∏–ª—É 3—ÖIQR)
        - –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–º–∞–∫—Å–∏–º—É–º > –º–µ–¥–∏–∞–Ω—ã * 3 –∏ > 1000)
        - –ê–Ω–∞–ª–∏–∑ –Ω–∞ –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—Å—Ç—å
        - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ñ–æ—Ä–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
   -------
    df : pd.DataFrame
        –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
    col : str
        –ù–∞–∑–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
    show_recommendations : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False
        –ï—Å–ª–∏ True - –≤—ã–≤–æ–¥–∏—Ç —Ä–∞–∑–¥–µ–ª —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
        –ï—Å–ª–∏ False - –≤—ã–≤–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç
   ----
    None
        –í—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –≤ –∫–æ–Ω—Å–æ–ª—å.
    """

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏–º–µ–Ω–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name, dataset_desc = label_for_dataset(df)
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' üìã {dataset_desc}")

    if col not in df.columns:
        print(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    if not is_numeric_dtype(df[col]):
        print(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤–æ–π")
        return

    data = df[col].dropna()
    n_total = len(df[col])
    n_valid = len(data)
    n_missing = n_total - n_valid

    # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    col_name, col_desc = label_for_column(col, separator='‚Ä¢')

    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {col}{col_desc}")
    print(f"     ‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {n_valid:,} (–ø—Ä–æ–ø—É—Å–∫–æ–≤: {n_missing})")
    print(f"     ‚Ä¢ –¢–∏–ø: {df[col].dtype}")

    if n_valid == 0:
        print("   ‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return

    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    min_val = data.min()
    max_val = data.max()
    mean_val = data.mean()
    median_val = data.median()
    std_val = data.std()
    skew_val = stats.skew(data)
    kurt_val = stats.kurtosis(data)  # —ç–∫—Å—Ü–µ—Å—Å (0 = –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ)

    print(f"üìà –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:")
    print(f"     ‚Ä¢ –ú–∏–Ω–∏–º—É–º: {min_val:.3f} | –ú–∞–∫—Å–∏–º—É–º: {max_val:.3f}")
    print(f"     ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {mean_val:.3f} | –ú–µ–¥–∏–∞–Ω–∞: {median_val:.3f} üíé —Å–º–µ—â–µ–Ω–∏–µ: {mean_val - median_val:+.3f}")
    print(f"     ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_val:.3f}")

    # –ö–≤–∞—Ä—Ç–∏–ª–∏ –∏ IQR
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    print(f"üìä –ö–≤–∞—Ä—Ç–∏–ª–∏:")
    print(f"     ‚Ä¢ Q1 (25%): {q1:.3f}")
    print(f"     ‚Ä¢ Q2 (50% / –º–µ–¥–∏–∞–Ω–∞): {median_val:.3f}")
    print(f"     ‚Ä¢ Q3 (75%): {q3:.3f}")
    print(f"     ‚Ä¢ IQR: {iqr:.3f}")

    # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è –∏ —ç–∫—Å—Ü–µ—Å—Å
    skew_desc = (
        "—Å–∏–ª—å–Ω–∞—è ‚ñ∂ –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è" if skew_val > 1 else
        "—É–º–µ—Ä–µ–Ω–Ω–∞—è ‚ñ∂ –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è" if skew_val > 0.5 else
        "—É–º–µ—Ä–µ–Ω–Ω–∞—è ‚óÄ –ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è" if skew_val < -0.5 else
        "—Å–∏–ª—å–Ω–∞—è ‚óÄ –ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è" if skew_val < -1 else
        "–±–ª–∏–∑–∫–∞ –∫ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–π"
    )
    print(f"‚öñÔ∏è –ê—Å–∏–º–º–µ—Ç—Ä–∏—è (skew): {skew_val:.2f} üíé {skew_desc}")

    kurt_desc = (
        "—Å–∏–ª—å–Ω–æ –æ—Å—Ç—Ä–æ–≤–µ—Ä—Ö–æ–µ" if kurt_val > 1 else
        "–æ—Å—Ç—Ä–æ–≤–µ—Ä—Ö–æ–µ" if kurt_val > 0 else
        "–ø–ª–æ—Å–∫–æ–≤–µ—Ä—Ö–æ–µ" if kurt_val < -0.5 else
        "–±–ª–∏–∑–∫–æ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É"
    )
    print(f"üìâ –≠–∫—Å—Ü–µ—Å—Å (kurtosis): {kurt_val:.2f} üíé {kurt_desc}")

    # –í—ã–±—Ä–æ—Å—ã –ø–æ –ø—Ä–∞–≤–∏–ª—É 1.5√óIQR
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers_low = data[data < lower_bound]
    outliers_high = data[data > upper_bound]
    n_outliers = len(outliers_low) + len(outliers_high)
    pct_outliers = n_outliers / n_valid * 100

    if n_outliers > 0:
        print(f"üî∂ –í—ã–±—Ä–æ—Å—ã (–ø–æ –ø—Ä–∞–≤–∏–ª—É 1.5√óIQR): {n_outliers} ({pct_outliers:.1f}%)")
        print(f"     ‚Ä¢ –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞  : {lower_bound:.3f}")
        print(f"     ‚Ä¢ –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ : {upper_bound:.3f}")
        print(f"     ‚Ä¢ –ù–∏–∂–µ –Ω–∏–∂–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã : {len(outliers_low)}")
        print(f"     ‚Ä¢ –í—ã—à–µ –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã: {len(outliers_high)}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if (median_val > 0 and 
            max_val > median_val * 3 and 
            max_val > 1000):
            print(f"     ‚Ä¢ üö® –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(max_val):,}")
    else:
        print(f"‚úîÔ∏è –í—ã–±—Ä–æ—Å–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ –ø—Ä–∞–≤–∏–ª—É 3√óIQR)
    lower_extreme = q1 - 3 * iqr
    upper_extreme = q3 + 3 * iqr
    extremes_low = data[data < lower_extreme]
    extremes_high = data[data > upper_extreme]
    n_extremes = len(extremes_low) + len(extremes_high)
    pct_extremes = n_extremes / n_valid * 100

    if n_extremes > 0:
        print(f"üí• –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ –ø—Ä–∞–≤–∏–ª—É 3√óIQR): {n_extremes} ({pct_extremes:.1f}%)")
        print(f"     ‚Ä¢ –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞  : {lower_extreme:.3f}")
        print(f"     ‚Ä¢ –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ : {upper_extreme:.3f}")
        print(f"     ‚Ä¢ –ù–∏–∂–µ –Ω–∏–∂–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã : {len(extremes_low)}")
        print(f"     ‚Ä¢ –í—ã—à–µ –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã: {len(extremes_high)}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if (median_val > 0 and 
            max_val > median_val * 3 and 
            max_val > 1000):
            print(f"      üö® –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {int(max_val):,}")
    else:
        print(f"‚úîÔ∏è –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

    # –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—Å—Ç—å
    n_unique = data.nunique()
    if n_unique == 1:
        print(f"üîá –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫: –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã ({min_val})")
    elif n_unique == 2 and len(data) > 10:
        top2_sum = data.value_counts().nlargest(2).sum()
        if top2_sum / len(data) > 0.99:
            top_vals = data.value_counts().nlargest(2).index.tolist()
            print(f"üîá –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫: 99%+ –∑–Ω–∞—á–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–æ –≤ –¥–≤—É—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö: {top_vals}")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if show_recommendations:
        print(f"\nüîç –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")

        if abs(skew_val) > 0.5:
            print("     ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—à–µ–Ω–æ üì¢ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ª–æ–≥-–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.")
        else:
            print("     ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–ª–∏–∑–∫–æ –∫ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–º—É.")

        if n_outliers > 0 or n_extremes > 0:
            print("     ‚Ä¢ –ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—ã–±—Ä–æ—Å—ã/—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è üì¢ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Ö –ø—Ä–∏—Ä–æ–¥—É (–æ—à–∏–±–∫–∞ –∏–ª–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å?).")
        else:
            print("     ‚Ä¢ –í—ã–±—Ä–æ—Å–æ–≤ –∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")

        if std_val > 0:
            if max_val - min_val > 100:
                print("     ‚Ä¢ –®–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π üì¢ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è ML.")
            elif max_val <= 1 and min_val >= 0:
                print("     ‚Ä¢ –î–∞–Ω–Ω—ã–µ –≤ [0,1] - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ, –≤–µ—Ä–æ—è—Ç–Ω–æ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            else:
                print("     ‚Ä¢ –î–ª—è –º–æ–¥–µ–ª–µ–π, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫ –º–∞—Å—à—Ç–∞–±—É, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—é –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é.")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_feature_distribution - –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏, –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏ –∏ –ª–æ–≥-—à–∫–∞–ª—ã
def plot_feature_distribution(
    df: pd.DataFrame,
    feature: str,
    hue: Optional[str] = None,
    bins: Union[int, Literal["auto"]] = "auto",
    palette: str = "tab10",
    stat: Literal['auto', 'count', 'density', 'probability'] = 'auto',
    log_scale: bool = False,
    table_metrics: Literal['basic', 'extended'] = 'basic',
    show_legend: bool = True 
) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏, –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏ –∏ –ª–æ–≥-—à–∫–∞–ª—ã.
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞:
        1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å KDE - –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ä–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.
        2. Boxplot + stripplot - –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–±—Ä–æ—Å–∞ –∏ –≤—ã–±—Ä–æ—Å–æ–≤.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        feature: str - –∏–º—è —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        hue: Optional[str] - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        bins: Union[int, Literal["auto"]] - —á–∏—Å–ª–æ –±–∏–Ω–æ–≤ –∏–ª–∏ "auto" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        palette: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ seaborn (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'tab10')
        stat: Literal['auto', 'count', 'density', 'probability'] - —Ç–∏–ø –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
            - 'auto': 'count' –±–µ–∑ hue, 'density' —Å hue (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
        log_scale: bool - –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫—É—é —à–∫–∞–ª—É –ø–æ X (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        table_metrics: Literal['basic', 'extended'] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
            - 'basic': –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –¥–æ–ª—è, —Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞ –∏ —Ç.–¥.
            - 'extended': + –∞—Å–∏–º–º–µ—Ç—Ä–∏—è, —ç–∫—Å—Ü–µ—Å—Å, IQR/–ú–µ–¥–∏–∞–Ω–∞, –¥–æ–ª—è –≤—ã–±—Ä–æ—Å–æ–≤ (%)
        show_legend: bool - –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ª–µ–≥–µ–Ω–¥—É –Ω–∞ –ª–µ–≤–æ–º –≥—Ä–∞—Ñ–∏–∫–µ (–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ + KDE)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        None - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü—É –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞
    """
    import warnings

    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    if feature not in df.columns:
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
    if not pd.api.types.is_numeric_dtype(df[feature]):
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º")


    #–û–±—Ä–∞–±–æ—Ç–∫–∞ hue: –∑–∞–º–µ–Ω–∞ NaN –∏ –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    #if hue is not None and hue in df.columns:
    #    if not pd.api.types.is_numeric_dtype(df[hue]):
    #        original_hue = df[hue].copy()
    #        processed_hue = original_hue.copy()
    #
    #        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ [–ø—Ä–æ–ø—É—Å–∫–∏]
    #        processed_hue = processed_hue.fillna('[–ø—Ä–æ–ø—É—Å–∫–∏]')
    #
    #        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤, –Ω–∞ [–ø—Ä–æ–±–µ–ª—ã]
    #        if pd.api.types.is_string_dtype(original_hue):
    #            whitespace_mask = (original_hue.astype(str).str.strip() == '') & original_hue.notna()
    #            processed_hue.loc[whitespace_mask] = '[–ø—Ä–æ–±–µ–ª—ã]'
    #
    #        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü hue –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ
    #        df = df.assign(**{hue: processed_hue}).copy()


    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    required_cols = [feature] if hue is None else [feature, hue]
    df_clean = df.dropna(subset=required_cols).copy()
    if len(df_clean) == 0:
        print("‚ö†Ô∏è –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç")
        return

    data = df_clean[feature]
    if len(data) == 0:
        print("‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–ø—É—Å–∫–∏")
        return

    # –õ–û–ì-–®–ö–ê–õ–ê: –ø—Ä–æ–≤–µ—Ä–∫–∞
    use_log = log_scale
    if use_log:
        if (data <= 0).any():
            warnings.warn(
                f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è. –õ–æ–≥-—à–∫–∞–ª–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.",
                UserWarning
            )
            use_log = False

    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï STAT
    resolved_stat = stat
    if stat == 'auto':
        resolved_stat = 'density' if (hue is not None and hue in df.columns) else 'count'
    else:
        if hue is not None and hue in df.columns and stat == 'count':
            warnings.warn(
                "–ü—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –≥—Ä—É–ø–ø —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ stat='count' –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ. "
                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ stat='density' –∏–ª–∏ 'probability' –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ñ–æ—Ä–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π.",
                UserWarning
            )

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
    mean_val = data.mean()
    median_val = data.median()
    std_val = data.std()
    min_val = data.min()
    max_val = data.max()
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = data[(data < lower_bound) | (data > upper_bound)]

    # –ê–≤—Ç–æ–≤—ã–±–æ—Ä –±–∏–Ω–æ–≤ (Freedman-Diaconis)
    if bins == "auto":
        series = data
        if series.empty or series.nunique() == 1:
            n_bins = 10
        else:
            q75, q25 = np.percentile(series, [75, 25])
            iqr_fd = q75 - q25
            h = 2 * iqr_fd / (len(series) ** (1/3)) if iqr_fd > 0 else 2 * series.std() / (len(series) ** (1/3))
            n_bins = int(np.ceil((series.max() - series.min()) / h)) if h > 0 else 10
            n_bins = max(5, min(n_bins, 50))
        bins = n_bins

    # –ü–æ–¥–ø–∏—Å–∏
    feature_name, feature_desc = label_for_column(feature, separator='‚Ä¢')
    feature_label = f"{feature_name}{feature_desc}" if feature_desc else feature_name
    xlabel = f"log({feature_label})" if use_log else feature_label

    # –ì—Ä–∞—Ñ–∏–∫–∏
    fig, axes = plt.subplots(1, 2, figsize=(18, 4))

    # 1. –ì–ò–°–¢–û–ì–†–ê–ú–ú–ê
    use_hue = hue is not None and hue in df.columns

    if use_hue:
        value_counts = df_clean[hue].value_counts()
        small_cats = value_counts[value_counts < 5].index.tolist()
        large_df = df_clean[~df_clean[hue].isin(small_cats)]
        small_df = df_clean[df_clean[hue].isin(small_cats)]

        hist_kwargs = dict(
            x=feature,
            hue=hue,
            bins=bins,
            palette=palette,
            ax=axes[0],
            alpha=0.7,
            edgecolor="white",
            linewidth=0.5,
            stat=resolved_stat,
            common_norm=False,
            log_scale=use_log,
            legend=show_legend
        )

        if not large_df.empty:
            sns.histplot(data=large_df, kde=True, **hist_kwargs)
        if not small_df.empty:
            sns.histplot(data=small_df, kde=False, **hist_kwargs)

        ylabel = {
            'count': "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            'density': "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å",
            'probability': "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"
        }.get(resolved_stat, resolved_stat.capitalize())

    else:
        sns.histplot(
            data=df_clean,
            x=feature,
            bins=bins,
            color="steelblue",
            ax=axes[0],
            alpha=0.7,
            edgecolor="white",
            linewidth=0.5,
            kde=True,
            stat=resolved_stat,
            log_scale=use_log,
            legend=False
        )
        ylabel = {
            'count': "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            'density': "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å",
            'probability': "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"
        }.get(resolved_stat, resolved_stat.capitalize())

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
    norm_note = "\n–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –≥—Ä—É–ø–ø–∞–º" if use_hue and resolved_stat != 'count' else ""
    log_note = "\n–ª–æ–≥-—à–∫–∞–ª–∞" if use_log else ""
    axes[0].set_title(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: \n{feature_label}{norm_note}{log_note}", fontsize=10)
    axes[0].set_xlabel(xlabel)
    axes[0].set_ylabel(ylabel)

    # 2. BOXPLOT + STRIP
    boxplot_kwargs = dict(
        data=df_clean,
        y=feature,
        ax=axes[1],
        linewidth=1.5,
        flierprops=dict(marker='o', markerfacecolor='orange', markeredgecolor='black', markersize=8, alpha=0.8),
        boxprops=dict(alpha=0.6 if not use_hue else 0.7, linewidth=1.5),
        whiskerprops=dict(linewidth=1.5),
        capprops=dict(linewidth=1.5),
        medianprops=dict(linewidth=2, color='darkred')
    )

    if use_hue:
        sns.boxplot(x=hue, **boxplot_kwargs, palette=palette, width=0.7)
        sns.stripplot(x=hue, y=feature, data=df_clean, color="#2E5472", alpha=0.5, size=1.5, ax=axes[1], jitter=0.25)
        axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0, ha="right")
        hue_name, hue_desc = label_for_column(hue, separator='‚Ä¢')
        hue_label = f"{hue_name}{hue_desc}" if hue_desc else hue_name
        axes[1].set_xlabel(hue_label)
    else:
        sns.boxplot(**boxplot_kwargs, color="lightsteelblue", width=0.5)
        sns.stripplot(y=feature, data=df_clean, color="#2E5472", alpha=0.5, size=1.5, ax=axes[1], jitter=0.25)
        axes[1].set_xlabel("")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥-—à–∫–∞–ª—É –∏ –∫ boxplot
    if use_log:
        axes[1].set_yscale('log')
        axes[1].set_ylabel(f"log({feature_name})")
    else:
        axes[1].set_ylabel(feature_name)

    axes[1].set_title(f"–í—ã–±—Ä–æ—Å—ã –∏ —Ä–∞–∑–±—Ä–æ—Å: \n{feature_label}{log_note}", fontsize=10)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ —É–≥–ª—É boxplot
    stats_text = (
        f"–°—Ä–µ–¥–Ω–µ–µ: {mean_val:.1f}\n"
        f"–ú–µ–¥–∏–∞–Ω–∞: {median_val:.1f}\n"
        f"–°—Ç–¥: {std_val:.1f}\n"
        f"–ú–∏–Ω: {min_val:.1f}\n"
        f"–ú–∞–∫—Å: {max_val:.1f}\n"
        f"Q1: {q1:.1f}\n"
        f"Q3: {q3:.1f}\n"
        f"IQR: {iqr:.1f}\n"
        f"–ì—Ä–∞–Ω–∏—Ü—ã: {lower_bound:.1f} ‚Äì {upper_bound:.1f}\n"
        f"–í—ã–±—Ä–æ—Å—ã: {len(outliers)}"
    )
    axes[1].text(
        0.98, 0.955,
        stats_text,
        transform=axes[1].transAxes,
        ha='right',
        va='top',
        fontsize=7,
        bbox=dict(boxstyle="round,pad=0.3", facecolor="white", edgecolor="#cccccc", alpha=0.8)
    )

    # –Ø–≤–Ω–æ –≤–∫–ª—é—á–∞–µ–º —Å–µ—Ç–∫—É
    axes[0].grid(True, linestyle='--', alpha=0.5)
    axes[1].grid(True, linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.show()

    # –¢–ê–ë–õ–ò–¶–ê –°–¢–ê–¢–ò–°–¢–ò–ö –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú
    if use_hue:
        total_count = len(df.dropna(subset=[feature]))
        categories = df_clean[hue].unique()
        colors = sns.color_palette(palette, n_colors=len(categories))
        color_map = dict(zip(categories, colors))
        
        stats_records = []
        single_value_cats = []
        
        for cat in categories:
            cat_data = df_clean[df_clean[hue] == cat][feature]
            n = len(cat_data)
            if n == 0:
                continue
                
            count = n
            pct = count / total_count * 100
            color_hex = matplotlib.colors.to_hex(color_map[cat])
            
            mean_val = cat_data.mean()
            median_val = cat_data.median()
            std_val = cat_data.std() if n > 1 else np.nan
            min_val = cat_data.min()
            max_val = cat_data.max()
            
            if n == 1:
                single_value_cats.append(str(cat))
            
            # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
            record = {
                "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": str(cat),
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ": count,
                "–î–æ–ª—è (%)": pct,
                "–°—Ä–µ–¥–Ω–µ–µ": mean_val,
                "–ú–µ–¥–∏–∞–Ω–∞": median_val,
                "–°—Ç–¥": std_val,
                "–ú–∏–Ω": min_val,
                "–ú–∞–∫—Å": max_val,
            }
            
            # Extended –º–µ—Ç—Ä–∏–∫–∏
            if table_metrics == 'extended':
                skew_val = skew(cat_data) if n > 2 else np.nan
                kurt_val = kurtosis(cat_data, fisher=False) if n > 3 else np.nan  # Pearson's kurtosis
                
                q1_local = cat_data.quantile(0.25)
                q3_local = cat_data.quantile(0.75)
                iqr_local = q3_local - q1_local
                iqr_over_med = iqr_local / median_val if median_val != 0 else np.nan
                
                lower_local = q1_local - 1.5 * iqr_local
                upper_local = q3_local + 1.5 * iqr_local
                n_outliers_local = ((cat_data < lower_local) | (cat_data > upper_local)).sum()
                outlier_pct_local = (n_outliers_local / n) * 100 if n > 0 else 0.0

                # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏
                if pd.isna(skew_val):
                    skew_dir = "-"
                elif skew_val > 0.5:
                    skew_dir = "‚ñ∂‚ñ∂ —Å–∏–ª—å–Ω–æ –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"
                elif skew_val > 0.1:
                    skew_dir = "‚ñ∂ –ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"
                elif skew_val < -0.5:
                    skew_dir = "‚óÄ‚óÄ —Å–∏–ª—å–Ω–æ –ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"
                elif skew_val < -0.1:
                    skew_dir = "‚óÄ –ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è"
                else:
                    skew_dir = "‚âà —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è"
                
                record.update({
                    "–ê—Å–∏–º–º–µ—Ç—Ä–∏—è": skew_val,
                    "–°–º–µ—â–µ–Ω–∏–µ": skew_dir,
                    "–≠–∫—Å—Ü–µ—Å—Å": kurt_val,
                    "IQR / –ú–µ–¥–∏–∞–Ω–∞": iqr_over_med,
                    "–í—ã–±—Ä–æ—Å—ã (%)": outlier_pct_local
                })
            
            stats_records.append(record)
        
        stats_df = pd.DataFrame(stats_records).sort_values("–î–æ–ª—è (%)", ascending=False)
        
        if single_value_cats:
            print(f"\n‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö {', '.join(single_value_cats)} —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ - –°—Ç–¥ –∏ —Ñ–æ—Ä–º–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã.")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø–æ–ª–Ω–æ—Ç—É –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ä–µ–¥–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.")
        
        
        
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º '{feature_name}'{feature_desc}")

        if not use_hue:
            hue_name, hue_desc = label_for_column(hue, separator='‚Ä¢')
            print(f"–¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞ '{hue_name}'{hue_desc}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        base_cols = ["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–î–æ–ª—è (%)", "–°—Ä–µ–¥–Ω–µ–µ", "–ú–µ–¥–∏–∞–Ω–∞", "–°—Ç–¥", "–ú–∏–Ω", "–ú–∞–∫—Å"]
        extended_cols = ["–ê—Å–∏–º–º–µ—Ç—Ä–∏—è", "–°–º–µ—â–µ–Ω–∏–µ", "–≠–∫—Å—Ü–µ—Å—Å", "IQR / –ú–µ–¥–∏–∞–Ω–∞", "–í—ã–±—Ä–æ—Å—ã (%)"]
        all_cols = base_cols + (extended_cols if table_metrics == 'extended' else [])
        color_map_for_styling = {str(cat): matplotlib.colors.to_hex(color_map[cat]) for cat in categories}

        def styler(s: pd.io.formats.style.Styler) -> pd.io.formats.style.Styler:
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–≤–µ—Ç–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è—Ä–∫–æ—Å—Ç–∏ —Ñ–æ–Ω–∞
            def get_text_color(bg_hex):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º HEX –≤ RGB
                r = int(bg_hex[1:3], 16) / 255.0
                g = int(bg_hex[3:5], 16) / 255.0
                b = int(bg_hex[5:7], 16) / 255.0

                # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —è—Ä–∫–æ—Å—Ç—å (W3C formula)
                luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b
                return "white" if luminance < 0.5 else "black"

            fmt_dict = {
                "–î–æ–ª—è (%)": "{:.1f}%",
                "–°—Ä–µ–¥–Ω–µ–µ": "{:.2f}",
                "–ú–µ–¥–∏–∞–Ω–∞": "{:.2f}",
                "–°—Ç–¥": "{:.2f}",
                "–ú–∏–Ω": "{:.2f}",
                "–ú–∞–∫—Å": "{:.2f}"
            }
            if table_metrics == 'extended':
                fmt_dict.update({
                    "–ê—Å–∏–º–º–µ—Ç—Ä–∏—è": "{:.2f}",
                    "–≠–∫—Å—Ü–µ—Å—Å": "{:.2f}",
                    "IQR / –ú–µ–¥–∏–∞–Ω–∞": "{:.2f}",
                    "–í—ã–±—Ä–æ—Å—ã (%)": "{:.1f}%"
                })

            s = s.format(fmt_dict, na_rep="-")
            s = s.set_properties(subset=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"], **{"text-align": "left"})
            s = s.background_gradient(subset=["–î–æ–ª—è (%)"], cmap="Oranges")

            if table_metrics == 'extended':
                s = s.background_gradient(subset=["–ê—Å–∏–º–º–µ—Ç—Ä–∏—è"], cmap="RdYlGn_r", vmin=-2, vmax=2)
                s = s.background_gradient(subset=["–í—ã–±—Ä–æ—Å—ã (%)"], cmap="Oranges")

            # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç —Ñ–æ–Ω–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞
            def apply_category_bg_color(col):
                colors = []
                for val in col:
                    bg_hex = color_map_for_styling.get(val, "#ffffff")  # default white
                    text_color = get_text_color(bg_hex)
                    colors.append(f"background-color: {bg_hex}; color: {text_color};")
                return colors

            s = s.apply(apply_category_bg_color, subset=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])
            return s

        display_table(
            stats_df[all_cols],
            rows=len(stats_df),
            float_precision=2,
            styler_func=styler
        )


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_feature_distribution_advanced ‚Ä¢ –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
def plot_feature_distribution_advanced(
    df: pd.DataFrame,
    col: str,
    bins='scott',
    figsize=(15, 4.5),
    xlim=None,
    binwidth=None,
    stat_type='count',
    outlier_iqr_multiplier: float = 1.5,
    ax: Optional[plt.Axes] = None,
    show_stats: bool = True,
    title: Optional[str] = None,
    show_outliers: int = 0
):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ –∏
    (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –≤—ã–≤–æ–¥–∏—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–º –≤—ã–±—Ä–æ—Å–∞–º.

    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É, KDE, boxplot, stripplot –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ –≤ –æ–¥–Ω–æ–º –≥—Ä–∞—Ñ–∏–∫–µ.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤, –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤ subplot'—ã –∏ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—é.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
   -------
    df : pd.DataFrame
        –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
    col : str
        –ù–∞–∑–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
    bins : str or int, optional
        –°–ø–æ—Å–æ–± –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–∞ –±–∏–Ω–æ–≤: 'scott', 'fd', 'auto' –∏–ª–∏ int.
    figsize : tuple, optional
        –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ ax=None.
    xlim : tuple, optional
        –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã –ø–æ –æ—Å–∏ X.
    binwidth : float, optional
        –®–∏—Ä–∏–Ω–∞ –±–∏–Ω–∞ –≤ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–µ.
    stat_type : str, optional
        –¢–∏–ø —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: 'count' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –∏–ª–∏ 'density'.
    outlier_iqr_multiplier : float, optional
        –ú–Ω–æ–∂–∏—Ç–µ–ª—å IQR –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü –≤—ã–±—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.5).
        –ó–Ω–∞—á–µ–Ω–∏—è ‚â•3.0 —Å—á–∏—Ç–∞—é—Ç—Å—è "—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏".
    ax : matplotlib.axes.Axes, optional
        –û—Å—å –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏. –ï—Å–ª–∏ None - —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤–∞—è —Ñ–∏–≥—É—Ä–∞.
    show_stats : bool, optional
        –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True).
    title : str, optional
        –ö–∞—Å—Ç–æ–º–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫. –ï—Å–ª–∏ None - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    show_outliers : int, optional
        –°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ –ø–æ–∫–∞–∑–∞—Ç—å –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0 - –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å).
        –ü—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏–∏ > 0 –≤—ã–≤–æ–¥–∏—Ç—Å—è –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π –∫–æ–ª–æ–Ω–∫–∏ `col`.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç
   -------
    None
        –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ç–∞–±–ª–∏—Ü—É —Å –≤—ã–±—Ä–æ—Å–∞–º–∏.
    """

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if stat_type not in ('count', 'density'):
        raise ValueError("stat_type must be 'count' or 'density'")
    

    col_name, col_desc = label_for_column(col, separator='‚Ä¢')

    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ {col_name}{col_desc} —Å KDE, boxplot, scatter –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏\n")
    dataset_profile(df, report='head')
    print()

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    dataset_name, dataset_desc = label_for_dataset(df, separator='‚Ä¢')
    data = df[col].dropna()
    if len(data) == 0:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col_name}' —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–ø—É—Å–∫–∏")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø (–∏—Å–∫–ª—é—á–∞—è bool)
    if not pd.api.types.is_numeric_dtype(data) or data.dtype == bool:
        raise TypeError(
            f"–ö–æ–ª–æ–Ω–∫–∞ '{col_name}' –∏–º–µ–µ—Ç —Ç–∏–ø {df[col].dtype} –∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∏–ª–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (int, float)."
        )

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    mean_val = data.mean()
    median_val = data.median()
    min_val = data.min()
    max_val = data.max()
    std_val = data.std()
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1

    lower_bound = q1 - outlier_iqr_multiplier * iqr
    upper_bound = q3 + outlier_iqr_multiplier * iqr
    outliers = data[(data < lower_bound) | (data > upper_bound)]

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Å–∏
    if ax is None:
        fig, ax1 = plt.subplots(figsize=figsize)
        is_own_figure = True
    else:
        ax1 = ax
        is_own_figure = False

    #col_name, col_desc = label_for_column(col, separator='‚Ä¢')

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    if title is None:
        title = f"–î–∞—Ç–∞—Å–µ—Ç  : {dataset_name}{dataset_desc}\n–ö–æ–ª–æ–Ω–∫–∞ : {col_name}{col_desc}"
    ax1.set_title(
        title,
        fontsize=12,
        fontweight='bold',
        loc='left',
        pad=20
    )

    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
    sns.histplot(
        data,
        ax=ax1,
        kde=False,
        bins=bins,
        binwidth=binwidth,
        color="#86BCE7",
        edgecolor="#E6F0F5",
        alpha=0.8,
        stat=stat_type
    )

    # –®–∏—Ä–∏–Ω–∞ –±–∏–Ω–∞ –∏ KDE
    patches = ax1.patches
    bin_width = patches[0].get_width() if patches else ((data.max() - data.min()) / bins if isinstance(bins, int) else 1.0)

    sns.kdeplot(data, ax=ax1, color="#295C96", linewidth=2.5, alpha=0.7)
    if stat_type == 'count':
        scale_factor = len(data) * bin_width
        ax1.lines[-1].set_ydata(ax1.lines[-1].get_ydata() * scale_factor)

    # –õ–∏–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
    ax1.axvline(mean_val, color='red', linestyle='--', linewidth=2, label='–°—Ä–µ–¥–Ω–µ–µ')
    ax1.axvline(median_val, color='#ef7a31', linestyle='-', linewidth=2, label='–ú–µ–¥–∏–∞–Ω–∞')
    ax1.axvline(min_val, color="#1aa38c81", linestyle='--', linewidth=1, label='–ú–∏–Ω–∏–º—É–º')
    ax1.axvline(max_val, color="#9d1aa477", linestyle='--', linewidth=1, label='–ú–∞–∫—Å–∏–º—É–º')
    ax1.axvspan(q1, q3, alpha=0.1, color='#9467bd', label='IQR')

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if show_stats:
        stats_text = (
            f"–°—Ä–µ–¥–Ω–µ–µ: {mean_val:.1f}\n"
            f"–ú–µ–¥–∏–∞–Ω–∞: {median_val:.1f}\n"
            f"–°—Ç–¥: {std_val:.1f}\n"
            f"–ú–∏–Ω: {min_val:.1f} | –ú–∞–∫—Å: {max_val:.1f}\n"
            f"Q1: {q1:.1f} | Q3: {q3:.1f} | IQR: {iqr:.1f}\n"
            f"–ì—Ä–∞–Ω–∏—Ü—ã ({outlier_iqr_multiplier}√óIQR): {lower_bound:.1f} ‚Äì {upper_bound:.1f}\n"
            f"–í—ã–±—Ä–æ—Å—ã: {len(outliers)}"
        )
        ax1.text(
            0.985, 0.95,
            stats_text,
            transform=ax1.transAxes,
            ha='right',
            va='top',
            fontsize=9,
            bbox=dict(boxstyle="round,pad=0.4", facecolor="white", edgecolor="#cccccc", alpha=0.7)
        )

    # –í—Ç–æ—Ä–∞—è –æ—Å—å: boxplot + stripplot
    ax2 = ax1.twinx()

    sns.boxplot(
        x=data,
        ax=ax2,
        width=0.1,
        color='orange',
        saturation=0.75,
        linewidth=1.5,
        flierprops=dict(
            marker='o',
            markerfacecolor='#ef7a31',
            markeredgecolor='#7b3910',
            markersize=10,
            alpha=0.8,
            zorder=8
        ),
        medianprops=dict(color='#ef7a31', linewidth=12, alpha=0.4),
        boxprops=dict(alpha=0.3, edgecolor='darkorange'),
        whiskerprops=dict(color='darkorange', linewidth=1.5),
        capprops=dict(color='darkorange', linewidth=1.5)
    )

    sns.stripplot(
        x=data,
        ax=ax2,
        color="#29648f",
        alpha=0.2,
        size=6,
        jitter=0.04,
        edgecolor='white',
        linewidth=0.5,
        zorder=1
    )

    # –ú–∞—Ä–∫–µ—Ä—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –Ω–∞ –æ—Å–∏ —Ç–æ—á–µ–∫
    ax2.scatter(mean_val, 0, color='red', edgecolors='white', s=120, marker='D', zorder=10)
    ax2.scatter(median_val, 0, color='#ef7a31', edgecolors='white', s=120, marker='^', zorder=10)
    ax2.scatter(q1, 0, color='#9467bd', edgecolors='white', s=120, marker='s', zorder=10)
    ax2.scatter(q3, 0, color='#9467bd', edgecolors='white', s=120, marker='s', zorder=10)

    ax2.set_ylabel('')
    ax2.set_yticklabels([])
    ax2.set_xlim(ax1.get_xlim())
    ax2.set_ylim(-0.1, 0.1)

    # –ì—Ä–∞–Ω–∏—Ü—ã –≤—ã–±—Ä–æ—Å–æ–≤
    if outlier_iqr_multiplier >= 3.0:
        bound_color = '#d32f2f'
        bound_style = '-.'
    else:
        bound_color = '#ea54f7'
        bound_style = '--'

    ax1.axvline(q1, color='#9467bd', linestyle='-.', linewidth=2)
    ax1.axvline(q3, color='#9467bd', linestyle='-.', linewidth=2)
    ax1.axvline(lower_bound, color=bound_color, linestyle=bound_style, linewidth=1.2, alpha=0.8, label=f'–ì—Ä–∞–Ω–∏—Ü—ã ({outlier_iqr_multiplier}√óIQR)')
    ax1.axvline(upper_bound, color=bound_color, linestyle=bound_style, linewidth=1.2, alpha=0.8)

    # –õ–µ–≥–µ–Ω–¥–∞
    ax1.plot([], [], color='red', marker='D', linestyle='', markersize=8, label='–°—Ä–µ–¥–Ω–µ–µ')
    ax1.plot([], [], color='#ef7a31', marker='^', linestyle='', markersize=8, label='–ú–µ–¥–∏–∞–Ω–∞')
    ax1.plot([], [], color='#9467bd', marker='s', linestyle='', markersize=8, label='Q1/Q3')

    if len(outliers) > 0:
        label_text = f"{len(outliers)} –≤—ã–±—Ä–æ—Å–æ–≤"
        ax2.text(
            0.98, 0.4,
            label_text,
            transform=ax2.transAxes,
            ha='right',
            va='center',
            fontsize=9,
            color='#7b3910',
            fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.2', facecolor='lightcoral', alpha=0.3)
        )

    if xlim is not None:
        ax1.set_xlim(xlim)

    ax1.legend(
        loc='upper left',
        fontsize=9,
        bbox_to_anchor=(1.02, 1.025),
        frameon=True,
        facecolor="#ffffff",
        fancybox=True,
        shadow=False,
        borderpad=1
    )

    # –¢–µ–ª–µ–º–µ—Ç—Ä–∏—è –∏ –≤—ã–≤–æ–¥ –≤—ã–±—Ä–æ—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    n_outliers = len(outliers)
    total_valid = len(data)
    pct_outliers = 100 * n_outliers / total_valid if total_valid > 0 else 0.0

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∫–∏ –∫–æ–ª–æ–Ω–∫–∏
    #col_name, col_desc = label_for_column(col, separator="‚Ä¢")
    col_label = f"{col_name}{col_desc}" if col_desc else col_name

    # –¢–µ–ª–µ–º–µ—Ç—Ä–∏—è –∏ –≤—ã–≤–æ–¥ –≤—ã–±—Ä–æ—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    n_outliers = len(outliers)
    total_valid = len(data)
    pct_outliers = 100 * n_outliers / total_valid if total_valid > 0 else 0.0

    if n_outliers > 0:
        if outlier_iqr_multiplier >= 3.0:
            label = f"üí• –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ({outlier_iqr_multiplier}√óIQR)"
        else:
            label = f"üî∂ –í—ã–±—Ä–æ—Å—ã ({outlier_iqr_multiplier}√óIQR)"
        
        print(f"{label} –≤ '{col_name}': {n_outliers} ({pct_outliers:.1f}%)")

    if show_outliers > 0 and n_outliers > 0:
        outlier_indices = data[(data < lower_bound) | (data > upper_bound)].index
        outliers_df = df.loc[outlier_indices].copy()
        outliers_df = outliers_df.sort_values(col, ascending=False)
        n_show = min(show_outliers, len(outliers_df))

        # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–π —Ç–µ—Ä–º–∏–Ω –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
        term = "—ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π" if outlier_iqr_multiplier >= 3.0 else "–≤—ã–±—Ä–æ—Å–æ–≤"

        if n_outliers <= show_outliers:
            print(f"\nüö® –í—Å–µ–≥–æ {term} –ø–æ –ø—Ä–∏–∑–Ω–∞–∫—É '{col_name}': {n_outliers}")
        else:
            print(f"\nüö® –¢–æ–ø-{n_show} {term} –∏–∑ {n_outliers} –ø–æ –ø—Ä–∏–∑–Ω–∞–∫—É '{col_name}':")

        def highlight_col(styler):
            return styler.background_gradient(subset=[col], cmap="Oranges")

        display_table(
            outliers_df,
            rows=n_show,
            float_precision=2,
            styler_func=highlight_col
        )

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    if is_own_figure:
        plt.subplots_adjust(left=0.06, right=0.75)

        # –Ø–≤–Ω–æ –≤–∫–ª—é—á–∞–µ–º —Å–µ—Ç–∫—É
        ax1.grid(True, linestyle='-', alpha=0.5)
        ax2.grid(True, linestyle='-', alpha=0.5)
        plt.tight_layout()
        plt.show()


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_target_relationships: –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞  
def plot_target_relationships(
    df: pd.DataFrame,
    target: str,
    hue: Optional[str] = None,
    exclude: Optional[List[str]] = None,
    include: Optional[List[str]] = None,
    cols_per_row: int = 3,
    palette: str = "tab10",
    report: Literal["summary", "full"] = "summary",
    method: Literal["pearson", "spearman"] = "spearman"
) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –°–æ–∑–¥–∞—ë—Ç scatter plot –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø—Ä–æ—Ç–∏–≤ —Ç–∞—Ä–≥–µ—Ç–∞.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ü–≤–µ—Ç–æ–≤—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É (hue).
        –í —Ä–µ–∂–∏–º–µ 'full' –≤—ã–≤–æ–¥–∏—Ç —Ç–∞–±–ª–∏—Ü—É –ª–µ–≥–µ–Ω–¥—ã —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ —Ç–∞—Ä–≥–µ—Ç—É.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–¥–ø–∏—Å–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –º–µ—Ç–æ–¥ —Ä–∞—Å—á—ë—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏: Pearson (—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±—Ä–æ—Å–∞–º)
        –∏–ª–∏ Spearman (—É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–º —Å–≤—è–∑—è–º).

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        target: str - –∏–º—è —Ü–µ–ª–µ–≤–æ–≥–æ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        hue: Optional[str] - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è —Ü–≤–µ—Ç–æ–≤–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        exclude: Optional[List[str]] - –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
        include: Optional[List[str]] - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        cols_per_row: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
        palette: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ seaborn (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'tab10')
        report: Literal["summary", "full"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏:
            - "summary": —Ç–æ–ª—å–∫–æ –≥—Ä–∞—Ñ–∏–∫–∏,
            - "full": –≥—Ä–∞—Ñ–∏–∫–∏ + —Ç–∞–±–ª–∏—Ü–∞ –ª–µ–≥–µ–Ω–¥—ã —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ —Ç–∞—Ä–≥–µ—Ç—É
        method: Literal["pearson", "spearman"] - –º–µ—Ç–æ–¥ —Ä–∞—Å—á—ë—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ 
            —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "spearman")

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ç–∞–±–ª–∏—Ü—É –ª–µ–≥–µ–Ω–¥—ã
    """
    
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    if method not in ("pearson", "spearman"):
        raise ValueError("–ü–∞—Ä–∞–º–µ—Ç—Ä 'method' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'pearson' –∏–ª–∏ 'spearman'")
        
     # –ü—Ä–æ–≤–µ—Ä–∫–∞: —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ target
    if target not in df.columns:
        available_cols = ", ".join(df.columns[:5]) + ("..." if len(df.columns) > 5 else "")
        error_msg = (
            f"‚ùå –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ '{target}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ\n"
            f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {available_cols}"
        )
        raise ValueError(error_msg)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞: —è–≤–ª—è–µ—Ç—Å—è –ª–∏ target —á–∏—Å–ª–æ–≤—ã–º
    target_series = df[target]
    if not pd.api.types.is_numeric_dtype(target_series):
        try:
            pd.to_numeric(target_series, errors='raise')
        except (ValueError, TypeError):
            sample_values = target_series.dropna().head(3).tolist()
            error_msg = (
                f"‚ùå –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ '{target}' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º\n"
                f"   –¢–µ–∫—É—â–∏–π —Ç–∏–ø: {target_series.dtype}\n"
                f"   –ü—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π: {sample_values}\n"
            )
            raise ValueError(error_msg)

    # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if exclude:
        num_cols = [col for col in num_cols if col not in exclude]
    if include:
        num_cols = [col for col in num_cols if col in include]
    if target in num_cols:
        num_cols.remove(target)
    
    if not num_cols:
        print("‚úîÔ∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        return

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥–ø–∏—Å–∏
    target_name, target_desc = label_for_column(target, separator='‚Ä¢')
    target_label = f"{target_name}{target_desc}"

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    n_plots = len(num_cols)
    n_rows = (n_plots + cols_per_row - 1) // cols_per_row
    
    figsize = (4 * cols_per_row, 5.0 * max(n_rows, 1))
    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=figsize, squeeze=False)
    axes = axes.flatten()

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    for i, col in enumerate(num_cols):
        ax = axes[i]
        
        if hue and hue in df.columns:
            sns.scatterplot(data=df, x=col, y=target, hue=hue, palette=palette, ax=ax, legend=False)
        else:
            sns.scatterplot(data=df, x=col, y=target, ax=ax, legend=False)
        
        col_name, col_desc = label_for_column(col, separator='‚Ä¢')
        col_label = f"{col_name}{col_desc}" if col_desc else col_name
        
        ax.set_title(f"{col_label}", fontsize=8)
        ax.set_xlabel(col_label, fontsize=8)
        ax.set_ylabel(target_label, fontsize=8)

    # –°–∫—Ä—ã—Ç–∏–µ –ø—É—Å—Ç—ã—Ö subplot'–æ–≤
    for i in range(n_plots, len(axes)):
        axes[i].set_visible(False)

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    fig.suptitle(
        f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞:\n{target_label}",
        fontsize=12, fontweight="bold", ha="left", x=0.02, y=0.98
    )

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    # –¢–∞–±–ª–∏—Ü–∞ –ª–µ–≥–µ–Ω–¥—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ 'full' –∏ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω hue)
    if report == "full" and hue and hue in df.columns:
        print(f"\n–õ–µ–≥–µ–Ω–¥–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É '{hue}':")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        value_counts = df[hue].value_counts()
        total = len(df)
        
        legend_records = []
        colors = sns.color_palette(palette, n_colors=len(value_counts))
        
        for i, (cat, count) in enumerate(value_counts.items()):
            pct = count / total * 100
            
            # –ë–ï–ó–û–ü–ê–°–ù–û–ï –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –ö –ß–ò–°–õ–£
            target_series = df[df[hue] == cat][target]
            target_values = pd.to_numeric(target_series, errors='coerce')
            
            if target_values.isna().all():
                target_mean = target_median = np.nan
            else:
                target_mean = target_values.mean()
                target_median = target_values.median()
            
            legend_records.append({
                "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": str(cat),
                "–ß–∞—Å—Ç–æ—Ç–∞": count,
                "–î–æ–ª—è (%)": pct,
                "–°—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É": target_mean,
                "–ú–µ–¥–∏–∞–Ω–∞ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É": target_median,
                # –ù–ï–¢ –∫–æ–ª–æ–Ω–∫–∏ "–¶–≤–µ—Ç"!
            })
        
        legend_df = pd.DataFrame(legend_records)
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Üí —Ü–≤–µ—Ç
        color_map = {}
        for i, (cat, _) in enumerate(value_counts.items()):
            color_map[str(cat)] = matplotlib.colors.to_hex(colors[i])

        def legend_styler(s: pd.io.formats.style.Styler) -> pd.io.formats.style.Styler:
            def get_text_color(bg_color):
                try:
                    bg_hex = matplotlib.colors.to_hex(bg_color)
                except:
                    bg_hex = "#ffffff"
                r = int(bg_hex[1:3], 16) / 255.0
                g = int(bg_hex[3:5], 16) / 255.0
                b = int(bg_hex[5:7], 16) / 255.0
                luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b
                return "white" if luminance < 0.5 else "black"

            def apply_bg_color(col):
                styles = []
                for val in col:
                    bg = color_map.get(val, "#ffffff")
                    text_color = get_text_color(bg)
                    styles.append(f"background-color: {bg}; color: {text_color};")
                return styles

            return (
                s.set_properties(subset=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"], **{"text-align": "left"})
                .background_gradient(subset=["–î–æ–ª—è (%)"], cmap="coolwarm")
                .apply(apply_bg_color, subset=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])
            )

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É 
        display_table(
            legend_df[["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–°—Ä–µ–¥–Ω–µ–µ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É", "–ú–µ–¥–∏–∞–Ω–∞ –ø–æ —Ç–∞—Ä–≥–µ—Ç—É", "–ß–∞—Å—Ç–æ—Ç–∞", "–î–æ–ª—è (%)"]],
            rows=len(legend_df),
            float_precision=0,
            styler_func=legend_styler
        )

    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π –∏ —Å–∏–ª–æ–π —Å–≤—è–∑–∏
    method_name = "–ü–∏—Ä—Å–æ–Ω–∞" if method == "pearson" else "–°–ø–∏—Ä–º–∞–Ω–∞"
    print(f"\n–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ ({method_name}) —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º '{target_label}': ")

    correlations = df[num_cols + [target]].corr(method=method)[target].drop(target)

    # –ù–ê–ó–ù–ê–ß–ê–ï–ú –¶–í–ï–¢–ê –ü–†–ò–ó–ù–ê–ö–ê–ú
    feature_colors = {}
    palette_colors = sns.color_palette(palette, n_colors=len(num_cols))
    for i, col in enumerate(num_cols):
        col_name, _ = label_for_column(col)
        feature_colors[col_name] = matplotlib.colors.to_hex(palette_colors[i])

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
    corr_records = []
    for col in correlations.index:
        corr_value = correlations[col]
        col_name, col_desc = label_for_column(col)
        
        abs_corr = abs(corr_value)
        if abs_corr < 0.1:
            strength = "–æ—á–µ–Ω—å —Å–ª–∞–±–∞—è"
        elif abs_corr < 0.3:
            strength = "—Å–ª–∞–±–∞—è"
        elif abs_corr < 0.5:
            strength = "—É–º–µ—Ä–µ–Ω–Ω–∞—è"
        else:
            strength = "—Å–∏–ª—å–Ω–∞—è"
        
        corr_records.append({
            "–ü—Ä–∏–∑–Ω–∞–∫": col_name,
            "–û–ø–∏—Å–∞–Ω–∏–µ": col_desc,
            "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è": corr_value,
            "–°–∏–ª–∞ —Å–≤—è–∑–∏": strength
        })

    corr_df = pd.DataFrame(corr_records).sort_values("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è", key=abs, ascending=False)

    strength_colors = {
        "–æ—á–µ–Ω—å —Å–ª–∞–±–∞—è": "#e8f5e8",
        "—Å–ª–∞–±–∞—è": "#c8e6c9",
        "—É–º–µ—Ä–µ–Ω–Ω–∞—è": "#a5d6a7",
        "—Å–∏–ª—å–Ω–∞—è": "#4caf50"
    }

    # –°–¢–ò–õ–ò–ó–ê–¶–ò–Ø –° –¶–í–ï–¢–ê–ú–ò –ü–†–ò–ó–ù–ê–ö–û–í
    def _style_corr_table(styler):
        styler = styler.background_gradient(subset=["–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è"], cmap="coolwarm")
        styler = styler.applymap(
            lambda x: f"background-color: {strength_colors.get(x, '')}; color: black",
            subset=["–°–∏–ª–∞ —Å–≤—è–∑–∏"]
        )

        return styler

    display_table(
        corr_df[["–ü—Ä–∏–∑–Ω–∞–∫", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è", "–°–∏–ª–∞ —Å–≤—è–∑–∏"]],
        rows=len(corr_df),
        float_precision=3,
        styler_func=_style_corr_table
    )


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_mixed_correlation ‚Ä¢ –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö (—á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ)
# UPD: add method: Literal['Pearson', 'Spearman'] = 'Pearson'
def calculate_cramers_v(x: pd.Series, y: pd.Series) -> float:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Cramer's V –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        x: pd.Series - –ø–µ—Ä–≤—ã–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        y: pd.Series - –≤—Ç–æ—Ä–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        float - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ Cramer's V
    """
    contingency_table = pd.crosstab(x, y)
    contingency_table = contingency_table.loc[(contingency_table != 0).any(axis=1)]
    contingency_table = contingency_table.loc[:, (contingency_table != 0).any(axis=0)]
    
    if contingency_table.empty:
        return 0.0
    
    chi2, _, _, _ = chi2_contingency(contingency_table)
    n = contingency_table.sum().sum()
    min_dim = min(contingency_table.shape) - 1
    
    if min_dim == 0:
        return 0.0
    
    cramers_v = np.sqrt(chi2 / (n * min_dim))
    return min(cramers_v, 1.0)


def plot_mixed_correlation(
    df: pd.DataFrame, 
    figsize: tuple = None,
    annot: bool = True,
    cmap: str = 'RdBu_r',
    threshold: float = None,
    hide_upper: bool = True,
    show_grid: bool = True,
    show_diagonal: bool = False,
    exclude: list = None,
    include: list = None,
    precision: int = 3,
    filter_no_corr: bool = True,
    auto_font_size: bool = True,
    method: Literal['pearson', 'spearman'] = 'pearson'
) -> pd.DataFrame:
    """
    –°—Ç—Ä–æ–∏—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –∏ —Ç–∞–±–ª–∏—Ü—É —Å–≤—è–∑–µ–π –¥–ª—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç—Ä–∏ —Ç–∏–ø–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: Pearson (—á–∏—Å–ª–æ-—á–∏—Å–ª–æ), Cramer‚Äôs V (–∫–∞—Ç–µ–≥–æ—Ä–∏—è-–∫–∞—Ç–µ–≥–æ—Ä–∏—è) –∏ point-biserial (—á–∏—Å–ª–æ-–∫–∞—Ç–µ–≥–æ—Ä–∏—è).
        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–∞—Å–∫–æ–π, –ø–æ—Ä–æ–≥–æ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è–º–∏ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        figsize: tuple - —Ä–∞–∑–º–µ—Ä –≥—Ä–∞—Ñ–∏–∫–∞ (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞); –µ—Å–ª–∏ None - –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        annot: bool - –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç–µ
        cmap: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'RdBu_r')
        threshold: float - –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (None - –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        hide_upper: bool - —Å–∫—Ä—ã–≤–∞—Ç—å –≤–µ—Ä—Ö–Ω–∏–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫ –º–∞—Ç—Ä–∏—Ü—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        show_grid: bool - –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å–µ—Ç–∫—É –º–µ–∂–¥—É —è—á–µ–π–∫–∞–º–∏
        show_diagonal: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –¥–∏–∞–≥–æ–Ω–∞–ª—å (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å —Å–∞–º–∏–º —Å–æ–±–æ–π)
        exclude: list - —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
        include: list - –µ—Å–ª–∏ –∑–∞–¥–∞–Ω, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        precision: int - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö –∏ —Ç–∞–±–ª–∏—Ü–µ
        filter_no_corr: bool - –∏—Å–∫–ª—é—á–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –≤—Å–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞
        auto_font_size: bool - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        pd.DataFrame - –ø–æ–ª–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –ø–æ—Ä–æ–≥—É)
    """
    if method not in ('pearson', 'spearman'):
        raise ValueError("–ü–∞—Ä–∞–º–µ—Ç—Ä 'method' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'pearson' –∏–ª–∏ 'spearman'")

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏–º–µ–Ω–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name, dataset_desc = label_for_dataset(df)
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' üìã {dataset_desc}")

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ exclude/include
    all_cols = df.columns.tolist()
    
    if exclude:
        all_cols = [col for col in all_cols if col not in exclude]
    
    if include:
        all_cols = [col for col in all_cols if col in include]
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ, –ø—Ä–æ–≤–µ—Ä—è—è —Ç–∏–ø—ã
    numeric_cols = []
    categorical_cols = []
    
    for col in all_cols:
        if col in df.select_dtypes(include=[np.number]).columns:
            try:
                pd.to_numeric(df[col].dropna(), errors='raise')
                numeric_cols.append(col)
            except (ValueError, TypeError):
                categorical_cols.append(col)
        else:
            categorical_cols.append(col)
    
    all_cols = [col for col in all_cols if col in numeric_cols or col in categorical_cols]
    
    if not all_cols:
        print("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π")
        return pd.DataFrame()
    
    n_cols = len(all_cols)
    corr_matrix = pd.DataFrame(
        np.zeros((n_cols, n_cols)), 
        index=all_cols, 
        columns=all_cols
    )
    
    for col1, col2 in combinations(all_cols, 2):
        idx1, idx2 = all_cols.index(col1), all_cols.index(col2)
        
        series1 = df[col1].dropna()
        series2 = df[col2].dropna()
        
        common_idx = series1.index.intersection(series2.index)
        if len(common_idx) == 0:
            corr_val = 0.0
        elif col1 in numeric_cols and col2 in numeric_cols:
            series1_clean = pd.to_numeric(series1.loc[common_idx], errors='coerce')
            series2_clean = pd.to_numeric(series2.loc[common_idx], errors='coerce')
            mask = ~(series1_clean.isna() | series2_clean.isna())
            if mask.sum() > 1:
                corr_val = pd.Series.corr(series1_clean[mask], series2_clean[mask], method=method)
            else:
                corr_val = 0.0
        elif col1 in categorical_cols and col2 in categorical_cols:
            corr_val = calculate_cramers_v(
                series1.loc[common_idx], 
                series2.loc[common_idx]
            )
        else:
            num_col = col1 if col1 in numeric_cols else col2
            cat_col = col2 if col2 in categorical_cols else col1
            
            series_num = df[num_col].loc[common_idx]
            series_cat = df[cat_col].loc[common_idx]
            
            series_num_clean = pd.to_numeric(series_num, errors='coerce').dropna()
            series_cat_clean = series_cat.loc[series_num_clean.index]
            
            mask = ~(series_num_clean.isna() | series_cat_clean.isna())
            if mask.sum() > 1:
                cat_encoded = pd.Categorical(series_cat_clean[mask]).codes
                corr_val, _ = pointbiserialr(series_num_clean[mask], cat_encoded)
            else:
                corr_val = 0.0
        
        corr_matrix.loc[col1, col2] = corr_val
        corr_matrix.loc[col2, col1] = corr_val
    
    np.fill_diagonal(corr_matrix.values, 1.0)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–µ–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
    if filter_no_corr and threshold is not None:
        abs_corr_matrix = np.abs(corr_matrix)
        np.fill_diagonal(abs_corr_matrix.values, 0)
        max_corr_per_feature = abs_corr_matrix.max(axis=1)
        active_features = max_corr_per_feature[max_corr_per_feature >= threshold].index.tolist()
        
        if active_features:
            corr_matrix = corr_matrix.loc[active_features, active_features]
            print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(all_cols) - len(active_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–µ–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π")
        else:
            print("–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞")
            return pd.DataFrame()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω)
    if figsize is None:
        size_per_feature = max(0.4, 8 / max(len(corr_matrix), 10))
        figsize = (max(8, len(corr_matrix) * size_per_feature), 
                   max(6, len(corr_matrix) * size_per_feature))
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É
    if hide_upper:
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=0)
        if threshold is not None:
            threshold_mask = np.abs(corr_matrix) < threshold
            mask = mask | threshold_mask
    else:
        mask = np.abs(corr_matrix) < threshold if threshold is not None else None
        if not show_diagonal:
            mask = mask | np.eye(len(corr_matrix), dtype=bool)
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    fmt_str = f'.{precision}f'
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É
    plt.figure(figsize=figsize)
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ (–≤—Å–µ–≥–¥–∞ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    annot_kws = {}
    if auto_font_size:
        font_size = max(5, min(10, 14 - len(corr_matrix) // 2))
        annot_kws = {'size': font_size}
    
    sns.heatmap(
        corr_matrix,
        annot=annot,
        cmap=cmap,
        center=0,
        square=True,
        fmt=fmt_str,
        cbar_kws={'shrink': 0.8},
        mask=mask,
        linewidths=0.5 if show_grid else 0,
        annot_kws=annot_kws
    )
    
    method_name = "–ü–∏—Ä—Å–æ–Ω–∞" if method == "pearson" else "–°–ø–∏—Ä–º–∞–Ω–∞"

    plt.suptitle(
        f'–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ ({method_name}, –ø–æ—Ä–æ–≥: {threshold})\n'
        f' ‚Ä¢ —á–∏—Å–ª–æ–≤—ã–µ: {method.capitalize()}\n'
        f' ‚Ä¢ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ: Cramer\'s V\n'
        f' ‚Ä¢ —Å–º–µ—à–∞–Ω–Ω—ã–µ: Point-biserial', 
        fontsize=max(10, min(10, 14 - len(corr_matrix) // 3)), 
        x=0.01,
        y=0.98,
        ha='left'
    )

    
    plt.xticks(rotation=45, ha='right', fontsize=max(8, min(6, 12 - len(corr_matrix) // 4)))
    plt.yticks(rotation=0, fontsize=max(8, min(6, 12 - len(corr_matrix) // 4)))
    plt.tight_layout()
    plt.show()
    
    # –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É —Å–≤—è–∑–µ–π
    print(f"–¢–∞–±–ª–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–ø–æ—Ä–æ–≥: {threshold})")
    
    pairs_data = []
    for i in range(len(corr_matrix)):
        for j in range(i + 1, len(corr_matrix)):
            corr_val = corr_matrix.iloc[i, j]
            if threshold is None or abs(corr_val) >= threshold:
                feature_1_name, feature_1_desc = label_for_column(corr_matrix.index[i], separator='‚Ä¢')
                feature_2_name, feature_2_desc = label_for_column(corr_matrix.columns[j], separator='‚Ä¢')

                pairs_data.append({
                    '–ü—Ä–∏–∑–Ω–∞–∫ 1': f'{feature_1_name}{feature_1_desc}',
                    '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è': corr_val,
                    '–ü—Ä–∏–∑–Ω–∞–∫ 2': f'{feature_2_name}{feature_2_desc}'
                })
    
    if pairs_data:
        pairs_df = pd.DataFrame(pairs_data)
        pairs_df = pairs_df.sort_values(by='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è', key=abs, ascending=False)
        #pairs_df.insert(0, '#', range(1, len(pairs_df) + 1))
        
        from matplotlib.colors import LinearSegmentedColormap
        colormap = plt.cm.RdBu_r
        
        def color_corr(val):
            normalized_val = (val + 1) / 2
            rgba_color = colormap(normalized_val)
            hex_color = '#{:02x}{:02x}{:02x}'.format(
                int(rgba_color[0] * 255),
                int(rgba_color[1] * 255), 
                int(rgba_color[2] * 255)
            )
            return f'background-color: {hex_color}; color: white' if normalized_val < 0.2 or normalized_val > 0.8 else f'background-color: {hex_color}; color: black'
        
        fmt_table = f'{{:.{precision}f}}'

        display_table(
            pairs_df,
            rows=len(pairs_df),
            float_precision=precision,
            styler_func=lambda s: s.background_gradient(
                subset=["–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è"], 
                cmap="RdBu_r", 
                low=0.3, 
                high=0.7
            )
        )
    else:
        print("–ù–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π, –ø—Ä–µ–≤—ã—à–∞—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥")
    
    return corr_matrix


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def plot_pairwise_correlations(
    df: pd.DataFrame,
    hue_col: Optional[str] = None,
    include: Optional[List[str]] = None,
    exclude: Optional[List[str]] = None,
    palette: str = 'tab10',
    threshold: Optional[float] = None,
    show_report: bool = True,
    report_threshold: float = 0.3,
    precision: int = 3,
    base_point_size: int = 20,
    dpi: int = 150,
    method: Literal['pearson', 'spearman'] = 'spearman' 
) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–º –æ—Ç—á—ë—Ç–æ–º.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –°–æ–∑–¥–∞—ë—Ç –º–∞—Ç—Ä–∏—Ü—É scatter-–≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–ü–∏—Ä—Å–æ–Ω–∞ –∏–ª–∏ –°–ø–∏—Ä–º–∞–Ω–∞). 
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ü–≤–µ—Ç–æ–≤—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É (hue_col).
        –í—ã–≤–æ–¥–∏—Ç —Ç–∞–±–ª–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–π –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π —Å–∏–ª—ã —Å–≤—è–∑–∏.
        –î–ª—è hue_col –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        hue_col: Optional[str] - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è —Ü–≤–µ—Ç–æ–≤–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        include: Optional[List[str]] - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        exclude: Optional[List[str]] - –∏—Å–∫–ª—é—á–∏—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
        palette: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ seaborn (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'tab10')
        threshold: Optional[float] - –æ—Ç–±–∏—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å |r| ‚â• threshold
        show_report: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        report_threshold: float - –ø–æ—Ä–æ–≥ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –ø–∞—Ä—ã –≤ –æ—Ç—á—ë—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3)
        precision: int - –∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
        base_point_size: int - –±–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —Ç–æ—á–µ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20)
        dpi: int - —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 150)
        method: Literal['pearson', 'spearman'] - –º–µ—Ç–æ–¥ —Ä–∞—Å—á—ë—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'spearman')

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç
    """
    # –ó–ê–©–ò–¢–ê –û–¢ –ü–£–°–¢–û–ì–û –î–ê–¢–ê–§–†–ï–ô–ú–ê
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return
    
    if method not in ("pearson", "spearman"):
        raise ValueError("–ü–∞—Ä–∞–º–µ—Ç—Ä 'method' –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'pearson' –∏–ª–∏ 'spearman'")

    # –ü–û–î–ü–ò–°–¨ –î–ê–¢–ê–°–ï–¢–ê
    dataset_name, dataset_desc = label_for_dataset(df)
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç {dataset_name} ‚Ä¢ {dataset_desc}\n")

    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ß–ò–°–õ–û–í–´–• –ö–û–õ–û–ù–û–ö
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if include is not None:
        numeric_cols = [col for col in include if col in numeric_cols]
    if exclude is not None:
        numeric_cols = [col for col in numeric_cols if col not in exclude]
    
    if not numeric_cols:
        print("‚ö†Ô∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        return
    
    if len(numeric_cols) == 1:
        print(f"‚ö†Ô∏è –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω —á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫: {numeric_cols[0]}. –ü–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
        return

    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –ü–û–†–û–ì–£ –ö–û–†–†–ï–õ–Ø–¶–ò–ò
    if threshold is not None:
        corr_matrix_full = df[numeric_cols].corr()
        relevant_pairs = []
        for i, j in combinations(range(len(numeric_cols)), 2):
            r = corr_matrix_full.iloc[i, j]
            if abs(r) >= threshold:
                relevant_pairs.append((numeric_cols[i], numeric_cols[j]))
        
        if not relevant_pairs:
            print(f"‚ö†Ô∏è –ù–µ—Ç –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π ‚â• {threshold}")
            return
        
        relevant_features = list(set([col for pair in relevant_pairs for col in pair]))
        numeric_cols = relevant_features
        print(f"üîç –û—Ç–æ–±—Ä–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({len(numeric_cols)}) —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π ‚â• {threshold}")
        for col in numeric_cols:
            col_name, col_desc = label_for_column(col, separator='‚Ä¢')
            col_label = f"{col_name}{col_desc}" if col_desc else col_name
            print(f"   ‚Ä¢ {col_label}")

    # –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
    cols_to_plot = numeric_cols + ([hue_col] if hue_col else [])
    data_to_plot = df[cols_to_plot].dropna(subset=numeric_cols)

    if len(data_to_plot) == 0:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö.")
        return

    # –û–ë–†–ê–ë–û–¢–ö–ê HUE_COL
    hue_col_final = hue_col
    if hue_col and hue_col in data_to_plot.columns:
        if pd.api.types.is_numeric_dtype(data_to_plot[hue_col]):
            unique_count = data_to_plot[hue_col].nunique()
            if unique_count <= 15:
                data_to_plot = data_to_plot.copy()
                data_to_plot[hue_col] = data_to_plot[hue_col].astype(str)
                print(f"üîÑ –ö–æ–ª–æ–Ω–∫–∞ '{hue_col}' –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é ({unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)")
            else:
                print(f"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ '{hue_col}' —Å–æ–¥–µ—Ä–∂–∏—Ç {unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ü–≤–µ—Ç–æ–≤–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏")
                hue_col_final = None
        else:
            print(f"üìã –ö–æ–ª–æ–Ω–∫–∞ '{hue_col}' –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è ({data_to_plot[hue_col].nunique()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)")
    elif hue_col:
        print(f"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ '{hue_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
        hue_col_final = None

    # –ó–ê–©–ò–¢–ê –û–¢ KDE-–û–®–ò–ë–û–ö
    # –£–¥–∞–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    if hue_col_final:
        valid_categories = []
        for cat in data_to_plot[hue_col_final].unique():
            cat_data = data_to_plot[data_to_plot[hue_col_final] == cat]
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ö–æ—Ç—è –±—ã –≤ –æ–¥–Ω–æ–º —á–∏—Å–ª–æ–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–µ
            has_variance = any(cat_data[col].nunique() > 1 for col in numeric_cols)
            if has_variance:
                valid_categories.append(cat)
            else:
                print(f"‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{cat}' –≤ '{hue_col_final}' –∏–º–µ–µ—Ç –Ω—É–ª–µ–≤—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é - –∏—Å–∫–ª—é—á–µ–Ω–∞ –∏–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        
        if not valid_categories:
            print("‚ö†Ô∏è –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ hue_col –∏–º–µ—é—Ç –Ω—É–ª–µ–≤—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é - –æ—Ç–∫–ª—é—á–∞–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É")
            hue_col_final = None
        else:
            data_to_plot = data_to_plot[data_to_plot[hue_col_final].isin(valid_categories)]

    # –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–†–ê–§–ò–ö–ê
    try:
        n_samples = len(data_to_plot)
        point_size = max(5, min(20, base_point_size * (1000 / n_samples)))

        g = sns.pairplot(
            data_to_plot,
            hue=hue_col_final,
            palette=palette,
            diag_kind='kde',
            corner=True,
            plot_kws={'alpha': 0.7, 's': point_size},
            diag_kws={'shade': True},
            height=2.5,
            aspect=1.0,  
        )

        n_features = len(numeric_cols)
        g.fig.set_size_inches(2 * n_features, 2 * n_features)
        g.fig.set_dpi(dpi)

        # –£–¥–∞–ª—è–µ–º –ª–µ–≥–µ–Ω–¥—É
        if hue_col_final is not None:
            for ax in g.axes.flat:
                if ax is not None and ax.legend_ is not None:
                    ax.legend_.remove()
            if hasattr(g, '_legend'):
                g._legend.remove()

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ —Ä–∞–º–∫–∏
        corr_matrix = df[numeric_cols].corr(method=method)
        for i in range(len(numeric_cols)):
            for j in range(len(numeric_cols)):
                ax = g.axes[i, j]
                if ax is None:
                    continue

                # –°–µ—Ä–∞—è —Ä–∞–º–∫–∞
                rect_border = plt.Rectangle(
                    (0, 0), 1, 1,
                    transform=ax.transAxes,
                    facecolor='none',
                    edgecolor='gray',
                    linewidth=1.5,
                    zorder=4
                )
                ax.add_patch(rect_border)

                # –î–∏–∞–≥–æ–Ω–∞–ª—å - —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                if i == j:
                    col_name, col_desc = label_for_column(numeric_cols[i], separator='')
                    col_label = f"{col_name}{col_desc}" if col_desc else col_name
                    ax.set_title(col_label, fontsize=5.5, pad=5, loc='left')
                    continue  # ‚Üê –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Å—å Y –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏

                # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ
                r = corr_matrix.iloc[i, j]
                abs_r = abs(r)
                if abs_r >= 0.7:
                    text_color = 'darkred'
                    fontweight = 'bold'
                elif abs_r >= 0.5:
                    text_color = 'red'
                    fontweight = 'bold'
                else:
                    text_color = 'gray'
                    fontweight = 'normal'

                ax.text(
                    0.05, 0.95, f'r={r:.{precision}f}',
                    transform=ax.transAxes,
                    fontsize=8,
                    verticalalignment='top',
                    color=text_color,
                    fontweight=fontweight,
                    bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=0.7)
                )

        # –õ–µ–≥–µ–Ω–¥–∞
        if hue_col_final is not None:
            from matplotlib.patches import Patch
            unique_cats = data_to_plot[hue_col_final].dropna().unique()
            colors = sns.color_palette(palette, n_colors=len(unique_cats))
            legend_elements = [
                Patch(facecolor=color, edgecolor='black', label=str(cat))
                for color, cat in zip(colors, unique_cats)
            ]
            g.fig.legend(
                legend_elements,
                [str(cat) for cat in unique_cats],
                loc='upper right',
                bbox_to_anchor=(0.99, 0.84),
                ncol=1,
                title=f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ {hue_col_final}",
                frameon=True,
                fontsize=6,
                title_fontsize=7
            )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        method_name = "–ü–∏—Ä—Å–æ–Ω–∞" if method == "pearson" else "–°–ø–∏—Ä–º–∞–Ω–∞"
        g.fig.suptitle(
            f"–ü–∞—Ä–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (r, –º–µ—Ç–æ–¥: {method_name})\n{dataset_desc}",
            fontsize=10,
            fontweight='bold',
            x=0.98,
            y=0.92,
            ha='right'
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç—Å—Ç—É–ø–æ–≤ - –±–µ–∑ tight_layout
        g.fig.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.1, wspace=0.3, hspace=0.3)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –º–µ—Ç–∫–∏ –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
        for i in range(len(numeric_cols)):
            ax = g.axes[i, i]
            if ax is not None:
                ax.set_ylabel('')  # –£–±–∏—Ä–∞–µ–º –º–µ—Ç–∫—É Y –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
                ax.set_xlabel('')  # –£–±–∏—Ä–∞–µ–º –º–µ—Ç–∫—É X –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.show()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}")
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ –æ—Ç–∫–ª—é—á–∏—Ç—å hue_col")

        # –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –û–¢–ß–Å–¢
        if show_report:
            pairs_data = []
            for i in range(len(corr_matrix)):
                for j in range(i + 1, len(corr_matrix)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) >= report_threshold:
                        f1_name, f1_desc = label_for_column(corr_matrix.index[i], separator='‚Ä¢')
                        f2_name, f2_desc = label_for_column(corr_matrix.columns[j], separator='‚Ä¢')
                        pairs_data.append({
                            "–ü—Ä–∏–∑–Ω–∞–∫ 1": f"{f1_name}{f1_desc}" if f1_desc else f1_name,
                            "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è": corr_val,
                            "–ü—Ä–∏–∑–Ω–∞–∫ 2": f"{f2_name}{f2_desc}" if f2_desc else f2_name
                        })
            
            if not pairs_data:
                print(f"üî∏ –ù–µ—Ç –ø–∞—Ä —Å |r| ‚â• {report_threshold}")
                return
            
            pairs_df = pd.DataFrame(pairs_data)
            pairs_df = pairs_df.sort_values("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è", key=abs, ascending=False)
            #pairs_df.insert(0, "#", range(1, len(pairs_df) + 1))
            
            print(f"\nüìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ—Ä–æ–≥ |r| ‚â• {report_threshold}):")
            display_table(
                pairs_df,
                rows=len(pairs_df),
                float_precision=precision,
                styler_func=lambda s: s.background_gradient(
                    subset=["–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è"], 
                    cmap="RdBu_r", 
                    low=0.3, 
                    high=0.7
                )
            )
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
            unique_cats = data_to_plot[hue_col_final].dropna().unique()
            colors = sns.color_palette(palette, n_colors=len(unique_cats))
            color_map = {str(cat): matplotlib.colors.to_hex(colors[i]) for i, cat in enumerate(unique_cats)}
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å value_counts, –Ω–æ –º–∞–ø–ø–∏–Ω–≥ - –ø–æ unique_cats!)
            group_counts = data_to_plot[hue_col_final].value_counts()
            total = len(data_to_plot)
            
            group_records = []
            for cat in unique_cats:  # ‚Üê –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É!
                count = group_counts.get(cat, 0)
                pct = count / total * 100 if total > 0 else 0
                group_records.append({
                    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": str(cat),
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ": count,
                    "–î–æ–ª—è (%)": pct,
                })
            
            groups_df = pd.DataFrame(group_records)

            def group_styler(s: pd.io.formats.style.Styler) -> pd.io.formats.style.Styler:
                def get_text_color(bg_color):
                    try:
                        bg_hex = matplotlib.colors.to_hex(bg_color)
                    except:
                        bg_hex = "#ffffff"
                    r = int(bg_hex[1:3], 16) / 255.0
                    g = int(bg_hex[3:5], 16) / 255.0
                    b = int(bg_hex[5:7], 16) / 255.0
                    luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b
                    return "white" if luminance < 0.5 else "black"

                def apply_bg_color(col):
                    styles = []
                    for val in col:
                        bg = color_map.get(val, "#ffffff")
                        text_color = get_text_color(bg)
                        styles.append(f"background-color: {bg}; color: {text_color};")
                    return styles

                return (
                    s.set_properties(subset=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"], **{"text-align": "left"})
                    .background_gradient(subset=["–î–æ–ª—è (%)"], cmap="coolwarm")
                    .apply(apply_bg_color, subset=["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])
                )

            display_table(
                groups_df[["–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–î–æ–ª—è (%)"]],
                rows=len(groups_df),
                float_precision=0,
                styler_func=group_styler
            )
            
            print(f"\nüí° –°–æ–≤–µ—Ç—ã –ø–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏:")
            print(f"   ‚Ä¢ –ï—Å–ª–∏ —Ç–æ—á–∫–∏ –æ–¥–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞ –æ–±—Ä–∞–∑—É—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ–±–ª–∞–∫–∞ - —É –≥—Ä—É–ø–ø—ã —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏")
            print(f"   ‚Ä¢ –ï—Å–ª–∏ —Ç–æ—á–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ü–≤–µ—Ç–æ–≤ –ø–µ—Ä–µ–º–µ—à–∞–Ω—ã - –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç {hue_label}")

        # –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø
        strong = pairs_df[pairs_df["–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è"].abs() >= 0.7]
        moderate = pairs_df[(pairs_df["–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è"].abs() >= 0.5) & (pairs_df["–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è"].abs() < 0.7)]
        
        print(f"\nüß† –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Å–∏–ª—ã —Å–≤—è–∑–µ–π:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data_to_plot):,}")
        print(f"   ‚Ä¢ –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_cols)}")
        if len(strong) > 0:
            print(f"   üî• {len(strong)} –æ—á–µ–Ω—å —Å–∏–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π (|r| ‚â• 0.7) - –≤–æ–∑–º–æ–∂–Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å")
        if len(moderate) > 0:
            print(f"   ‚ö° {len(moderate)} —É–º–µ—Ä–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π (0.5 ‚â§ |r| < 0.7) - –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏")
        if len(pairs_df) == len(strong) + len(moderate):
            print(f"   üî∏ –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ - —Å–ª–∞–±—ã–µ (|r| < 0.5)")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_categorical_distribution: –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
def plot_categorical_distribution(
    df: pd.DataFrame,
    feature: str,
    hue: Optional[str] = None,
    report: Literal["summary", "full"] = "summary",
    palette: str = "tab10",
    min_freq_threshold: float = 0.01,  # 1%
) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å —É—á—ë—Ç–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç:
        1. –°—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —á–∞—Å—Ç–æ—Ç —Å –ª–∏–Ω–∏–µ–π –º–µ–¥–∏–∞–Ω—ã –¥–æ–ª–∏ –∏ —Ü–≤–µ—Ç–æ–≤–æ–π –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
        2. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Boxplot –ø–æ hue, –µ—Å–ª–∏ hue - —á–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫.
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ü—Ä–æ–ø—É—Å–∫–∏ (NaN) –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è **[–ø—Ä–æ–ø—É—Å–∫–∏]**.
        - –°—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤, –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –∫–∞–∫ **[–ø—Ä–æ–±–µ–ª—ã]**.
        - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç 100% —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
        - –ú–µ–¥–∏–∞–Ω–∞ –∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ ¬´–Ω–∞—Å—Ç–æ—è—â–∏–º¬ª –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
        –í—ã–≤–æ–¥–∏—Ç:
        - –ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–±–ª–µ–º (–¥–∏—Å–±–∞–ª–∞–Ω—Å, –º—É—Å–æ—Ä, —Ä–µ–¥–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏),
        - (–í —Ä–µ–∂–∏–º–µ 'full') —Ç–∞–±–ª–∏—Ü—É —á–∞—Å—Ç–æ—Ç —Å —Ü–≤–µ—Ç–æ–≤–æ–π –º–µ—Ç–∫–æ–π –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ COLUMN_DESCRIPTIONS –∏ DATASET_DESCRIPTIONS.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        feature: str - –∏–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        hue: Optional[str] - —á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        report: Literal["summary", "full"] - —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        palette: str - –ø–∞–ª–∏—Ç—Ä–∞ seaborn (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'tab10')
        min_freq_threshold: float - –ø–æ—Ä–æ–≥ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.01 = 1%)

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ display_table –∏ –≥—Ä–∞—Ñ–∏–∫–∏
    """
    if df.empty:
        print("‚ö†Ô∏è –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç")
        return

    if feature not in df.columns:
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")

    # –ü–æ–¥–ø–∏—Å—å –ø—Ä–∏–∑–Ω–∞–∫–∞
    feature_name, feature_desc = label_for_column(feature, separator='‚Ä¢')
    feature_label = f"{feature_name}{feature_desc}" if feature_desc else feature_name

    # –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    df_processed = df[[feature]].copy()

    # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ [–ø—Ä–æ–ø—É—Å–∫–∏]
    missing_mask = df_processed[feature].isna()
    n_missing = missing_mask.sum()
    df_processed.loc[missing_mask, feature] = "[–ø—Ä–æ–ø—É—Å–∫–∏]"

    # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–¥–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ [–æ–¥–Ω–∏ –ø—Ä–æ–±–µ–ª—ã]
    whitespace_mask = df_processed[feature].astype(str).str.match(r'^\s*$') & ~missing_mask
    n_whitespace = whitespace_mask.sum()
    df_processed.loc[whitespace_mask, feature] = "[–æ–¥–Ω–∏ –ø—Ä–æ–±–µ–ª—ã]"

    n_total = len(df)
    missing_pct = n_missing / n_total * 100 if n_total > 0 else 0
    whitespace_pct = n_whitespace / n_total * 100 if n_total > 0 else 0

    # –ß–∞—Å—Ç–æ—Ç—ã (–≤–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
    value_counts = df_processed[feature].value_counts(dropna=False)
    freq_df = pd.DataFrame({
        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': value_counts.index,
        '–ß–∞—Å—Ç–æ—Ç–∞': value_counts.values,
        '–î–æ–ª—è (%)': (value_counts.values / n_total * 100)
    }).reset_index(drop=True)

    # –ú–µ–¥–∏–∞–Ω–∞ –¥–æ–ª–∏ - —Ç–æ–ª—å–∫–æ –ø–æ ¬´–Ω–∞—Å—Ç–æ—è—â–∏–º¬ª –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    clean_freqs = freq_df[~freq_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].isin(["[–ø—Ä–æ–ø—É—Å–∫–∏]", "[–æ–¥–Ω–∏ –ø—Ä–æ–±–µ–ª—ã]"])]
    median_pct = clean_freqs['–î–æ–ª—è (%)'].median() if not clean_freqs.empty else 0.0

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
    issues = []
    rare_cats = clean_freqs[clean_freqs['–î–æ–ª—è (%)'] < min_freq_threshold * 100]['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].tolist()
    if rare_cats:
        issues.append(f"—Ä–µ–¥–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (<{min_freq_threshold:.0%}): {len(rare_cats)} —à—Ç")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä (–∏—Å–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
    str_series = df[feature].dropna().astype(str)
    non_whitespace = ~str_series.str.match(r'^\s*$')
    junk_mask = non_whitespace & str_series.str.lower().isin(['null', 'n/a', 'nan', 'none', ''])
    junk_values = str_series[junk_mask]
    if not junk_values.empty:
        issues.append(f"–º—É—Å–æ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {junk_values.nunique()} —Ç–∏–ø–æ–≤")

    # –î–∏—Å–±–∞–ª–∞–Ω—Å - —Ç–æ–ª—å–∫–æ –ø–æ ¬´–Ω–∞—Å—Ç–æ—è—â–∏–º¬ª –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    if not clean_freqs.empty:
        max_pct = clean_freqs['–î–æ–ª—è (%)'].max()
        if max_pct > 95:
            top_cat = clean_freqs.loc[clean_freqs['–î–æ–ª—è (%)'].idxmax(), '–ö–∞—Ç–µ–≥–æ—Ä–∏—è']
            issues.append(f"—Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å: '{top_cat}' - {max_pct:.1f}%")

    # –í–´–í–û–î –û–¢–ß–Å–¢–ê
    dataset_name, dataset_desc = label_for_dataset(df, separator="‚Ä¢")
    print(f"\nüóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç: {dataset_name}{dataset_desc}")
    print(f"üè∑Ô∏è –ü—Ä–∏–∑–Ω–∞–∫: {feature_label}")

    if n_missing > 0:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–æ–≤ (NaN): {n_missing} ({missing_pct:.1f}%)")
    else:
        print("‚úîÔ∏è –ü—Ä–æ–ø—É—Å–∫–æ–≤ (NaN) –Ω–µ—Ç")

    if n_whitespace > 0:
        print(f"‚ö†Ô∏è –°—Ç—Ä–æ–∫ –∏–∑ –æ–¥–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤: {n_whitespace} ({whitespace_pct:.1f}%)")

    if issues:
        print("üö® –ü—Ä–æ–±–ª–µ–º—ã:")
        for issue in issues:
            print(f"    ‚Ä¢ {issue}")
    else:
        print("üíé –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ö–æ—Ä–æ—à–µ–µ")

    # –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
    n_plots = 2 if hue and hue in df.columns else 1

    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –≤—ã—Å–æ—Ç–∞
    if n_plots == 1:
        height = min(5, max(3, len(freq_df) * 0.4))
        figsize = (16, height)
    else:
        height = max(5, len(freq_df) * 0.4)
        figsize = (16, height)

    fig, axes = plt.subplots(1, n_plots, figsize=figsize, squeeze=False)
    ax1 = axes[0, 0]

    # –¶–≤–µ—Ç–∞: —Å–ª—É–∂–µ–±–Ω—ã–µ - —Å–µ—Ä—ã–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - –∏–∑ –ø–∞–ª–∏—Ç—Ä—ã
    colors = []
    palette_colors = sns.color_palette(palette, n_colors=len(freq_df))
    for i, row in freq_df.iterrows():
        if row['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] in ["[–ø—Ä–æ–ø—É—Å–∫–∏]", "[–ø—Ä–æ–±–µ–ª—ã]"]:
            colors.append('lightgray')
        else:
            colors.append(palette_colors[i % len(palette_colors)])

    # –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    bars = ax1.bar(
        range(len(freq_df)),
        freq_df['–ß–∞—Å—Ç–æ—Ç–∞'],
        color=colors,
        edgecolor='white',
        linewidth=0.8
    )
    ax1.set_xticks(range(len(freq_df)))
    ax1.set_xticklabels(freq_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'], rotation=0, ha='right', fontsize=9)
    ax1.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=10)
    ax1.set_title(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {feature_label}", fontsize=11)

    # –õ–∏–Ω–∏—è –º–µ–¥–∏–∞–Ω—ã –¥–æ–ª–∏ (–≤ —à–∫–∞–ª–µ —á–∞—Å—Ç–æ—Ç)
    median_freq = median_pct / 100 * n_total
    ax1.axhline(median_freq, color='darkred', linestyle='--', linewidth=1.2, label=f'–ú–µ–¥–∏–∞–Ω–∞ –¥–æ–ª–∏ ({median_pct:.1f}%)')
    ax1.legend(fontsize=9)

    # –í—Ç–æ—Ä–æ–π –≥—Ä–∞—Ñ–∏–∫: boxplot –ø–æ hue (–µ—Å–ª–∏ hue —á–∏—Å–ª–æ–≤–æ–π)
    if n_plots == 2:
        ax2 = axes[0, 1]
        if pd.api.types.is_numeric_dtype(df[hue]):
            sns.boxplot(
                data=df.dropna(subset=[feature, hue]),
                x=feature,
                y=hue,
                ax=ax2,
                palette=palette,
                flierprops=dict(
                    marker='o',
                    markerfacecolor="#DE1885",
                    markeredgecolor="#560A34",
                    markersize=8,
                    alpha=0.5
                ),
                medianprops=dict(color='darkred', linewidth=2),
                boxprops=dict(alpha=0.5, linewidth=1.5)
            )
            ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0, ha='right')
            ax2.set_title(f"{feature_label} vs {hue}", fontsize=11)
            hue_name, hue_desc = label_for_column(hue, separator='‚Ä¢')
            ax2.set_ylabel(f"{hue_name}{hue_desc}")
        else:
            crosstab = pd.crosstab(df[feature], df[hue])
            sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues', ax=ax2)
            ax2.set_title(f"–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {feature_label} –∏ {hue}", fontsize=11)

    plt.tight_layout()
    plt.show()

    # –û—Ç—á—ë—Ç
    if report == "full" and not freq_df.empty:
        # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Üí —Ü–≤–µ—Ç (–±–µ–∑ HTML!)
        color_map = {}
        for i, cat in enumerate(freq_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è']):
            if cat in ["[–ø—Ä–æ–ø—É—Å–∫–∏]", "[–æ–¥–Ω–∏ –ø—Ä–æ–±–µ–ª—ã]"]:
                color_map[cat] = "lightgray"
            else:
                color_map[cat] = matplotlib.colors.to_hex(palette_colors[i % len(palette_colors)])

        def styler(s: pd.io.formats.style.Styler) -> pd.io.formats.style.Styler:
            def get_text_color(bg_color):
                try:
                    bg_hex = matplotlib.colors.to_hex(bg_color)
                except:
                    bg_hex = "#ffffff"
                r = int(bg_hex[1:3], 16) / 255.0
                g = int(bg_hex[3:5], 16) / 255.0
                b = int(bg_hex[5:7], 16) / 255.0
                luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b
                return "white" if luminance < 0.5 else "black"

            def apply_bg_color(col):
                styles = []
                for val in col:
                    bg = color_map.get(val, "#ffffff")
                    text_color = get_text_color(bg)
                    styles.append(f"background-color: {bg}; color: {text_color};")
                return styles

            return (
                s.format({'–î–æ–ª—è (%)': '{:.2f}%'})
                .set_properties(subset=['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'], **{'text-align': 'left'})
                .background_gradient(subset=['–î–æ–ª—è (%)'], cmap='coolwarm')
                .apply(apply_bg_color, subset=['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'])
            )

        print(f"\nüìã –¢–∞–±–ª–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç ({len(freq_df)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π):")
        display_table(
            freq_df[['–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ß–∞—Å—Ç–æ—Ç–∞', '–î–æ–ª—è (%)']],
            rows=len(freq_df),
            float_precision=2,
            styler_func=styler
        )


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_phik_correlation: –°—Ç—Ä–æ–∏—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –Ω–∞ –æ—Å–Ω–æ–≤–µ Phik - –º–µ—Ä—ã –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
def plot_phik_correlation(
    df: pd.DataFrame,
    interval_cols: Optional[List[str]] = None,
    include: Optional[List[str]] = None,
    exclude: Optional[List[str]] = None,
    threshold: float = 0.3,
    figsize: Optional[tuple] = None,
    cmap: str = 'Blues',
    annot: bool = True,
    precision: int = 3,
    show_report: bool = True,
    report_threshold: Optional[float] = None,
    show_triangle: str = 'lower',  # 'lower', 'upper', 'full'
    hide_empty_labels: bool = True,
    grid_color: str = "#c5d1e0",
    show_border: bool = True,
    border_color: str = "#407B8D",
    dpi: int = 150
) -> pd.DataFrame:
    """
    –°—Ç—Ä–æ–∏—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –Ω–∞ –æ—Å–Ω–æ–≤–µ Phik - –º–µ—Ä—ã –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É `phik` –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞:
        - —á–∏—Å–ª–æ–≤—ã–µ ‚Üî —á–∏—Å–ª–æ–≤—ã–µ ‚Üí –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ (–≤–Ω—É—Ç—Ä–∏ phik),
        - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ‚Üî –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ‚Üí Cram√©r‚Äôs V,
        - —á–∏—Å–ª–æ–≤—ã–µ ‚Üî –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ‚Üí –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ä–∞–Ω–≥–∞–º–∏.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é, —Ç—Ä–µ—É–≥–æ–ª—å–Ω—É—é/–ø–æ–ª–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –≥–∏–±–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        ‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è,
        ‚Ä¢ –£—Å—Ç–æ–π—á–∏–≤ –∫ –ø—Ä–æ–ø—É—Å–∫–∞–º –∏ –Ω–∏–∑–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏,
        ‚Ä¢ –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á—ë—Ç —Å —Å–∏–ª–æ–π —Å–≤—è–∑–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏,
        ‚Ä¢ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã –∏ —à—Ä–∏—Ñ—Ç–æ–≤ –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,
        ‚Ä¢ –Ø–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ DPI - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ plt.rcParams.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        interval_cols: Optional[List[str]] - —Å–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        include/exclude: Optional[List[str]] - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
        threshold: float - –ø–æ—Ä–æ–≥ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ Phik –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3)
        figsize: Optional[tuple] - —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (–µ—Å–ª–∏ None - –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        cmap: str - —Ü–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'Blues')
        annot: bool - –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        precision: int - –∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
        show_report: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        report_threshold: Optional[float] - –ø–æ—Ä–æ–≥ –¥–ª—è –æ—Ç—á—ë—Ç–∞ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è threshold)
        show_triangle: str - 'lower', 'upper' –∏–ª–∏ 'full' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'lower')
        hide_empty_labels: bool - —Å–∫—Ä—ã–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –∑–Ω–∞—á–∏–º—ã—Ö —Å–≤—è–∑–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        grid_color: str - —Ü–≤–µ—Ç —Å–µ—Ç–∫–∏ –º–µ–∂–¥—É —è—á–µ–π–∫–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "#c5d1e0")
        show_border: bool - —Ä–∏—Å–æ–≤–∞—Ç—å –ª–∏ –≤–Ω–µ—à–Ω—é—é —Ä–∞–º–∫—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        border_color: str - —Ü–≤–µ—Ç –≤–Ω–µ—à–Ω–µ–π —Ä–∞–º–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "#407B8D")
        dpi: int - —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 150)

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        pd.DataFrame - –ø–æ–ª–Ω–∞—è Phik-–º–∞—Ç—Ä–∏—Ü–∞

    –ü—Ä–∏–º–µ—Ä—ã:
        >>> # –ë–∞–∑–æ–≤—ã–π –≤—ã–∑–æ–≤
        >>> plot_phik_correlation(df)
        
        >>> # –° —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –≤—ã—Å–æ–∫–∏–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º
        >>> plot_phik_correlation(
        ...     df,
        ...     exclude=['id'],
        ...     threshold=0.25,
        ...     dpi=300,
        ...     show_report=True
        ... )

    –ó–∞–º–µ—á–∞–Ω–∏—è:
        - –¢—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `phik`: `pip install phik`
        - Phik ‚àà [0, 1]: 0 - –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å, 1 - –ø–æ–ª–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å
        - –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Å–∏–ª—ã —Å–≤—è–∑–∏:
            ‚Ä¢ < 0.1 - –æ—á–µ–Ω—å —Å–ª–∞–±–∞—è
            ‚Ä¢ 0.1‚Äì0.3 - —Å–ª–∞–±–∞—è
            ‚Ä¢ 0.3‚Äì0.5 - —É–º–µ—Ä–µ–Ω–Ω–∞—è
            ‚Ä¢ ‚â• 0.5 - —Å–∏–ª—å–Ω–∞—è
    """
    try:
        import phik
    except ImportError:
        raise ImportError("‚ùó –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'phik' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –í—ã–ø–æ–ª–Ω–∏—Ç–µ: !pip install phik -q")

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
    all_cols = df.columns.tolist()
    if exclude:
        all_cols = [col for col in all_cols if col not in exclude]
    if include:
        all_cols = [col for col in all_cols if col in include]
    if not all_cols:
        print("‚ö†Ô∏è –ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
        return pd.DataFrame()

    df_subset = df[all_cols].copy()

    # 2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    if interval_cols is None:
        interval_cols = df_subset.select_dtypes(include=[np.number]).columns.tolist()
        print(f"üîç –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(interval_cols)} —à—Ç.")
    else:
        missing = set(interval_cols) - set(df_subset.columns)
        if missing:
            raise ValueError(f"‚ùå –ö–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ: {missing}")

    # 3. –†–∞—Å—á—ë—Ç Phik-–º–∞—Ç—Ä–∏—Ü—ã
    print(f"üßÆ –†–∞—Å—á—ë—Ç Phik-–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤).\n")
    dataset_profile(df, report='summary')
    phik_matrix_full = df_subset.phik_matrix(interval_cols=interval_cols)

    # 4. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å
    if hide_empty_labels:
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Å–≤—è–∑—å >= threshold
        mask_by_threshold = phik_matrix_full >= threshold
        # –ò—Å–∫–ª—é—á–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å (Phik=1)
        np.fill_diagonal(mask_by_threshold.values, False)
        has_significant = mask_by_threshold.any(axis=1)
        cols_to_show = phik_matrix_full.columns[has_significant]
        if len(cols_to_show) == 0:
            print(f"üî∏ –ù–µ—Ç –ø–∞—Ä —Å Phik ‚â• {threshold}")
            if show_report:
                print(f"üìã –¢–∞–±–ª–∏—Ü–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π (Phik ‚â• {report_threshold or threshold}):")
                print("–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö –ø–∞—Ä.")
            return phik_matrix_full
    else:
        cols_to_show = phik_matrix_full.columns

    # 5. –°–æ–∑–¥–∞—ë–º –ø–æ–¥–º–∞—Ç—Ä–∏—Ü—É —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    phik_vis = phik_matrix_full.loc[cols_to_show, cols_to_show]

    # 6. –ú–∞—Å–∫–∞ –¥–ª—è —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if show_triangle == 'lower':
        triangle_mask = np.triu(np.ones_like(phik_vis, dtype=bool), k=1)
    elif show_triangle == 'upper':
        triangle_mask = np.tril(np.ones_like(phik_vis, dtype=bool), k=-1)
    else:
        triangle_mask = np.zeros_like(phik_vis, dtype=bool)

    # üî• –°–∫—Ä—ã–≤–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å
    np.fill_diagonal(triangle_mask, True)

    # 7. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
    n_vis = len(phik_vis)
    if figsize is None:
        base_size_per_feature = 1.2
        min_figsize = 5.0
        max_figsize = 16.0
        fig_size = max(min_figsize, min(max_figsize, n_vis * base_size_per_feature))
        figsize = (fig_size, fig_size)

    # 8. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
    base_fontsize = max(10, 14 - n_vis // 2)

    sns.heatmap(
        phik_vis,
        mask=triangle_mask,
        annot=annot,
        cmap=cmap,
        square=True,
        fmt=f'.{precision}f',
        cbar_kws={'shrink': 0.8, 'label': 'Phi-K'},
        linewidths=0.5,
        linecolor=grid_color,
        annot_kws={'size': max(8, base_fontsize - 2)},
        ax=ax
    )

    # –í–Ω–µ—à–Ω—è—è —Ä–∞–º–∫–∞
    if show_border:
        for spine in ax.spines.values():
            spine.set_color(border_color)
            spine.set_linewidth(0.8)

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    dataset_name, dataset_desc = label_for_dataset(df, separator='‚Ä¢')
    ax.set_title(
        f'Phik-–º–∞—Ç—Ä–∏—Ü–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π\n'
        f' ‚Ä¢ –¥–∞—Ç–∞—Å–µ—Ç: {dataset_name}{dataset_desc}\n'
        f' ‚Ä¢ –ø–æ—Ä–æ–≥: {threshold}\n'
        f' ‚Ä¢ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {n_vis}',
        fontsize=max(9, base_fontsize - 2),
        loc='left',
        pad=12
    )

    # –ü–æ–¥–ø–∏—Å–∏ –æ—Å–µ–π
    ax.set_xticklabels(cols_to_show, rotation=45, ha='right', fontsize=max(8, base_fontsize - 2))
    ax.set_yticklabels(cols_to_show, rotation=0, fontsize=max(8, base_fontsize - 2))

    plt.tight_layout()
    plt.show()

    # 9. –û—Ç—á—ë—Ç
    if not show_report:
        return phik_matrix_full

    if report_threshold is None:
        report_threshold = threshold

    pairs_data = []
    cols = phik_matrix_full.columns
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            phik_val = phik_matrix_full.iloc[i, j]
            if phik_val >= report_threshold:
                col1, col1_desc = label_for_column(cols[i], separator='‚Ä¢')
                col2, col2_desc = label_for_column(cols[j], separator='‚Ä¢')
                pairs_data.append({
                    '–ü—Ä–∏–∑–Ω–∞–∫ 1': f'{col1}{col1_desc}' if col1_desc else col1,
                    'Phik': phik_val,
                    '–ü—Ä–∏–∑–Ω–∞–∫ 2': f'{col2}{col2_desc}' if col2_desc else col2
                })

    if not pairs_data:
        print(f"üî∏ –ù–µ—Ç –ø–∞—Ä —Å Phik ‚â• {report_threshold}")
        return phik_matrix_full

    pairs_df = pd.DataFrame(pairs_data).sort_values('Phik', ascending=False)

    def _phik_strength(val: float) -> str:
        if val < 0.1:
            return "–æ—á–µ–Ω—å —Å–ª–∞–±–∞—è"
        elif val < 0.3:
            return "—Å–ª–∞–±–∞—è"
        elif val < 0.5:
            return "—É–º–µ—Ä–µ–Ω–Ω–∞—è"
        else:
            return "—Å–∏–ª—å–Ω–∞—è"

    pairs_df['–°–∏–ª–∞ —Å–≤—è–∑–∏'] = pairs_df['Phik'].apply(_phik_strength)

    print(f"\nüìã –¢–∞–±–ª–∏—Ü–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π (Phik ‚â• {report_threshold}):")
    display_table(
        pairs_df[['–ü—Ä–∏–∑–Ω–∞–∫ 1', 'Phik', '–ü—Ä–∏–∑–Ω–∞–∫ 2', '–°–∏–ª–∞ —Å–≤—è–∑–∏']],
        rows=len(pairs_df),
        float_precision=precision,
        styler_func=lambda s: (
            s.background_gradient(subset=['Phik'], cmap='Blues', low=0.2, high=0.8)
            .applymap(
                lambda x: "background-color: #e8f5e8; color: #69a85d" if x == "–æ—á–µ–Ω—å —Å–ª–∞–±–∞—è" else
                          "background-color: #c8e6c9; color: #458239" if x == "—Å–ª–∞–±–∞—è" else
                          "background-color: #a5d6a7; color: #2e5e25" if x == "—É–º–µ—Ä–µ–Ω–Ω–∞—è" else
                          "background-color: #4caf50; color: #f9ff80",
                subset=['–°–∏–ª–∞ —Å–≤—è–∑–∏']
            )
        )
    )

    return # phik_matrix_full


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# plot_train_test_distribution - –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –º–µ–∂–¥—É –æ–±—É—á–∞—é—â–µ–π (train) –∏ —Ç–µ—Å—Ç–æ–≤–æ–π (test) –≤—ã–±–æ—Ä–∫–∞–º–∏
def plot_train_test_distribution(
    train: pd.DataFrame,
    test: pd.DataFrame,
    feature: str,
    feature_label: str = "",
    palette: str = "tab10",
    table_metrics: Literal['basic', 'extended'] = 'extended'
) -> None:
    """
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –º–µ–∂–¥—É –æ–±—É—á–∞—é—â–µ–π (train) –∏ —Ç–µ—Å—Ç–æ–≤–æ–π (test) –≤—ã–±–æ—Ä–∫–∞–º–∏.
        
        –§—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏ —Å—Ç—Ä–æ–∏—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é:
        
        - –î–ª—è **—á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:  
        –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `plot_feature_distribution` —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ 'dataset' (train/test),  
        —Å—Ç—Ä–æ–∏—Ç –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É —Å KDE, boxplot –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ (–µ—Å–ª–∏ table_metrics='extended').
        
        - –î–ª—è **–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:  
        —Å—Ç—Ä–æ–∏—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π barplot —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º,  
        –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏,  
        –∞ —Ç–∞–∫–∂–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü–µ–π –≤ –¥–æ–ª—è—Ö (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞—Ö).
        
        –¶–µ–ª—å: –≤—ã—è–≤–∏—Ç—å **—Å–¥–≤–∏–≥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (data drift)** –º–µ–∂–¥—É train –∏ test,  
        –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
       -------
        train : pd.DataFrame
            –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞.
        test : pd.DataFrame
            –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞.
        feature : str
            –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –î–æ–ª–∂–µ–Ω –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –æ–±–æ–∏—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö.
        feature_label : str, optional
            –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –æ—Å–µ–π.  
            –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ `feature`.
        palette : str, optional
            –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–≤–µ—Ç–æ–≤–æ–π –ø–∞–ª–∏—Ç—Ä—ã seaborn (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'tab10', 'Set2', 'husl').  
            –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞—Å–∫—Ä–∞—Å–∫–∏ –≥—Ä—É–ø–ø 'train' –∏ 'test'. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'Set2'.
        table_metrics : {'basic', 'extended'}, optional
            –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ (—Ç–æ–ª—å–∫–æ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):
            - 'basic': –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞, —Å—Ç–¥ –∏ —Ç.–¥.)
            - 'extended': + –∞—Å–∏–º–º–µ—Ç—Ä–∏—è, —ç–∫—Å—Ü–µ—Å—Å, IQR/–º–µ–¥–∏–∞–Ω–∞, –¥–æ–ª—è –≤—ã–±—Ä–æ—Å–æ–≤ (%)
            –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'extended'.
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç
       -------
        None
            –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∏ —Ç–∞–±–ª–∏—Ü—É –≤ —è—á–µ–π–∫—É Jupyter Notebook.
        
        –ü—Ä–∏–º–µ—Ä—ã
       -----
        >>> # –ß–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫
        >>> plot_train_test_distribution(train, test, 'pages_per_visit', '–°—Ç—Ä–∞–Ω–∏—Ü –∑–∞ –≤–∏–∑–∏—Ç')
        
        >>> # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        >>> plot_train_test_distribution(train, test, 'top_category', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞', palette='tab10')
        
        –ó–∞–º–µ—á–∞–Ω–∏—è
       ------
        - –î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è, —á—Ç–æ–±—ã —Ñ—É–Ω–∫—Ü–∏—è `plot_feature_distribution` 
        –±—ã–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≤ —Ç–æ–º –∂–µ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).
        - –ö–∞—Ç–µ–≥–æ—Ä–∏–∏, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –±—É–¥—É—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º 
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤ –¥—Ä—É–≥–æ–º - —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–∏—Ç—å –Ω–æ–≤—ã–µ/–ø—Ä–æ–ø–∞–≤—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è.
        - –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ (`Œî –¥–æ–ª—è (pp)`) —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –≤ **–ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞—Ö (pp)**, 
        –∞ –Ω–µ –≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö.
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞
    if feature not in train.columns or feature not in test.columns:
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ train –∏–ª–∏ test")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    train_labeled = train[[feature]].copy()
    train_labeled['dataset'] = 'train'
    test_labeled = test[[feature]].copy()
    test_labeled['dataset'] = 'test'
    combined = pd.concat([train_labeled, test_labeled], ignore_index=True)

    if not feature_label:
        feature_label = feature

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞
    is_categorical = not pd.api.types.is_numeric_dtype(combined[feature])

    col_name, col_desc = label_for_column(feature_label, separator="‚Ä¢")
    full_col_name = f"'{col_name}'{col_desc}"

    if is_categorical:
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
        total_train = len(combined[combined['dataset'] == 'train'])
        total_test = len(combined[combined['dataset'] == 'test'])
        if total_train == 0 or total_test == 0:
            print("‚ö†Ô∏è –û–¥–∏–Ω –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—É—Å—Ç")
            return

        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –Ω–∞ —è–≤–Ω—É—é –º–µ—Ç–∫—É –î–û –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        combined = combined.copy()
        combined[feature] = combined[feature].fillna("[–ø—Ä–æ–ø—É—Å–∫–∏ ")
        # –¢–∞–∫–∂–µ –∑–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        combined[feature] = combined[feature].replace("", "[–ø—Ä–æ–±–µ–ª—ã]")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        stats = (
            combined.groupby(['dataset', feature])
            .size()
            .reset_index(name='count')
            .pivot(index=feature, columns='dataset', values='count')
            .fillna(0)
            .astype(int)
            .reset_index()
        )
        stats.columns.name = None

        stats['train_pct'] = stats['train'] / total_train * 100
        stats['test_pct'] = stats['test'] / total_test * 100
        stats['Œî –¥–æ–ª—è (pp)'] = (stats['train_pct'] - stats['test_pct']).abs()
        stats = stats.sort_values('Œî –¥–æ–ª—è (pp)', ascending=False)

        # –ü–æ–ª—É—á–∞–µ–º —Ü–≤–µ—Ç–∞ –∏–∑ –ø–∞–ª–∏—Ç—Ä—ã
        unique_datasets = ['train', 'test']
        colors = sns.color_palette(palette, n_colors=len(unique_datasets))
        palette_dict = dict(zip(unique_datasets, colors))

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
        stats_long = stats.melt(
            id_vars=feature,
            value_vars=unique_datasets,
            var_name='dataset',
            value_name='count'
        )

        # –†–∞—Å—á—ë—Ç Cram√©r‚Äôs V –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        observed = pd.crosstab(combined['dataset'], combined[feature])
        try:
            chi2, _, _, expected = chi2_contingency(observed)
            n = observed.sum().sum()
            min_dim = min(observed.shape) - 1
            if min_dim > 0 and n > 0:
                cramers_v = np.sqrt(chi2 / (n * min_dim))
                cramers_v = min(cramers_v, 1.0)
            else:
                cramers_v = 0.0
        except:
            cramers_v = 0.0

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –≤—ã—Å–æ—Ç–∞ –≥—Ä–∞—Ñ–∏–∫–∞
        n_categories = len(stats)
        height = min(12.0, max(3.5, n_categories * 0.4))
        figsize = (16, height)

        # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π barplot —Å constrained_layout
        fig, ax = plt.subplots(figsize=figsize, constrained_layout=True)
        sns.barplot(
            data=stats_long,
            y=feature,
            x='count',
            hue='dataset',
            palette=palette_dict,
            ax=ax,
            edgecolor="white",
            linewidth=0.8,
            dodge=True
        )
        ax.set_title(
            f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {full_col_name} (train vs test)\n"
            f"Cram√©r‚Äôs V = {cramers_v:.3f}",
            fontsize=12, fontweight='bold'
        )
        ax.set_xlabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", fontsize=10)
        ax.set_ylabel(feature_label, fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')
        legend = ax.legend(title="Dataset", loc='upper right', bbox_to_anchor=(1.0, 1.0), frameon=True)
        plt.setp(legend.get_title(), fontsize=10)
        plt.setp(legend.get_texts(), fontsize=9)
        plt.show()

        print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ–ª–µ–π –ø–æ {full_col_name}:")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
        def _interpret_diff(diff: float, category: str):
            if "[ –ø—Ä–æ–ø—É—Å–∫ ]" in category:
                if diff > 0:
                    return "üö® –ü—Ä–æ–ø—É—Å–∫–∏ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ", "critical", "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö"
                else:
                    return "üü¢ –ü—Ä–æ–ø—É—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã", "low", ""
            elif diff < 1.0:
                return "üü¢ –ù–æ—Ä–º–∞", "low", ""
            elif diff < 3.0:
                return "üü† –í–Ω–∏–º–∞–Ω–∏–µ", "medium", "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å"
            elif diff < 5.0:
                return "üî¥ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ", "high", "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é"
            else:
                return "üí• –ö—Ä–∏—Ç–∏—á–Ω–æ", "critical", "–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"

        stats[['–°—Ç–∞—Ç—É—Å', '–£—Ä–æ–≤–µ–Ω—å', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è']] = stats.apply(
            lambda row: pd.Series(_interpret_diff(row['Œî –¥–æ–ª—è (pp)'], row[feature])), axis=1
        )

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–∏—Å–∫—É
        risk_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        stats['risk_sort'] = stats['–£—Ä–æ–≤–µ–Ω—å'].map(risk_order)
        stats = stats.sort_values(['risk_sort', 'Œî –¥–æ–ª—è (pp)'], ascending=[True, False]).drop(columns='risk_sort')

        # –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
        display_table(
            stats[[
                feature, 'train', 'test', 'train_pct', 'test_pct', 'Œî –¥–æ–ª—è (pp)',
                '–°—Ç–∞—Ç—É—Å', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è'
            ]],
            rows=len(stats),
            float_precision=2,
            styler_func=lambda s: (
                s.format({
                    'train_pct': '{:.1f}%',
                    'test_pct': '{:.1f}%',
                    'Œî –¥–æ–ª—è (pp)': '{:.1f} pp'
                }, na_rep="-")                
                .background_gradient(subset=['train'], cmap='vlag')
                .background_gradient(subset=['test'], cmap='vlag')
                .background_gradient(subset=['Œî –¥–æ–ª—è (pp)'], cmap='Reds')
            )
        )

    else:
        # –ß–∏—Å–ª–æ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫
        try:
            plot_feature_distribution(
                df=combined,
                feature=feature,
                hue='dataset',
                stat='density',
                table_metrics=table_metrics,
                palette=palette
            )
        except NameError:
            print("‚ö†Ô∏è –§—É–Ω–∫—Ü–∏—è plot_feature_distribution –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞.")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def plot_discrete_train_test(
    train: pd.DataFrame,
    test: pd.DataFrame,
    feature: str,
    figsize: Tuple[int, int] = (12, 3),
    palette: Tuple[str, str] = ('#295C96', '#ffa230'),
    table_metrics: Optional[Literal['basic', 'extended']] = None  # –ò–∑–º–µ–Ω–µ–Ω–æ: —Ç–µ–ø–µ—Ä—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
) -> None:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–ò–°–ö–†–ï–¢–ù–û–ì–û –ø—Ä–∏–∑–Ω–∞–∫–∞ –º–µ–∂–¥—É train –∏ test —á–µ—Ä–µ–∑ countplot –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ç–∞–±–ª–∏—Ü—É.

    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–≤—É—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ `sns.countplot` –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö. –¢–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤—ã–≤–æ–¥–∏—Ç—å
    —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏, —Ä–∞–∑–Ω–∏—Ü–µ–π –≤ –¥–æ–ª—è—Ö
    –∏ —Å—Ç–∞—Ç—É—Å–æ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ DATASET_DESCRIPTIONS
    –∏ COLUMN_DESCRIPTIONS –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥–ø–∏—Å–µ–π.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    ----------
    train : pd.DataFrame
        –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞.
    test : pd.DataFrame
        –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞.
    feature : str
        –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'employment_years').
    figsize : tuple, optional
        –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (12, 3)).
    palette : tuple, optional
        –¶–≤–µ—Ç–∞ –¥–ª—è train –∏ test (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ('#295C96', '#ffa230')).
    table_metrics : {'basic', 'extended', None}, optional
        –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã:
        - 'basic': —á–∞—Å—Ç–æ—Ç—ã, –¥–æ–ª–∏, –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (pp), —Å—Ç–∞—Ç—É—Å.
        - 'extended': –∫–∞–∫ 'basic' + Cram√©r‚Äôs V.
        - None (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é): —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –≤—ã–≤–æ–¥–∏—Ç—Å—è.
    """
    # –ü–æ–¥–ø–∏—Å–∏
    train_name, train_desc = label_for_dataset(train, separator="‚Ä¢")
    test_name, test_desc = label_for_dataset(test, separator="‚Ä¢")
    feature_name, feature_desc = label_for_column(feature, separator="‚Ä¢")
    full_feature_label = f"{feature_name}{feature_desc}" if feature_desc else feature_name

    # –ì—Ä–∞—Ñ–∏–∫
    fig, axes = plt.subplots(1, 2, figsize=figsize)
    fig.suptitle(
        f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞: {full_feature_label}",
        fontsize=12,
        fontweight=200,
        y=1.02
    )

    sns.countplot(data=train, x=feature, ax=axes[0], color=palette[0], alpha=0.8)
    axes[0].set_title(f"{train_name}", fontsize=9)
    axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=9)
    axes[0].set_xlabel(full_feature_label, fontsize=9)
    axes[0].tick_params(axis='x', labelsize=8)
    axes[0].grid(axis='y', alpha=0.3)

    sns.countplot(data=test, x=feature, ax=axes[1], color=palette[1], alpha=0.8)
    axes[1].set_title(f"{test_name}", fontsize=9)
    axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=9)
    axes[1].set_xlabel(full_feature_label, fontsize=9)
    axes[1].tick_params(axis='x', labelsize=8)
    axes[1].grid(axis='y', alpha=0.3)

    fig.text(0.5, -0.02, f"–ü—Ä–∏–∑–Ω–∞–∫: {full_feature_label}", ha='center', fontsize=9, style='italic')
    plt.tight_layout()
    plt.show()

    # –¢–∞–±–ª–∏—Ü–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if table_metrics is None:
        return

    print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–ª–µ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ '{feature_name}'{feature_desc}:")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_labeled = train[[feature]].copy()
    train_labeled['dataset'] = 'train'
    test_labeled = test[[feature]].copy()
    test_labeled['dataset'] = 'test'
    combined = pd.concat([train_labeled, test_labeled], ignore_index=True)

    counts = pd.crosstab(combined[feature], combined['dataset'], dropna=False)
    total_train = counts['train'].sum()
    total_test = counts['test'].sum()

    result = pd.DataFrame({
        'train': counts['train'],
        'test': counts['test'],
        'train_pct': (counts['train'] / total_train * 100).round(1),
        'test_pct': (counts['test'] / total_test * 100).round(1)
    }).reset_index()
    result['Œî –¥–æ–ª—è (pp)'] = (result['train_pct'] - result['test_pct']).abs().round(1)

    def _get_status(diff: float) -> Tuple[str, str]:
        if diff < 1.0:
            return "üü¢ –ù–æ—Ä–º–∞", ""
        elif diff < 3.0:
            return "üü† –í–Ω–∏–º–∞–Ω–∏–µ", "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å"
        else:
            return "üî¥ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ", "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é"

    result[['–°—Ç–∞—Ç—É—Å', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è']] = result['Œî –¥–æ–ª—è (pp)'].apply(
        lambda x: pd.Series(_get_status(x))
    )
    result = result.sort_values('Œî –¥–æ–ª—è (pp)', ascending=False)

    display_table(
        result,
        rows=len(result),
        float_precision=1,
        styler_func=lambda s: s.format({
            'train_pct': '{:.1f}%',
            'test_pct': '{:.1f}%',
            'Œî –¥–æ–ª—è (pp)': '{:.1f} pp'
        }).background_gradient(subset=['Œî –¥–æ–ª—è (pp)'], cmap='Reds')
    )

    # Cram√©r‚Äôs V (—Ç–æ–ª—å–∫–æ –≤ 'extended')
    if table_metrics == 'extended':
        observed = counts.copy()
        try:
            chi2, p_val, dof, expected = chi2_contingency(observed)
            n = observed.sum().sum()
            min_dim = min(observed.shape) - 1
            cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 and n > 0 else 0.0
            cramers_v = min(cramers_v, 1.0)
            print(f"\nüìä Cram√©r‚Äôs V = {cramers_v:.3f} {'(—Å–ª–∞–±–∞—è —Å–≤—è–∑—å)' if cramers_v < 0.1 else '(—É–º–µ—Ä–µ–Ω–Ω–∞—è —Å–≤—è–∑—å)' if cramers_v < 0.3 else '(—Å–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å)'}")
        except Exception as e:
            print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å Cram√©r‚Äôs V: {e}")
   


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢



# plot_compare_train_test_ecdf - –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ –º–µ–∂–¥—É train –∏ test
def plot_compare_train_test_ecdf(
    train: pd.DataFrame,
    test: pd.DataFrame,
    feature: str,
    feature_label: Optional[str] = None,
    palette: str = "tab10",
    show_stats: bool = True,
    figsize: tuple = (18, 5)
) -> None:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ –º–µ–∂–¥—É train –∏ test —Å –ø–æ–º–æ—â—å—é —Ç—Ä—ë—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞ (—á–∏—Å–ª–æ–≤–æ–π / –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π).
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:
        - –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ + KDE
        - Boxplot
        - ECDF + KS-—Ç–µ—Å—Ç
        
        –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:
        - Barplot –¥–æ–ª–µ–π
        - –¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏
        - Cram√©r's V + Chi¬≤ —Ç–µ—Å—Ç
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ COLUMN_DESCRIPTIONS –∏ DATASET_DESCRIPTIONS.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        train/test: pd.DataFrame - –≤—ã–±–æ—Ä–∫–∏
        feature: str - –∏–º—è –ø—Ä–∏–∑–Ω–∞–∫–∞
        feature_label: Optional[str] - —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        palette: str - –ø–∞–ª–∏—Ç—Ä–∞ seaborn
        show_stats: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        figsize: tuple - —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    """
    if feature not in train.columns or feature not in test.columns:
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ train –∏–ª–∏ test")

    # –ü–æ–¥–ø–∏—Å—å –ø—Ä–∏–∑–Ω–∞–∫–∞
    col_name, col_desc = label_for_column(feature, separator='‚Ä¢')
    if not feature_label:
        feature_label = f"{col_name}{col_desc}" if col_desc else col_name

    # –¶–≤–µ—Ç–∞ - –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
    colors = sns.color_palette(palette, n_colors=2)
    color_train, color_test = colors[0], colors[1]

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞
    is_numeric = pd.api.types.is_numeric_dtype(train[feature]) and pd.api.types.is_numeric_dtype(test[feature])
    n_unique_train = train[feature].nunique()
    n_unique_test = test[feature].nunique()

    # –ï—Å–ª–∏ —á–∏—Å–ª–æ–≤–æ–π –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if is_numeric and n_unique_train > 20 and n_unique_test > 20:
        # –ß–ò–°–õ–û–í–û–ô –†–ï–ñ–ò–ú
        colors = sns.color_palette(palette, n_colors=2)
        color_train, color_test = colors[0], colors[1]

        train_data = train[feature].dropna()
        test_data = test[feature].dropna()

        if len(train_data) == 0 or len(test_data) == 0:
            print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ {feature_label}")
            return

        fig, axes = plt.subplots(1, 3, figsize=figsize)

        # 1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ + KDE
        axes[0].hist(train_data, bins=20, alpha=0.7, color=color_train, label='Train', density=True)
        axes[0].hist(test_data, bins=20, alpha=0.7, color=color_test, label='Test', density=True)
        sns.kdeplot(data=train_data, color=color_train, ax=axes[0], linewidth=2)
        sns.kdeplot(data=test_data, color=color_test, ax=axes[0], linewidth=2)
        axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: Train vs Test')
        axes[0].set_xlabel(feature_label)
        axes[0].set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # 2. Boxplot
        data_to_plot = [train_data, test_data]
        bp = axes[1].boxplot(data_to_plot, labels=['Train', 'Test'], patch_artist=True,
                            boxprops=dict(facecolor=color_train), medianprops=dict(color='white'))
        bp['boxes'][1].set_facecolor(color_test)
        axes[1].set_title('Boxplot: Train vs Test')
        axes[1].set_ylabel(feature_label)
        axes[1].grid(True, alpha=0.3)

        # 3. ECDF
        try:
            from statsmodels.distributions.empirical_distribution import ECDF
            ecdf_train = ECDF(train_data)
            ecdf_test = ECDF(test_data)
            axes[2].plot(ecdf_train.x, ecdf_train.y, color=color_train, label='Train', linewidth=2)
            axes[2].plot(ecdf_test.x, ecdf_test.y, color=color_test, label='Test', linewidth=2)
            axes[2].set_title('ECDF: Train vs Test')
            axes[2].set_xlabel(feature_label)
            axes[2].set_ylabel('–ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
            axes[2].legend()
            axes[2].grid(True, linestyle='--', alpha=0.6)
        except ImportError:
            axes[2].text(0.5, 0.5, 'statsmodels –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞', ha='center', va='center',
                        transform=axes[2].transAxes)
            axes[2].set_title('ECDF: –æ—à–∏–±–∫–∞')

        plt.tight_layout()
        plt.show()

        if show_stats:
            ks_stat, p_value = stats.ks_2samp(train_data, test_data)
            print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è '{feature_label}':")
            print(f"   Kolmogorov-Smirnov test:")
            print(f"     ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {ks_stat:.4f}")
            print(f"     ‚Ä¢ p-value: {p_value:.4f}")
            if p_value < 0.05:
                print("   üî∫ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è (H‚ÇÅ)")
            else:
                print("   ‚úîÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è (H‚ÇÄ)")

    else:
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
        fig, axes = plt.subplots(1, 3, figsize=figsize)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        combined = pd.concat([
            train[[feature]].assign(dataset='train'),
            test[[feature]].assign(dataset='test')
        ], ignore_index=True).dropna()

        # 1. Barplot –¥–æ–ª–µ–π
        counts = pd.crosstab(combined[feature], combined['dataset'])
        counts.plot(kind='bar', ax=axes[0], color=[colors[0], colors[1]], alpha=0.8, edgecolor='white')
        axes[0].set_title('–ß–∞—Å—Ç–æ—Ç—ã: Train vs Test')
        axes[0].set_xlabel(feature_label)
        axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        axes[0].tick_params(axis='x', rotation=45)
        axes[0].grid(True, axis='y', alpha=0.3)

        # 2. Boxplot - –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Ç–∞–±–ª–∏—Ü—É —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏
        table = pd.crosstab(combined['dataset'], combined[feature])
        ax2_text = "–¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏:\n\n" + table.to_string()
        axes[1].text(0.5, 0.5, ax2_text, ha='center', va='center', transform=axes[1].transAxes,
                    fontfamily='monospace', fontsize=9)
        axes[1].set_title('–¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏')
        axes[1].axis('off')

        # 3. ECDF - –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ Cram√©r's V
        observed = pd.crosstab(combined['dataset'], combined[feature])
        chi2, p_val, dof, expected = chi2_contingency(observed)
        n = observed.sum().sum()
        min_dim = min(observed.shape) - 1
        cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0.0

        stat_text = f"Cramer's V = {cramers_v:.4f}\nœá¬≤ p-value = {p_val:.4f}"
        axes[2].text(0.5, 0.5, stat_text, ha='center', va='center', transform=axes[2].transAxes,
                    fontsize=12, bbox=dict(boxstyle="round", facecolor="wheat"))
        axes[2].set_title('–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å')
        axes[2].axis('off')

        plt.tight_layout()
        plt.show()

        if show_stats:
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è '{feature_label}':")
            print(f"   Cramer's V: {cramers_v:.4f}")
            print(f"   œá¬≤ p-value: {p_val:.4f}")
            if p_val < 0.05 and cramers_v > 0.1:
                print("   üî∫ –ó–Ω–∞—á–∏–º–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (H‚ÇÅ)")
            else:
                print("   ‚úîÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã (H‚ÇÄ)")




#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


def plot_discrete_train_test(
    train: pd.DataFrame,
    test: pd.DataFrame,
    feature: str,
    figsize: Tuple[int, int] = (12, 3),
    palette: Tuple[str, str] = ('#295C96', '#ffa230'),
    table_metrics: Optional[Literal['basic', 'extended']] = None  # –ò–∑–º–µ–Ω–µ–Ω–æ: —Ç–µ–ø–µ—Ä—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
) -> None:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–ò–°–ö–†–ï–¢–ù–û–ì–û –ø—Ä–∏–∑–Ω–∞–∫–∞ –º–µ–∂–¥—É train –∏ test —á–µ—Ä–µ–∑ countplot –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ç–∞–±–ª–∏—Ü—É.

    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–≤—É—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ `sns.countplot` –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö. –¢–∞–∫–∂–µ –º–æ–∂–µ—Ç –≤—ã–≤–æ–¥–∏—Ç—å
    —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏, —Ä–∞–∑–Ω–∏—Ü–µ–π –≤ –¥–æ–ª—è—Ö
    –∏ —Å—Ç–∞—Ç—É—Å–æ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ DATASET_DESCRIPTIONS
    –∏ COLUMN_DESCRIPTIONS –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥–ø–∏—Å–µ–π.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    ----------
    train : pd.DataFrame
        –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞.
    test : pd.DataFrame
        –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞.
    feature : str
        –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'employment_years').
    figsize : tuple, optional
        –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (12, 3)).
    palette : tuple, optional
        –¶–≤–µ—Ç–∞ –¥–ª—è train –∏ test (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ('#295C96', '#ffa230')).
    table_metrics : {'basic', 'extended', None}, optional
        –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü—ã:
        - 'basic': —á–∞—Å—Ç–æ—Ç—ã, –¥–æ–ª–∏, –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (pp), —Å—Ç–∞—Ç—É—Å.
        - 'extended': –∫–∞–∫ 'basic' + Cram√©r‚Äôs V.
        - None (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é): —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –≤—ã–≤–æ–¥–∏—Ç—Å—è.
    """
    # –ü–æ–¥–ø–∏—Å–∏
    train_name, train_desc = label_for_dataset(train, separator="‚Ä¢")
    test_name, test_desc = label_for_dataset(test, separator="‚Ä¢")
    feature_name, feature_desc = label_for_column(feature, separator="‚Ä¢")
    full_feature_label = f"{feature_name}{feature_desc}" if feature_desc else feature_name

    # –ì—Ä–∞—Ñ–∏–∫
    fig, axes = plt.subplots(1, 2, figsize=figsize)
    fig.suptitle(
        f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞: {full_feature_label}",
        fontsize=12,
        fontweight=200,
        y=1.02
    )

    sns.countplot(data=train, x=feature, ax=axes[0], color=palette[0], alpha=0.8)
    axes[0].set_title(f"{train_name}", fontsize=9)
    axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=9)
    axes[0].set_xlabel(full_feature_label, fontsize=9)
    axes[0].tick_params(axis='x', labelsize=8)
    axes[0].grid(axis='y', alpha=0.3)

    sns.countplot(data=test, x=feature, ax=axes[1], color=palette[1], alpha=0.8)
    axes[1].set_title(f"{test_name}", fontsize=9)
    axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=9)
    axes[1].set_xlabel(full_feature_label, fontsize=9)
    axes[1].tick_params(axis='x', labelsize=8)
    axes[1].grid(axis='y', alpha=0.3)

    fig.text(0.5, -0.02, f"–ü—Ä–∏–∑–Ω–∞–∫: {full_feature_label}", ha='center', fontsize=9, style='italic')
    plt.tight_layout()
    plt.show()

    # –¢–∞–±–ª–∏—Ü–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if table_metrics is None:
        return

    print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–ª–µ–π –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ '{feature_name}'{feature_desc}:")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_labeled = train[[feature]].copy()
    train_labeled['dataset'] = 'train'
    test_labeled = test[[feature]].copy()
    test_labeled['dataset'] = 'test'
    combined = pd.concat([train_labeled, test_labeled], ignore_index=True)

    counts = pd.crosstab(combined[feature], combined['dataset'], dropna=False)
    total_train = counts['train'].sum()
    total_test = counts['test'].sum()

    result = pd.DataFrame({
        'train': counts['train'],
        'test': counts['test'],
        'train_pct': (counts['train'] / total_train * 100).round(1),
        'test_pct': (counts['test'] / total_test * 100).round(1)
    }).reset_index()
    result['Œî –¥–æ–ª—è (pp)'] = (result['train_pct'] - result['test_pct']).abs().round(1)

    def _get_status(diff: float) -> Tuple[str, str]:
        if diff < 1.0:
            return "üü¢ –ù–æ—Ä–º–∞", ""
        elif diff < 3.0:
            return "üü† –í–Ω–∏–º–∞–Ω–∏–µ", "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å"
        else:
            return "üî¥ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ", "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é"

    result[['–°—Ç–∞—Ç—É—Å', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è']] = result['Œî –¥–æ–ª—è (pp)'].apply(
        lambda x: pd.Series(_get_status(x))
    )
    result = result.sort_values('Œî –¥–æ–ª—è (pp)', ascending=False)

    display_table(
        result,
        rows=len(result),
        float_precision=1,
        styler_func=lambda s: s.format({
            'train_pct': '{:.1f}%',
            'test_pct': '{:.1f}%',
            'Œî –¥–æ–ª—è (pp)': '{:.1f} pp'
        }).background_gradient(subset=['Œî –¥–æ–ª—è (pp)'], cmap='Reds')
    )

    # Cram√©r‚Äôs V (—Ç–æ–ª—å–∫–æ –≤ 'extended')
    if table_metrics == 'extended':
        observed = counts.copy()
        try:
            chi2, p_val, dof, expected = chi2_contingency(observed)
            n = observed.sum().sum()
            min_dim = min(observed.shape) - 1
            cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 and n > 0 else 0.0
            cramers_v = min(cramers_v, 1.0)
            print(f"\nüìä Cram√©r‚Äôs V = {cramers_v:.3f} {'(—Å–ª–∞–±–∞—è —Å–≤—è–∑—å)' if cramers_v < 0.1 else '(—É–º–µ—Ä–µ–Ω–Ω–∞—è —Å–≤—è–∑—å)' if cramers_v < 0.3 else '(—Å–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å)'}")
        except Exception as e:
            print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å Cram√©r‚Äôs V: {e}")
            



#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢



def plot_shap_summary(
    model_pipeline,
    X_test,
    feature_names_mapping=None,
    max_display=10,
    figsize=(12, 8),
    title_fontsize=12,
    axis_fontsize=8,
    label_mode='combined',
    preprocessor_step_name='preprocessor'
):
    """
    –°—Ç—Ä–æ–∏—Ç SHAP Summary Plot –¥–ª—è –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π –≤ —Å–ª–æ–∂–Ω–æ–º –ø–∞–π–ø–ª–∞–π–Ω–µ.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - model_pipeline: –æ–±—É—á–µ–Ω–Ω—ã–π Pipeline
    - X_test: —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (DataFrame) –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
    - feature_names_mapping: dict –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    - max_display: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ
    - figsize: —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
    - title_fontsize: —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    - axis_fontsize: —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –º–µ—Ç–æ–∫ –æ—Å–µ–π
    - label_mode: —Å–ø–æ—Å–æ–± –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        'name' - —Ç–æ–ª—å–∫–æ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ (dept)
        'description' - —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ (–æ—Ç–¥–µ–ª)
        'combined' - –∏–º—è –∏ –æ–ø–∏—Å–∞–Ω–∏–µ (dept ‚Ä¢ –æ—Ç–¥–µ–ª) [–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é]
    - preprocessor_step_name: –∏–º—è —à–∞–≥–∞ –≤ model_pipeline, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'preprocessor')
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    if 'model' not in model_pipeline.named_steps:
        raise ValueError("Pipeline –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —à–∞–≥ —Å –∏–º–µ–Ω–µ–º 'model'")
    model = model_pipeline.named_steps['model']

    if preprocessor_step_name not in model_pipeline.named_steps:
        raise ValueError(f"Pipeline –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —à–∞–≥ '{preprocessor_step_name}'")
    preprocessor = model_pipeline.named_steps[preprocessor_step_name]

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
    X_test_processed = preprocessor.transform(X_test)

    # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    try:
        # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± (scikit-learn >= 1.0)
        if hasattr(preprocessor, 'get_feature_names_out'):
            feature_names = preprocessor.get_feature_names_out()
        else:
            raise AttributeError()
    except (AttributeError, NotImplementedError):
        # Fallback: —Å–æ–±–µ—Ä—ë–º –≤—Ä—É—á–Ω—É—é (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        feature_names = []
        if hasattr(preprocessor, 'transformers_'):
            for name, trans, cols in preprocessor.transformers_:
                if trans == 'drop' or trans is None:
                    continue
                if hasattr(trans, 'get_feature_names_out'):
                    try:
                        feature_names.extend(trans.get_feature_names_out(cols))
                    except:
                        feature_names.extend([f"{name}_{i}" for i in range(len(cols))])
                else:
                    feature_names.extend(cols)
        else:
            feature_names = [f"feature_{i}" for i in range(X_test_processed.shape[1])]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö –∏–º—ë–Ω (–≤–∞—à–∞ –ª–æ–≥–∏–∫–∞)
    display_names = []
    for name in feature_names:
        if feature_names_mapping and name in feature_names_mapping:
            display_name = feature_names_mapping[name]
        else:
            try:
                col_name, col_desc = label_for_column(name, separator="‚Ä¢")
                if label_mode == 'name':
                    display_name = col_name
                elif label_mode == 'description':
                    display_name = col_desc.strip()
                elif label_mode == 'combined':
                    display_name = f"{col_name}{col_desc}"
                else:
                    display_name = name
            except:
                display_name = name
        display_names.append(display_name)

    # SHAP –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test_processed)

    plt.figure(figsize=figsize)
    shap.summary_plot(
        shap_values,
        X_test_processed,
        feature_names=display_names,
        max_display=max_display,
        show=False
    )

    ax = plt.gca()
    ax.set_title("SHAP Summary Plot: –í–∫–ª–∞–¥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", fontsize=title_fontsize, pad=20)
    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_fontsize(axis_fontsize)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()








#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢





def plot_numerical_profile(
    df: pd.DataFrame,
    feature: str,
    compare_df: Optional[pd.DataFrame] = None,
    compare_label: str = "test",
    palette: str = "tab10",
    figsize: Tuple[int, int] = (16, 4),
    show_report: bool = True,
    show_recommendations: bool = True
) -> None:
    """
    –°—Ç—Ä–æ–∏—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞:
        - –±–∏–Ω–∞—Ä–Ω—ã–π (2 –∑–Ω–∞—á–µ–Ω–∏—è) ‚Üí barplot + boxplot + ECDF
        - –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π (3‚Äì20 —Ü–µ–ª—ã—Ö) ‚Üí barplot + boxplot + ECDF
        - –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π ‚Üí hist+KDE + boxplot + Q-Q + ECDF
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–æ–π –≤—ã–±–æ—Ä–∫–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, test).
        –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df: pd.DataFrame - –æ—Å–Ω–æ–≤–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, train)
        feature: str - –∏–º—è —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        compare_df: Optional[pd.DataFrame] - –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, test)
        compare_label: str - –º–µ—Ç–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "test")
        palette: str - –ø–∞–ª–∏—Ç—Ä–∞ seaborn (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "tab10")
        figsize: Tuple[int, int] - —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
        show_report: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        show_recommendations: bool - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        None - –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ matplotlib –∏ display_table

    –ü—Ä–∏–º–µ—Ä—ã:
        1. plot_numerical_profile(train_df, 'stress_level', compare_df=test_df, compare_label="test")
        2. plot_numerical_profile(train_df, 'stress_level')

    –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
        from typing import Optional, Tuple
    """
    from scipy import stats as scipy_stats
    from statsmodels.distributions.empirical_distribution import ECDF

    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    if feature not in df.columns:
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
    if not pd.api.types.is_numeric_dtype(df[feature]):
        raise ValueError(f"–ü—Ä–∏–∑–Ω–∞–∫ '{feature}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º")

    # –î–∞–Ω–Ω—ã–µ
    data = df[feature].dropna()
    n_total = len(df[feature])
    n_valid = len(data)
    n_missing = n_total - n_valid

    if n_valid == 0:
        print(f"‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫ '{feature}' —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–ø—É—Å–∫–∏")
        return

    # –ü–æ–¥–ø–∏—Å–∏
    col_name, col_desc = label_for_column(feature, separator='‚Ä¢')
    full_label = f"{col_name}{col_desc}" if col_desc else col_name
    dataset_name, dataset_desc = label_for_dataset(df, separator='‚Ä¢')

    print(f"üîç –ü—Ä–æ—Ñ–∏–ª—å –ø—Ä–∏–∑–Ω–∞–∫–∞: {full_label}")
    print(f"üóÉÔ∏è –î–∞—Ç–∞—Å–µ—Ç: {dataset_name}{dataset_desc}")
    print(f"     ‚Ä¢ –í–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {n_valid:,} (–ø—Ä–æ–ø—É—Å–∫–æ–≤: {n_missing})")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    mean_val = data.mean()
    median_val = data.median()
    std_val = data.std()
    min_val = data.min()
    max_val = data.max()
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    skew_val = scipy_stats.skew(data)
    kurt_val = scipy_stats.kurtosis(data)
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    n_outliers = len(outliers)

    # –¢–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–∞
    n_unique = data.nunique()
    is_integer = pd.api.types.is_integer_dtype(data)
    if n_unique <= 1:
        feature_type = "üîá –ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π"
    elif n_unique == 2:
        feature_type = "üíä –±–∏–Ω–∞—Ä–Ω—ã–π"
    elif n_unique <= 20 and is_integer:
        feature_type = "üî¢ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π"
    else:
        feature_type = "üî¢ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π"

    print(f"     ‚Ä¢ –¢–∏–ø: {feature_type} ({n_unique} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)")

    # –¶–≤–µ—Ç–∞
    colors = sns.color_palette(palette, n_colors=2)
    color_train = colors[0]
    color_test = colors[1]

    # –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
    if feature_type in ["üíä –±–∏–Ω–∞—Ä–Ω—ã–π", "üî¢ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π"]:
        # –î–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö: 3 –≥—Ä–∞—Ñ–∏–∫–∞
        fig, axes = plt.subplots(1, 3, figsize=figsize)
        
        # 1. Barplot
        value_counts_train = data.value_counts().sort_index()
        bars_train = axes[0].bar(
            value_counts_train.index, value_counts_train.values,
            color=color_train, alpha=0.8, label="train", width=0.4
        )
        if compare_df is not None and feature in compare_df.columns:
            test_data = compare_df[feature].dropna()
            value_counts_test = test_data.value_counts().sort_index()
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            all_keys = sorted(set(value_counts_train.index) | set(value_counts_test.index))
            train_vals = [value_counts_train.get(k, 0) for k in all_keys]
            test_vals = [value_counts_test.get(k, 0) for k in all_keys]
            bars_test = axes[0].bar(
                [k + 0.4 for k in all_keys], test_vals,
                color=color_test, alpha=0.8, label=compare_label, width=0.4, hatch='//'
            )
        axes[0].set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π", fontsize=11, fontweight='bold')
        axes[0].set_xlabel(full_label)
        axes[0].set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
        axes[0].legend()
        axes[0].grid(True, linestyle='--', alpha=0.4)

        # 2. Boxplot
        box_data = [data]
        box_labels = ["train"]
        box_colors = [color_train]
        if compare_df is not None and feature in compare_df.columns:
            box_data.append(compare_df[feature].dropna())
            box_labels.append(compare_label)
            box_colors.append(color_test)
        bplot = axes[1].boxplot(
            box_data, labels=box_labels, patch_artist=True,
            medianprops=dict(color='white', linewidth=1.5)
        )
        for patch, color in zip(bplot['boxes'], box_colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        axes[1].set_title("Boxplot", fontsize=11, fontweight='bold')
        axes[1].set_ylabel(full_label)
        axes[1].grid(True, linestyle='--', alpha=0.4)

        # 3. ECDF
        ecdf_train = ECDF(data)
        axes[2].step(ecdf_train.x, ecdf_train.y, where='post', color=color_train, linewidth=2.5, label="train")
        if compare_df is not None and feature in compare_df.columns:
            ecdf_test = ECDF(compare_df[feature].dropna())
            axes[2].step(ecdf_test.x, ecdf_test.y, where='post', color=color_test, linewidth=2.5, linestyle='--', label=compare_label)
        axes[2].set_title("ECDF", fontsize=11, fontweight='bold')
        axes[2].set_xlabel(full_label)
        axes[2].set_ylabel("–ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å")
        axes[2].legend()
        axes[2].grid(True, linestyle='--', alpha=0.4)

    else:  # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π
        fig, axes = plt.subplots(1, 4, figsize=figsize)
        
        # 1. Hist + KDE
        bins = min(50, max(10, int(np.sqrt(n_valid))))
        axes[0].hist(data, bins=bins, density=True, alpha=0.6, color=color_train, edgecolor='white', linewidth=0.5, label="train")
        if n_unique > 10:
            sns.kdeplot(data, ax=axes[0], color=color_train, linewidth=2.5)
        if compare_df is not None and feature in compare_df.columns:
            comp_data = compare_df[feature].dropna()
            axes[0].hist(comp_data, bins=bins, density=True, alpha=0.4, color=color_test, edgecolor='white', linewidth=0.5, label=compare_label)
            if len(comp_data) > 10:
                sns.kdeplot(comp_data, ax=axes[0], color=color_test, linewidth=2.5, linestyle="--")
        axes[0].set_title("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ + KDE", fontsize=11, fontweight='bold')
        axes[0].set_xlabel(full_label)
        axes[0].legend()
        axes[0].grid(True, linestyle='--', alpha=0.4)

        # 2. Boxplot
        box_data = [data]
        box_labels = ["train"]
        box_colors = [color_train]
        if compare_df is not None and feature in compare_df.columns:
            box_data.append(compare_df[feature].dropna())
            box_labels.append(compare_label)
            box_colors.append(color_test)
        bplot = axes[1].boxplot(
            box_data, labels=box_labels, patch_artist=True,
            medianprops=dict(color='white', linewidth=1.5)
        )
        for patch, color in zip(bplot['boxes'], box_colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        axes[1].set_title("Boxplot", fontsize=11, fontweight='bold')
        axes[1].set_ylabel(full_label)
        axes[1].grid(True, linestyle='--', alpha=0.4)

        # 3. Q-Q plot
        scipy_stats.probplot(data, dist="norm", plot=axes[2])
        axes[2].get_lines()[0].set_markerfacecolor(color_train)
        axes[2].get_lines()[0].set_markersize(4)
        axes[2].get_lines()[1].set_color("red")
        axes[2].set_title("Q-Q plot (–Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å)", fontsize=11, fontweight='bold')
        axes[2].grid(True, linestyle='--', alpha=0.5)

        # 4. ECDF
        ecdf_train = ECDF(data)
        axes[3].step(ecdf_train.x, ecdf_train.y, where='post', color=color_train, linewidth=2.5, label="train")
        if compare_df is not None and feature in compare_df.columns:
            ecdf_test = ECDF(compare_df[feature].dropna())
            axes[3].step(ecdf_test.x, ecdf_test.y, where='post', color=color_test, linewidth=2.5, linestyle='--', label=compare_label)
        axes[3].set_title("ECDF", fontsize=11, fontweight='bold')
        axes[3].set_xlabel(full_label)
        axes[3].set_ylabel("–ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å")
        axes[3].legend()
        axes[3].grid(True, linestyle='--', alpha=0.4)

    plt.tight_layout()
    plt.show()

    # –°–í–û–î–ö–ê –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
    if show_report:
        stats_df = pd.DataFrame({
            "–ú–µ—Ç—Ä–∏–∫–∞": ["–°—Ä–µ–¥–Ω–µ–µ", "–ú–µ–¥–∏–∞–Ω–∞", "–°—Ç–¥", "–ú–∏–Ω", "–ú–∞–∫—Å", "Q1", "Q3", "IQR", "–ê—Å–∏–º–º–µ—Ç—Ä–∏—è", "–≠–∫—Å—Ü–µ—Å—Å", "–í—ã–±—Ä–æ—Å—ã"],
            "–ó–Ω–∞—á–µ–Ω–∏–µ": [mean_val, median_val, std_val, min_val, max_val, q1, q3, iqr, skew_val, kurt_val, n_outliers]
        })
        print(f"\nüìã –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ({n_valid} –∑–Ω–∞—á–µ–Ω–∏–π):")
        display_table(
            stats_df,
            rows=len(stats_df),
            float_precision=3,
            styler_func=lambda s: s.format({"–ó–Ω–∞—á–µ–Ω–∏–µ": "{:.3f}"})
        )

    if show_recommendations:
        print(f"\nüîç –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        if feature_type == "üíä –±–∏–Ω–∞—Ä–Ω—ã–π":
            print("   ‚úîÔ∏è –ü—Ä–∏–∑–Ω–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–π - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–∞–∫ –µ—Å—Ç—å –∏–ª–∏ –∑–∞–∫–æ–¥–∏—Ä—É–π—Ç–µ –≤ 0/1")
        elif feature_type == "üî¢ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π":
            print("   ‚ÑπÔ∏è –ü—Ä–∏–∑–Ω–∞–∫ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ —á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π")
        else:
            # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π
            if abs(skew_val) > 1.0:
                print("   üì¢ –°–∏–ª—å–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ log/sqrt/Box-Cox —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            elif abs(skew_val) > 0.5:
                print("   üí° –£–º–µ—Ä–µ–Ω–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è - –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é")
            else:
                print("   ‚úîÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–ª–∏–∑–∫–æ –∫ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–º—É")

        # –í—ã–±—Ä–æ—Å—ã
        if n_outliers > 0:
            pct_out = n_outliers / n_valid * 100
            if pct_out > 5:
                print(f"   ‚ö†Ô∏è –ú–Ω–æ–≥–æ –≤—ã–±—Ä–æ—Å–æ–≤ ({pct_out:.1f}%) - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Ö –ø—Ä–∏—Ä–æ–¥—É")
            else:
                print(f"   ‚úîÔ∏è –í—ã–±—Ä–æ—Å—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã ({pct_out:.1f}%)")

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        if min_val >= 0 and max_val <= 1:
            print("   ‚úîÔ∏è –î–∞–Ω–Ω—ã–µ —É–∂–µ –≤ [0, 1] - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        elif std_val > 100:
            print("   ‚ö†Ô∏è –®–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—é")
        else:
            print("   üí° –î–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ")

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
        if compare_df is not None and feature in compare_df.columns:
            from scipy.stats import ks_2samp
            comp_data = compare_df[feature].dropna()
            if len(comp_data) > 0:
                ks_stat, p_val = ks_2samp(data, comp_data)
                print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å {compare_label}:")
                print(f"   ‚Ä¢ Kolmogorov-Smirnov: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞={ks_stat:.4f}, p={p_val:.4f}")
                if p_val < 0.05:
                    print("   üî∫ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è (–¥—Ä–∏—Ñ—Ç!)")
                else:
                    print("   ‚úîÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã")



#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢
def plot_missing_summary(
    df: pd.DataFrame,
    threshold: float = 0.0,
    top_n: Optional[int] = None,
    figsize: Tuple[float, float] = (14, None),
    palette: str = "Reds_r",
    show_values: bool = True,
    value_threshold: float = 1.5,
    grid_color: str = "#e0e0e0",
    spine_color: str = "#ddd",
    title: Optional[str] = None,
    xlabel: str = "–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ (%)",
    ylabel: str = "",
    xticks_step: int = 10,
    dpi: Optional[int] = None,
    use_descriptions: bool = True
) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º –≤ –≤–∏–¥–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –§—É–Ω–∫—Ü–∏—è —Å—Ç—Ä–æ–∏—Ç —ç—Å—Ç–µ—Ç–∏—á–Ω—É—é –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é barplot-–¥–∏–∞–≥—Ä–∞–º–º—É,
        –ø–æ–∫–∞–∑—ã–≤–∞—é—â—É—é –¥–æ–ª—é –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É, –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—é —Ü–≤–µ—Ç–æ–≤ –∏ —Å–µ—Ç–∫–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç COLUMN_DESCRIPTIONS –¥–ª—è –ø–æ–¥–ø–∏—Å–µ–π –∫–æ–ª–æ–Ω–æ–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ).
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df : pd.DataFrame
            –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º.
        threshold : float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.0
            –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏.
        top_n : Optional[int], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –°–∫–æ–ª—å–∫–æ —Ç–æ–ø-–∫–æ–ª–æ–Ω–æ–∫ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å.
        figsize : Tuple[float, float], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (14, None)
            –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã. –í—ã—Å–æ—Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫,
            –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –∫–∞–∫ None.
        palette : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "Reds_r"
            –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ seaborn –¥–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤.
        show_values : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (%) –Ω–∞ –∫–æ–Ω—Ü–∞—Ö —Å—Ç–æ–ª–±—Ü–æ–≤.
        value_threshold : float, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.5
            –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π % –ø—Ä–æ–ø—É—Å–∫–∞, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ.
        grid_color : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "#e0e0e0"
            –¶–≤–µ—Ç —Å–µ—Ç–∫–∏.
        spine_color : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "#ddd"
            –¶–≤–µ—Ç –æ—Å–µ–≤—ã—Ö –ª–∏–Ω–∏–π (spines).
        title : Optional[str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞. –ï—Å–ª–∏ None - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
        xlabel : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ (%)"
            –ü–æ–¥–ø–∏—Å—å –æ—Å–∏ X.
        ylabel : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ""
            –ü–æ–¥–ø–∏—Å—å –æ—Å–∏ Y.
        xticks_step : int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10
            –®–∞–≥ –¥–µ–ª–µ–Ω–∏–π –ø–æ –æ—Å–∏ X (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∂–¥—ã–µ 10%).
        dpi : Optional[int], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞.
        use_descriptions : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ COLUMN_DESCRIPTIONS –¥–ª—è –ø–æ–¥–ø–∏—Å–µ–π –∫–æ–ª–æ–Ω–æ–∫.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        None - –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —á–µ—Ä–µ–∑ plt.show().
    """
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    na_percent = df.isna().mean() * 100
    na_percent = na_percent[na_percent > threshold].sort_values(ascending=False)
    
    if top_n is not None:
        na_percent = na_percent.head(top_n)
    
    if na_percent.empty:
        print("‚úîÔ∏è –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        return
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤—ã—Å–æ—Ç–∞
    height = max(4, len(na_percent) * 0.45) if figsize[1] is None else figsize[1]
    figsize_use = (figsize[0], height)
    
    # DPI
    if dpi is not None:
        plt.rcParams['figure.dpi'] = dpi
        plt.rcParams['savefig.dpi'] = dpi
    
    # –°—Ç–∏–ª—å
    sns.set_theme(style="white")
    fig, ax = plt.subplots(figsize=figsize_use)
    
    # –¶–≤–µ—Ç–∞
    colors = sns.color_palette(palette, len(na_percent))
    
    # –ü–æ–¥–ø–∏—Å–∏ —Å —É—á—ë—Ç–æ–º –æ–ø–∏—Å–∞–Ω–∏–π
    if use_descriptions:
        y_labels = [
            label_for_column(col, separator="‚Ä¢", format="string")
            for col in na_percent.index
        ]
    else:
        y_labels = list(na_percent.index)
    
    # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    bars = ax.barh(y_labels, na_percent.values, color=colors, edgecolor='white', linewidth=1.2)
    
    # –ü–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π
    if show_values:
        for i, pct in enumerate(na_percent.values):
            if pct >= value_threshold:
                ax.text(pct + 0.6, i, f"{pct:.1f}%", va='center', fontsize=9, color='#222')
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    if title is None:
        title = f"–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º ({len(na_percent)} –∏–∑ {df.shape[1]} —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏)"
    ax.set_title(title, fontsize=16, weight='bold', pad=20)
    
    # –û—Å–∏
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel)
    ax.set_xlim(0, 100)
    ax.set_xticks(range(0, 101, xticks_step))
    
    # –°–ø–∞–π–Ω—ã
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_color(spine_color)
    ax.spines['bottom'].set_color(spine_color)
    
    # –°–µ—Ç–∫–∞
    ax.grid(True, which='major', axis='both', linestyle='--', linewidth=0.7, color=grid_color, alpha=0.7)
    ax.set_axisbelow(True)
    
    plt.tight_layout()
    plt.show()



#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢
def plot_binary_heatmap(
    df: pd.DataFrame,
    legend_labels: Tuple[str, str] = ("–î–∞–Ω–Ω—ã–µ –µ—Å—Ç—å", "–ü—Ä–æ–ø—É—Å–∫"),
    figsize: Tuple[float, float] = (16, 3),
    cmap_present: str = "#31434E",
    cmap_missing: str = "#f2d70e",
    legend_loc: str = "upper right",
    title: Optional[str] = None,
    xlabel: str = "–°—Ç—Ä–æ–∫–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è",
    ylabel: str = "–ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏",
    rotate_yticks: bool = True,
    max_y_labels: int = 20,
    dpi: Optional[int] = None,
    use_descriptions: bool = True
) -> None:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤ –≤–∏–¥–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –ª–µ–≥–µ–Ω–¥–æ–π.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –§—É–Ω–∫—Ü–∏—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–∏–Ω–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (True/False, 0/1, NaN/not-NaN),
        –≤ –≤–∏–¥–µ heatmap, –≥–¥–µ –∫–∞–∂–¥—ã–π –ø–∏–∫—Å–µ–ª—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —è—á–µ–π–∫–µ:
        - `cmap_negative` ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º" (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å, –æ—à–∏–±–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç),
        - `cmap_positive` ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º" (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–ø—É—Å–∫, –∞–Ω–æ–º–∞–ª–∏—è, —Å–æ–±—ã—Ç–∏–µ).
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ —á–µ—Ä–µ–∑ COLUMN_DESCRIPTIONS.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df : pd.DataFrame
            –ë–∏–Ω–∞—Ä–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º. –ó–Ω–∞—á–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É—é—Ç—Å—è –∫–∞–∫:
            - "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ": True, 1, np.nan (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî not-NaN)
            - "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ": False, 0, not-NaN
        figsize : Tuple[float, float], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (16, 3)
            –†–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞).
        cmap_negative : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "#31434E"
            –¶–≤–µ—Ç –¥–ª—è "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö" –∑–Ω–∞—á–µ–Ω–∏–π (—Ñ–æ–Ω, –Ω–æ—Ä–º–∞, –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å).
        cmap_positive : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "#f2d70e"
            –¶–≤–µ—Ç –¥–ª—è "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö" –∑–Ω–∞—á–µ–Ω–∏–π (–∞–Ω–æ–º–∞–ª–∏–∏, –ø—Ä–æ–ø—É—Å–∫–∏, –æ—à–∏–±–∫–∏).
        legend_labels : Tuple[str, str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç", "–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            –ü–æ–¥–ø–∏—Å–∏ –¥–ª—è –ª–µ–≥–µ–Ω–¥—ã: (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å, –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å).
        legend_loc : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "upper right"
            –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –ª–µ–≥–µ–Ω–¥—ã.
        title : Optional[str], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞. –ï—Å–ª–∏ None ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
        xlabel : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "–ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏ (–Ω–∞–±–ª—é–¥–µ–Ω–∏—è)"
            –ü–æ–¥–ø–∏—Å—å –æ—Å–∏ X.
        ylabel : str, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "–ö–æ–ª–æ–Ω–∫–∏"
            –ü–æ–¥–ø–∏—Å—å –æ—Å–∏ Y.
        rotate_yticks : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –ü–æ–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å –ª–∏ –ø–æ–¥–ø–∏—Å–∏ –∫–æ–ª–æ–Ω–æ–∫.
        max_y_labels : int, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20
            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –ø–æ–¥–ø–∏—Å–∏ –Ω–µ –ø–æ–≤–æ—Ä–∞—á–∏–≤–∞—é—Ç—Å—è.
        dpi : Optional[int], –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None
            –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞.
        use_descriptions : bool, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ COLUMN_DESCRIPTIONS –¥–ª—è –ø–æ–¥–ø–∏—Å–µ–π –∫–æ–ª–æ–Ω–æ–∫.
    
    –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        # 1. –ö–∞—Ä—Ç–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing_df = df.isna()
        plot_binary_heatmap(missing_df, 
                           cmap_negative="#31434E", 
                           cmap_positive="#f2d70e",
                           legend_labels=("–î–∞–Ω–Ω—ã–µ –µ—Å—Ç—å", "–ü—Ä–æ–ø—É—Å–∫"),
                           title="–ü—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö")

        # 2. –ö–∞—Ä—Ç–∞ –∞–Ω–æ–º–∞–ª–∏–π –¥–∞–≤–ª–µ–Ω–∏—è
        error_df = pd.DataFrame({
            'invalid_bp': df['systolic'] <= df['diastolic']
        })
        plot_binary_heatmap(error_df,
                           cmap_negative="#31434E",
                           cmap_positive="#ff4444",
                           legend_labels=("–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ", "–û—à–∏–±–∫–∞: systolic ‚â§ diastolic"),
                           title="–ê–Ω–æ–º–∞–ª–∏–∏ –∞—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –¥–∞–≤–ª–µ–Ω–∏—è")
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        None ‚Äî –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —á–µ—Ä–µ–∑ plt.show().
    """
    # –í—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
    cols_with_na = df.columns[df.isna().any()]
    if len(cols_with_na) == 0:
        print("‚úîÔ∏è –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        return
    
    df_na = df[cols_with_na].isna()
    
    # DPI
    if dpi is not None:
        plt.rcParams['figure.dpi'] = dpi
        plt.rcParams['savefig.dpi'] = dpi
    
    # –°—Ç–∏–ª—å
    sns.set_theme(style="white")
    fig, ax = plt.subplots(figsize=figsize)
    
    # Heatmap –±–µ–∑ colorbar
    sns.heatmap(
        df_na.T,
        cmap=[cmap_present, cmap_missing],
        cbar=False,
        yticklabels=True,
        xticklabels=False,
        ax=ax
    )
    
    # –õ–µ–≥–µ–Ω–¥–∞
    import matplotlib.patches as mpatches
    legend_elements = [
        mpatches.Patch(color=cmap_present, label=legend_labels[0]),
        mpatches.Patch(color=cmap_missing, label=legend_labels[1])
    ]
    ax.legend(handles=legend_elements, loc=legend_loc, title="–°—Ç–∞—Ç—É—Å –¥–∞–Ω–Ω—ã—Ö", frameon=True)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    if title is None:
        title = f"–ü—Ä–æ–ø—É—Å–∫–∏ –≤ {df_na.shape[1]} –∏–∑ {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫"
    ax.set_title(title, fontsize=16, pad=20)
    
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    
    # –ü–æ–¥–ø–∏—Å–∏ –∫–æ–ª–æ–Ω–æ–∫ —Å —É—á—ë—Ç–æ–º –æ–ø–∏—Å–∞–Ω–∏–π
    if use_descriptions:
        y_labels = [
            label_for_column(col, separator="‚Ä¢", format="string")
            for col in cols_with_na
        ]
        ax.set_yticklabels(y_labels)
    
    # –ü–æ–≤–æ—Ä–æ—Ç –ø–æ–¥–ø–∏—Å–µ–π
    if len(cols_with_na) <= max_y_labels:
        plt.yticks(rotation=0)
    elif rotate_yticks:
        plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.show()


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# compare_train_test_overview - EDA: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ train/test —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º data drift
def compare_train_test_overview(
    train: pd.DataFrame,
    test: pd.DataFrame,
    target: Optional[str] = None,
    include: Optional[List[str]] = None,
    exclude: Optional[List[str]] = None,
    numeric_threshold: float = 0.35,
    categorical_threshold: float = 0.015,
    max_categories: int = 100,
    show_plot: Union[bool, Literal["all", "problematic"]] = False,
    palette: str = "tab10"
) -> pd.DataFrame:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π EDA: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ train/test —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º data drift.
    –í—ã—è–≤–ª—è–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ.

    –£–ª—É—á—à–µ–Ω–∏—è:
        ‚Ä¢ –ü–æ—Ä–æ–≥–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–ø—Ä—è–º—É—é: –µ—Å–ª–∏ Cohen's d > numeric_threshold ‚Üí —Ñ–ª–∞–≥—É–µ—Ç—Å—è
        ‚Ä¢ –°—Ç–∞—Ç. –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (p-value) –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ - –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ñ–ª–∞–≥
        ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã: ¬´–°–¥–≤–∏–≥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω¬ª vs ¬´–°–¥–≤–∏–≥ –≤–æ–∑–º–æ–∂–µ–Ω¬ª
        ‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ `target` - –∞–Ω–∞–ª–∏–∑ —Å–¥–≤–∏–≥–∞ —Ç–∞—Ä–≥–µ—Ç–∞ –∏ –µ–≥–æ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        ‚Ä¢ –ê–≤—Ç–æ-–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ ID-–∫–æ–ª–æ–Ω–æ–∫ (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
        ‚Ä¢ –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞: ¬´–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: X –∏–∑ Y¬ª

    –ú–µ—Ç—Ä–∏–∫–∏ –∏ –ø–æ—Ä–æ–≥–∏:
        - –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: **Cohen's d** (–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ —Å—Ä–µ–¥–Ω–∏—Ö)
            ‚Ä¢ –ü–æ—Ä–æ–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.35 (0.2 = –º–∞–ª—ã–π —ç—Ñ—Ñ–µ–∫—Ç, 0.5 = —Å—Ä–µ–¥–Ω–∏–π)
        - –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: **Cramer's V** (—Å–∏–ª–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏)
            ‚Ä¢ –ü–æ—Ä–æ–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1 (0.1 = —Å–ª–∞–±–∞—è —Å–≤—è–∑—å, 0.3 = —É–º–µ—Ä–µ–Ω–Ω–∞—è, 0.5 = —Å–∏–ª—å–Ω–∞—è)
        - –î–ª—è —Ç–∞—Ä–≥–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ (0.1) –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        train: pd.DataFrame - –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞
        test: pd.DataFrame - —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞
        target: Optional[str] - —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–∞–≤—Ç–æ–∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
        include/exclude: Optional[List[str]] - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        numeric_threshold: float - –ø–æ—Ä–æ–≥ Cohen's d –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.35)
        categorical_threshold: float - –ø–æ—Ä–æ–≥ Cramer's V –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1)
        max_categories: int - –º–∞–∫—Å. —á–∏—Å–ª–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
        show_plot: bool - —Ä–∏—Å–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
        palette: str - –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "tab10")

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 
        pd.DataFrame - –æ—Ç—á—ë—Ç —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
            - –ü—Ä–∏–∑–Ω–∞–∫
            - –û–ø–∏—Å–∞–Ω–∏–µ
            - –¢–∏–ø (—á–∏—Å–ª–æ–≤–æ–π / –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π)
            - –°—Ç–∞—Ç—É—Å (‚úîÔ∏è Ok / üö® –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ / ‚ö†Ô∏è –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π)
            - –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ (Cohen's d –∏–ª–∏ Cramer's V)
            - –ú–µ—Ç—Ä–∏–∫–∞
            - p-value (—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å)
            - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

    –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:
        - Cohen's d = |Œº‚ÇÅ - Œº‚ÇÇ| / œÉ_pooled - –º–∞—Å—à—Ç–∞–±-–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–∞—è –º–µ—Ä–∞ —ç—Ñ—Ñ–µ–∫—Ç–∞
        - Cramer's V = ‚àö(œá¬≤ / (n √ó min(r-1, c-1))) - –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ä–∞ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏

    –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã:
        - –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:
            ‚Ä¢ Welch's t-test (–µ—Å–ª–∏ ‚â•20 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –æ–±–µ–∏—Ö –≤—ã–±–æ—Ä–∫–∞—Ö),
            ‚Ä¢ Mann-Whitney U test (–≤ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö, –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π).
        - –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:
            ‚Ä¢ Chi-square test of independence –Ω–∞ —Ç–∞–±–ª–∏—Ü–µ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏.
        - p-value –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è;
          —Ñ–ª–∞–≥ ¬´üö® –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ¬ª —Å—Ç–∞–≤–∏—Ç—Å—è –ø–æ —Ä–∞–∑–º–µ—Ä—É —ç—Ñ—Ñ–µ–∫—Ç–∞ (Cohen's d / Cramer's V),
          —á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–∫–∞—Ö.

    –ü—Ä–∏–º–µ—Ä—ã:
        >>> # –ë–∞–∑–æ–≤—ã–π –≤—ã–∑–æ–≤ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–∞
        report = compare_train_test_overview(X_train, X_test)

        >>> # –° —Ç–∞—Ä–≥–µ—Ç–æ–º –∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        report = compare_train_test_overview(
            train=df_train,
            test=df_test,
            target='profit',
            exclude=['id'],
            numeric_threshold=0.35,
            categorical_threshold=0.1,
            show_plot=True
        )

        >>> # –°–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        report = compare_train_test_overview(
            train=df_train,
            test=df_test,
            numeric_threshold=0.1,      # –º–∞–ª—ã–π —ç—Ñ—Ñ–µ–∫—Ç
            categorical_threshold=0.05  # —Å–ª–∞–±–∞—è —Å–≤—è–∑—å
        )    
    """    

    # 1. –û–±—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    common_cols = sorted(set(train.columns) & set(test.columns))
    common_cols_original = common_cols.copy()
    if target and target in common_cols:
        target_name, target_desc = label_for_column(target, separator="‚Ä¢")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{target_name}'{target_desc}")
        print(f"    üëÅÔ∏è‚Äçüó®Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–¥–≤–∏–≥–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...")
        # –ê–Ω–∞–ª–∏–∑ —Ç–∞—Ä–≥–µ—Ç–∞
        target_has_drift = False
        tr = train[target].dropna()
        te = test[target].dropna()
        if pd.api.types.is_numeric_dtype(tr) and pd.api.types.is_numeric_dtype(te):
            n_tr, n_te = len(tr), len(te)
            if n_tr > 1 and n_te > 1:
                pooled_std = np.sqrt(((n_tr - 1) * tr.std()**2 + (n_te - 1) * te.std()**2) / (n_tr + n_te - 2))
                cohens_d = abs(tr.mean() - te.mean()) / pooled_std if pooled_std > 0 else 0.0
                try:
                    _, p_val = ttest_ind(tr, te, equal_var=False)
                except:
                    p_val = np.nan
                if cohens_d > 0.1:
                    print(f"    üö® –°–¥–≤–∏–≥ —Ç–∞—Ä–≥–µ—Ç–∞:  d={cohens_d:.3f}, p={p_val:.3f}\n")
                    target_has_drift = True
                else:
                    print(f"    ‚úîÔ∏è –¢–∞—Ä–≥–µ—Ç —Å—Ç–∞–±–∏–ª–µ–Ω: Cohen's d={cohens_d:.3f}\n")
        else:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            train_labeled = train[[target]].copy()
            train_labeled['dataset'] = 'train'
            test_labeled = test[[target]].copy()
            test_labeled['dataset'] = 'test'
            combined_target = pd.concat([train_labeled, test_labeled], ignore_index=True)

            # –¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏
            observed = pd.crosstab(combined_target['dataset'], combined_target[target])

            # Chi-square –∏ Cramer's V
            try:
                chi2, p_val, dof, expected = chi2_contingency(observed)
                n = observed.sum().sum()
                min_dim = min(observed.shape) - 1
                if min_dim > 0 and n > 0:
                    cramers_v = np.sqrt(chi2 / (n * min_dim))
                    cramers_v = min(cramers_v, 1.0)
                else:
                    cramers_v = 0.0
                    p_val = np.nan
            except:
                cramers_v = 0.0
                p_val = np.nan

            if cramers_v > 0.1:
                print(f"    üö® –°–¥–≤–∏–≥ —Ç–∞—Ä–≥–µ—Ç–∞: Cramer's V={cramers_v:.3f}, p={p_val:.3f}\n")
                target_has_drift = True
            else:
                print(f"    ‚úîÔ∏è –¢–∞—Ä–≥–µ—Ç —Å—Ç–∞–±–∏–ª–µ–Ω: Cramer's V={cramers_v:.3f}\n")
        common_cols = [col for col in common_cols if col != target]

    if include is not None:
        common_cols = [col for col in common_cols if col in include]
    if exclude is not None:
        common_cols = [col for col in common_cols if col not in exclude]

    if not common_cols:
        print("‚ö†Ô∏è –ù–µ—Ç –æ–±—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return pd.DataFrame()

    # 1.5. –ê–≤—Ç–æ-–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ ID-–ø–æ–¥–æ–±–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    id_candidates = []
    for col in common_cols:
        is_unique_train = (train[col].nunique() == len(train)) if len(train) > 1 else False
        is_unique_test = (test[col].nunique() == len(test)) if len(test) > 1 else False
        if is_unique_train and is_unique_test:
            if re.search(r'id$', col, re.IGNORECASE):
                id_candidates.append(col)
            elif train[col].dtype == 'object':
                non_null = pd.concat([train[col], test[col]], ignore_index=True).dropna().astype(str)
                if len(non_null) > 0 and non_null.str.match(r'^[a-zA-Z0-9._-]+$').all():
                    id_candidates.append(col)

    train_name, train_desc = label_for_dataset(train, separator='‚Ä¢')
    test_name, test_desc = label_for_dataset(test, separator='‚Ä¢')

    if id_candidates:
        print(f"üì¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ üÜî –∫–æ–ª–æ–Ω–∫–∏: {id_candidates}")
        print(f"    üìå –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç—å –∏—Ö —á–µ—Ä–µ–∑ exclude=.\n")

    print(f"üïµ –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏:")
    print(f"    üóÉÔ∏è {train_name}{train_desc}")
    print(f"    üóÉÔ∏è {test_name}{test_desc}")
    print(f"–ø–æ {len(common_cols)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º")

    # 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç–∏–ø—ã
    numeric_cols = [col for col in common_cols if pd.api.types.is_numeric_dtype(train[col]) and pd.api.types.is_numeric_dtype(test[col])]
    categorical_cols = [col for col in common_cols if col not in numeric_cols]
    all_analyzed_cols = numeric_cols + categorical_cols

    results = []

    # 3. –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ - Cohen's d + p-value
    for col in numeric_cols:
        tr = train[col].dropna()
        te = test[col].dropna()
        if len(tr) == 0 or len(te) == 0:
            continue

        if tr.nunique() <= 1 and te.nunique() <= 1:
            results.append({
                "–ü—Ä–∏–∑–Ω–∞–∫": col,
                "–¢–∏–ø": "1Ô∏è‚É£ —á–∏—Å–ª–æ–≤–æ–π",
                "–°—Ç–∞—Ç—É—Å": "‚ö†Ô∏è –ü–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π",
                "–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ": 0.0,
                "–ú–µ—Ç—Ä–∏–∫–∞": "–ø–æ—á—Ç–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π",
                "p-value": np.nan,
                "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö"
            })
            continue

        n_tr, n_te = len(tr), len(te)
        mean_tr, mean_te = tr.mean(), te.mean()
        std_tr, std_te = tr.std(), te.std()

        pooled_std = np.sqrt(((n_tr - 1) * std_tr**2 + (n_te - 1) * std_te**2) / (n_tr + n_te - 2))
        cohens_d = abs(mean_tr - mean_te) / pooled_std if pooled_std > 0 else 0.0

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
        p_val = np.nan
        if n_tr > 1 and n_te > 1:
            try:
                if min(tr.nunique(), te.nunique()) > 20:
                    _, p_val = ttest_ind(tr, te, equal_var=False)
                else:
                    _, p_val = mannwhitneyu(tr, te, alternative='two-sided')
            except:
                p_val = np.nan

        # –†–µ—à–µ–Ω–∏–µ –ø–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
        if cohens_d > numeric_threshold:
            status = "üö® –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ"
            if not np.isnan(p_val) and p_val < 0.05:
                rec = "–°–¥–≤–∏–≥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏"
            else:
                rec = "–°–¥–≤–∏–≥ –≤–æ–∑–º–æ–∂–µ–Ω - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é"
        else:
            status = "‚úîÔ∏è Ok"
            rec = ""

        col_name, col_desc = label_for_column(col, separator="")

        results.append({
            "–ü—Ä–∏–∑–Ω–∞–∫": col_name,
            "–û–ø–∏—Å–∞–Ω–∏–µ": col_desc,
            "–¢–∏–ø": "1Ô∏è‚É£ —á–∏—Å–ª–æ–≤–æ–π",
            "–°—Ç–∞—Ç—É—Å": status,
            "–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ": cohens_d,
            "–ú–µ—Ç—Ä–∏–∫–∞": "Cohen's d",
            "p-value": p_val,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": rec
        })

    # 4. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ - Cramer's V + chi-square p-value
    min_freq = 0.005
    for col in categorical_cols:
        if max(train[col].nunique(), test[col].nunique()) > max_categories:
            continue

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        train_labeled = train[[col]].copy()
        train_labeled['dataset'] = 'train'
        test_labeled = test[[col]].copy()
        test_labeled['dataset'] = 'test'
        combined = pd.concat([train_labeled, test_labeled], ignore_index=True)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        value_counts = combined[col].value_counts(normalize=True)
        keep_cats = value_counts[value_counts >= min_freq].index
        if len(keep_cats) == 0:
            continue
        combined = combined[combined[col].isin(keep_cats)]

        # –¢–∞–±–ª–∏—Ü–∞ —Å–æ–ø—Ä—è–∂—ë–Ω–Ω–æ—Å—Ç–∏
        observed = pd.crosstab(combined['dataset'], combined[col])

        # Chi-square test –∏ Cramer's V
        try:
            chi2, p_val, dof, expected = chi2_contingency(observed)
            n = observed.sum().sum()
            min_dim = min(observed.shape) - 1
            if min_dim > 0 and n > 0:
                cramers_v = np.sqrt(chi2 / (n * min_dim))
                cramers_v = min(cramers_v, 1.0)  # –∑–∞—â–∏—Ç–∞ –æ—Ç —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
            else:
                cramers_v = 0.0
                p_val = np.nan
        except:
            cramers_v = 0.0
            p_val = np.nan

        # –†–µ—à–µ–Ω–∏–µ –ø–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
        if cramers_v > categorical_threshold:
            status = "üö® –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ"
            if not np.isnan(p_val) and p_val < 0.05:
                rec = "–°–¥–≤–∏–≥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏"
            else:
                rec = "–°–¥–≤–∏–≥ –≤–æ–∑–º–æ–∂–µ–Ω - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö"
        else:
            status = "‚úîÔ∏è Ok"
            rec = ""

        col_name, col_desc = label_for_column(col, separator="")

        results.append({
            "–ü—Ä–∏–∑–Ω–∞–∫": col_name,
            "–û–ø–∏—Å–∞–Ω–∏–µ": col_desc,
            "–¢–∏–ø": "üè∑Ô∏è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π",
            "–°—Ç–∞—Ç—É—Å": status,
            "–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ": cramers_v,
            "–ú–µ—Ç—Ä–∏–∫–∞": "Cramer's V",
            "p-value": p_val,
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": rec
        })

    # 5. –í—ã–≤–æ–¥ –æ—Ç—á—ë—Ç–∞
    if not results:
        print("‚úîÔ∏è –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return pd.DataFrame()

    report_df = pd.DataFrame(results)

    # –°–≤–æ–¥–∫–∞
    n_issues = len(report_df[report_df["–°—Ç–∞—Ç—É—Å"].str.contains("üö®")])
    print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {n_issues} –∏–∑ {len(report_df)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    if n_issues == 0:
        print("üíé –î–∞–Ω–Ω—ã–µ train/test —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã - –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é!")

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ —Ç–∏–ø—É, –ø–æ—Ç–æ–º –ø–æ –∏–º–µ–Ω–∏
    report_df["–ü—Ä–∏–∑–Ω–∞–∫_—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞"] = report_df["–ü—Ä–∏–∑–Ω–∞–∫"].apply(
        lambda x: x.split("‚Ä¢")[0].strip() if "‚Ä¢" in x else x
    )
    report_df["–¢–∏–ø_—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞"] = report_df["–¢–∏–ø"].map({"1Ô∏è‚É£ —á–∏—Å–ª–æ–≤–æ–π": 0, "üè∑Ô∏è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π": 1})
    report_df = report_df.sort_values(
        ["–¢–∏–ø_—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", "–ü—Ä–∏–∑–Ω–∞–∫_—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞"]
    ).reset_index(drop=True)
    report_df = report_df.drop(columns=["–¢–∏–ø_—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", "–ü—Ä–∏–∑–Ω–∞–∫_—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞"])

    # –ù—É–º–µ—Ä–∞—Ü–∏—è
    cols_order = ['–ü—Ä–∏–∑–Ω–∞–∫', '–û–ø–∏—Å–∞–Ω–∏–µ', '–¢–∏–ø', '–°—Ç–∞—Ç—É—Å', '–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ', '–ú–µ—Ç—Ä–∏–∫–∞', 'p-value', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è']
    report_df = report_df[cols_order]

    def _color_status(val):
        if "üö®" in val:
            return "background-color: #ffebee; color: #c62828"
        elif "‚ö†Ô∏è" in val:
            return "background-color: #fff3e0; color: #ef6c00"
        return ""

    display_table(
        report_df,
        rows=len(report_df),
        float_precision=3,
        styler_func=lambda s: s.applymap(_color_status, subset=["–°—Ç–∞—Ç—É—Å"])
    )    

    # 6. –ì—Ä–∞—Ñ–∏–∫–∏ - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç—Ä—ë—Ö —Ä–µ–∂–∏–º–æ–≤
    plot_mode = show_plot
    if isinstance(plot_mode, bool):
        plot_mode = "problematic" if plot_mode else None

    cols_to_plot = []

    if plot_mode == "problematic":
        # –¢–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å üö® + target (–µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–π)
        cols_to_plot = report_df[report_df["–°—Ç–∞—Ç—É—Å"].str.contains("üö®")]["–ü—Ä–∏–∑–Ω–∞–∫"].tolist()
        if target and target_has_drift:
            cols_to_plot = [target] + cols_to_plot

    elif plot_mode == "all":
        # –í—Å–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ + target (–µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª—Å—è)
        cols_to_plot = all_analyzed_cols.copy()
        if target and target in common_cols_original:
            # target –∏—Å–∫–ª—é—á—ë–Ω –∏–∑ –∞–Ω–∞–ª–∏–∑–∞, –Ω–æ –µ—Å–ª–∏ –æ–Ω –±—ã–ª - –¥–æ–±–∞–≤–∏–º –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏
            if target_has_drift:
                cols_to_plot = [target] + cols_to_plot
            else:
                # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –¥—Ä–∏—Ñ—Ç–∞–Ω—É–ª - –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å, —Ç.–∫. —Ä–µ–∂–∏–º 'all'
                cols_to_plot = [target] + cols_to_plot

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ 15 –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ—Å—Ç–∞—ë—Ç—Å—è (–ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
    if cols_to_plot:
        print(f"\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ({len(cols_to_plot)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 5):")
        for col in cols_to_plot[:15]:
            try:
                col_name, col_desc = label_for_column(col, separator="‚Ä¢")
                print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ üìÑ {col_name}{col_desc}")
                plot_train_test_distribution(train, test, col, palette=palette, table_metrics='extended')
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è {col}: {e}")

    return report_df



#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢



# validate_datasets_consistency: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É –æ—Å–Ω–æ–≤–Ω—ã–º –∏ –∑–∞–≤–∏—Å–∏–º—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
def validate_datasets_consistency(
    master_df: pd.DataFrame,
    id_column: str,
    dependent_dfs: list[pd.DataFrame]
) -> None:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É –æ—Å–Ω–æ–≤–Ω—ã–º –∏ –∑–∞–≤–∏—Å–∏–º—ã–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ ID –∏–∑ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–∞ —Å ID –≤ –∫–∞–∂–¥–æ–º –∑–∞–≤–∏—Å–∏–º–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç `label_for_dataset` –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º—ë–Ω —Ç–∞–±–ª–∏—Ü.
        –î–ª—è –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã –≤—ã–≤–æ–¥–∏—Ç:
        - –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ ID (–µ—Å—Ç—å –≤ –º–∞—Å—Ç–µ—Ä, –Ω–æ –Ω–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ–π),
        - –ª–∏—à–Ω–∏–µ ID (–µ—Å—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ–π, –Ω–æ –Ω–µ—Ç –≤ –º–∞—Å—Ç–µ—Ä).
        –ü–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø—Ä–∏ join –∏ "–º—É—Å–æ—Ä–Ω—ã—Ö" –∑–∞–ø–∏—Å–µ–π.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        master_df : pd.DataFrame
            –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç - –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã –ø–æ —Å–ø–∏—Å–∫—É –∫–ª–∏–µ–Ω—Ç–æ–≤/–æ–±—ä–µ–∫—Ç–æ–≤.
        id_column : col
            –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–æ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö).
        dependent_dfs : list[pd.DataFrame]
            –°–ø–∏—Å–æ–∫ –∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ (–±–µ–∑ —è–≤–Ω—ã—Ö –∏–º—ë–Ω - –∏–º–µ–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏).

    –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
        None - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ —è—á–µ–π–∫—É –Ω–æ—É—Ç–±—É–∫–∞.
        
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        validate_datasets_consistency(
            master_df=df_market_file,
            id_column='id',
            dependent_dfs=[df_market_money, df_market_time, df_money]
        )
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è id_column –≤ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–µ
    if id_column not in master_df.columns:
        print(f"‚ùå –û–®–ò–ë–ö–ê: –≤ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü '{id_column}'")
        return

    # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–∞
    master_name, master_desc = label_for_dataset(master_df, separator="‚Ä¢")
    master_label = f"{master_name}{master_desc}" if master_desc else master_name

    master_ids = set(master_df[id_column].unique())
    n_master = len(master_ids)


    col_name, col_desc = label_for_column(id_column, separator="()")
    full_col_name = f"'{col_name}'{col_desc}"

    dataset_profile(master_df, report='summary')
    print(f"\nüïµ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ —Å—Ç–æ–ª–±—Ü—É {full_col_name}")

    all_good = True

    for df in dependent_dfs:
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        df_name, df_desc = label_for_dataset(df, separator="‚Ä¢")
        df_label = f"{df_name}{df_desc}" if df_desc else df_name

        if id_column not in df.columns:
            print(f"     ‚ùå –û–®–ò–ë–ö–ê: –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ '{df_label}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü '{id_column}'")
            all_good = False
            continue

        table_ids = set(df[id_column].unique())
        missing = master_ids - table_ids      # –µ—Å—Ç—å –≤ –º–∞—Å—Ç–µ—Ä, –Ω–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ–π
        extra = table_ids - master_ids        # –µ—Å—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ–π, –Ω–µ—Ç –≤ –º–∞—Å—Ç–µ—Ä

        if not missing and not extra:
            print(f"     ‚úîÔ∏è {df_label} üíé –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ '{col_name}'")
        else:
            print(f"     ‚ö†Ô∏è  {df_label}:")
            if missing:
                print(f"    ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ç–∞–±–ª–∏—Ü–µ: {sorted(missing)}")
            if extra:
                print(f"    üóëÔ∏è –õ–∏—à–Ω–∏–µ ID: {sorted(extra)}")
            all_good = False

    if all_good:
        print(f"\nüíé –í—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –ø–æ —Å—Ç–æ–ª–±—Ü—É {full_col_name}")
    else:
        print(f"\nüí° –°–æ–≤–µ—Ç: –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ join –æ—Å—Ç–∞–≤—å—Ç–µ –≤ –∑–∞–≤–∏—Å–∏–º—ã—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö —Ç–æ–ª—å–∫–æ ID –∏–∑ –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç–∞.")


#‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢


# check_train_test_id_leakage - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ ID –º–µ–∂–¥—É train –∏ test
def check_train_test_id_leakage(
    train: pd.DataFrame,
    test: pd.DataFrame,
    id_column: str = "id"
) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ ID –º–µ–∂–¥—É train –∏ test.
    
    –û–ø–∏—Å–∞–Ω–∏–µ:
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ ID –≤ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö.
        –ï—Å–ª–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –Ω–µ –ø—É—Å—Ç–æ - —ç—Ç–æ data leakage, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç `_label_for_dataset` –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–¥–ø–∏—Å–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        train: pd.DataFrame - –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞
        test: pd.DataFrame - —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞
        id_column: str - –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'id')
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool - True, –µ—Å–ª–∏ —É—Ç–µ—á–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ ‚â† ‚àÖ), –∏–Ω–∞—á–µ False
    
    –ü—Ä–∏–º–µ—Ä:
        >>> has_leak = check_train_test_id_leakage(df_train, df_test, id_column='user_id')
        >>> if has_leak:
        ...     print("‚ö†Ô∏è –£—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö! –ù—É–∂–Ω–æ –ø–µ—Ä–µ—Ä–∞–∑–¥–µ–ª–∏—Ç—å –≤—ã–±–æ—Ä–∫–∏.")
    """
    if id_column not in train.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{id_column}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ train")
    if id_column not in test.columns:
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{id_column}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ test")
    
    train_ids = set(train[id_column].dropna().unique())
    test_ids = set(test[id_column].dropna().unique())
    overlap = train_ids & test_ids
    
    train_name, train_desc = label_for_dataset(train, separator="‚Ä¢")
    test_name, test_desc = label_for_dataset(test, separator="‚Ä¢")
    
    if overlap:
        print(f"üö® –£–¢–ï–ß–ö–ê –î–ê–ù–ù–´–•: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(overlap)} –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è ID –º–µ–∂–¥—É {train_name} –∏ {test_name}")
        if len(overlap) <= 10:
            print(f"   üìå ID: {sorted(overlap)}")
        else:
            sample_overlap = sorted(list(overlap))[:5]
            print(f"   üìå –ü—Ä–∏–º–µ—Ä—ã ID: {sample_overlap} ... (–≤—Å–µ–≥–æ {len(overlap)})")
        return True
    else:
        print(f"üïµ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ [ {id_column} ] –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞—Ö :")        
        print(f"    üóÉÔ∏è {train_name}{train_desc}")
        print(f"    üóÉÔ∏è {test_name}{test_desc}")
        print(f"‚úîÔ∏è –ù–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è [ {id_column} ]")
        print(f"üíé –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return False



# ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢ –ö–û–ù–ï–¶ –§–£–ù–ö–¶–ò–ô 3Filoff ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢



# –≠–∫—Å–ø–æ—Ä—Ç —Ç–æ–ª—å–∫–æ –ø—É–±–ª–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π (–±–µ–∑ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è)
__all__ = [
    "set_global_styles",
    "set_output_mode",
    "label_for_column",
    "label_for_dataset",
    "display_table",
    "standardize_column_names",
    "preview",
    "dataset_convert_datetime",
    "load_dataset",
    "dataset_profile",
    "dataset_quick_audit",
    "dataset_overview",
    "handle_duplicates",
    "audit_numerical",
    "report_numerical_consistency",
    "audit_categorical",
    "audit_categorical_frequencies",
    "audit_categorical_cross",
    "audit_categorical_typos",
    "audit_numerical_distribution",
    "plot_feature_distribution",
    "plot_feature_distribution_advanced",
    "plot_target_relationships",
    "plot_mixed_correlation",
    "plot_pairwise_correlations",
    "plot_categorical_distribution",
    "plot_phik_correlation",
    "plot_train_test_distribution",
    "plot_compare_train_test_ecdf",
    "plot_shap_summary",
    "plot_discrete_train_test",
    "plot_numerical_profile",
    "plot_missing_summary",
    "plot_binary_heatmap",
    "compare_train_test_overview",
    "validate_datasets_consistency",
    "check_train_test_id_leakage"
]